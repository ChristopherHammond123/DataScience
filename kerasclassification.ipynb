{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Home_Team</th>\n",
       "      <th>Away_Team</th>\n",
       "      <th>Season</th>\n",
       "      <th>Round</th>\n",
       "      <th>Home_Score</th>\n",
       "      <th>Away_Score</th>\n",
       "      <th>Home_Win</th>\n",
       "      <th>Away_Win</th>\n",
       "      <th>Draw</th>\n",
       "      <th>Elo_Home</th>\n",
       "      <th>...</th>\n",
       "      <th>Home_Team_Loss_Number</th>\n",
       "      <th>Away_Team_Loss_Number</th>\n",
       "      <th>Home_Score_for_Number</th>\n",
       "      <th>Home_Score_against_Number</th>\n",
       "      <th>Away_Score_for_Number</th>\n",
       "      <th>Away_Score_against_Number</th>\n",
       "      <th>Home_Team_for_Average</th>\n",
       "      <th>Home_Team_against_Average</th>\n",
       "      <th>Away_Team_for_Average</th>\n",
       "      <th>Away_Team_against_Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saarbrücken</td>\n",
       "      <td>Stuttgarter Kickers</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-186.0</td>\n",
       "      <td>267</td>\n",
       "      <td>-204.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>-281</td>\n",
       "      <td>1.570588</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>1.010870</td>\n",
       "      <td>-1.527174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SC Freiburg</td>\n",
       "      <td>Unterhaching</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-534.0</td>\n",
       "      <td>-248.0</td>\n",
       "      <td>890</td>\n",
       "      <td>-651.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>-398</td>\n",
       "      <td>1.633028</td>\n",
       "      <td>-1.194495</td>\n",
       "      <td>0.995633</td>\n",
       "      <td>-1.737991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VfL Osnabrück</td>\n",
       "      <td>Meppen</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-208.0</td>\n",
       "      <td>-157.0</td>\n",
       "      <td>267</td>\n",
       "      <td>-249.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>-251</td>\n",
       "      <td>1.517045</td>\n",
       "      <td>-1.414773</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>-1.695946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RW Essen</td>\n",
       "      <td>Schalke 04</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-128.0</td>\n",
       "      <td>-492.0</td>\n",
       "      <td>151</td>\n",
       "      <td>-137.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>-813</td>\n",
       "      <td>1.398148</td>\n",
       "      <td>-1.268519</td>\n",
       "      <td>1.186813</td>\n",
       "      <td>-1.489011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alemannia Aachen</td>\n",
       "      <td>MSV Duisburg</td>\n",
       "      <td>1990</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-237.0</td>\n",
       "      <td>-483.0</td>\n",
       "      <td>400</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>-731</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-1.250000</td>\n",
       "      <td>1.110403</td>\n",
       "      <td>-1.552017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146493</th>\n",
       "      <td>Lecce</td>\n",
       "      <td>SPAL</td>\n",
       "      <td>2021</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-442.0</td>\n",
       "      <td>-131.0</td>\n",
       "      <td>538</td>\n",
       "      <td>-478.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>-161</td>\n",
       "      <td>1.315403</td>\n",
       "      <td>-1.168704</td>\n",
       "      <td>0.929825</td>\n",
       "      <td>-1.412281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146494</th>\n",
       "      <td>Frosinone</td>\n",
       "      <td>Cittadella</td>\n",
       "      <td>2021</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-258.0</td>\n",
       "      <td>-292.0</td>\n",
       "      <td>326</td>\n",
       "      <td>-283.0</td>\n",
       "      <td>307.0</td>\n",
       "      <td>-390</td>\n",
       "      <td>1.347107</td>\n",
       "      <td>-1.169421</td>\n",
       "      <td>1.092527</td>\n",
       "      <td>-1.387900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146495</th>\n",
       "      <td>Reggina</td>\n",
       "      <td>Vicenza</td>\n",
       "      <td>2021</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-369.0</td>\n",
       "      <td>-467.0</td>\n",
       "      <td>415</td>\n",
       "      <td>-341.0</td>\n",
       "      <td>404.0</td>\n",
       "      <td>-608</td>\n",
       "      <td>1.195965</td>\n",
       "      <td>-0.982709</td>\n",
       "      <td>0.922374</td>\n",
       "      <td>-1.388128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146496</th>\n",
       "      <td>Venezia</td>\n",
       "      <td>Nuova Cosenza</td>\n",
       "      <td>2021</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-310.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>391</td>\n",
       "      <td>-300.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>-72</td>\n",
       "      <td>1.273616</td>\n",
       "      <td>-0.977199</td>\n",
       "      <td>1.075472</td>\n",
       "      <td>-1.358491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146497</th>\n",
       "      <td>Chievo</td>\n",
       "      <td>Pisa</td>\n",
       "      <td>2021</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-490.0</td>\n",
       "      <td>-193.0</td>\n",
       "      <td>587</td>\n",
       "      <td>-500.0</td>\n",
       "      <td>197.0</td>\n",
       "      <td>-256</td>\n",
       "      <td>1.246285</td>\n",
       "      <td>-1.061571</td>\n",
       "      <td>1.031414</td>\n",
       "      <td>-1.340314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>146498 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Home_Team            Away_Team  Season  Round  Home_Score  \\\n",
       "0            Saarbrücken  Stuttgarter Kickers    1990      1           1   \n",
       "1            SC Freiburg         Unterhaching    1990      1           2   \n",
       "2          VfL Osnabrück               Meppen    1990      1           1   \n",
       "3               RW Essen           Schalke 04    1990      1           0   \n",
       "4       Alemannia Aachen         MSV Duisburg    1990      1           1   \n",
       "...                  ...                  ...     ...    ...         ...   \n",
       "146493             Lecce                 SPAL    2021     33           1   \n",
       "146494         Frosinone           Cittadella    2021     33           1   \n",
       "146495           Reggina             Vicenza     2021     33           3   \n",
       "146496           Venezia        Nuova Cosenza    2021     33           3   \n",
       "146497            Chievo                 Pisa    2021     33           2   \n",
       "\n",
       "        Away_Score  Home_Win  Away_Win  Draw  Elo_Home  ...  \\\n",
       "0              0.0         1         0     0      56.0  ...   \n",
       "1              3.0         0         1     0      53.0  ...   \n",
       "2              1.0         0         0     1      52.0  ...   \n",
       "3              0.0         0         0     1      53.0  ...   \n",
       "4              2.0         0         1     0      57.0  ...   \n",
       "...            ...       ...       ...   ...       ...  ...   \n",
       "146493         2.0         0         1     0       0.0  ...   \n",
       "146494         1.0         0         0     1       0.0  ...   \n",
       "146495         0.0         1         0     0       0.0  ...   \n",
       "146496         0.0         1         0     0       0.0  ...   \n",
       "146497         0.0         1         0     0       0.0  ...   \n",
       "\n",
       "        Home_Team_Loss_Number  Away_Team_Loss_Number  Home_Score_for_Number  \\\n",
       "0                      -180.0                 -186.0                    267   \n",
       "1                      -534.0                 -248.0                    890   \n",
       "2                      -208.0                 -157.0                    267   \n",
       "3                      -128.0                 -492.0                    151   \n",
       "4                      -237.0                 -483.0                    400   \n",
       "...                       ...                    ...                    ...   \n",
       "146493                 -442.0                 -131.0                    538   \n",
       "146494                 -258.0                 -292.0                    326   \n",
       "146495                 -369.0                 -467.0                    415   \n",
       "146496                 -310.0                  -61.0                    391   \n",
       "146497                 -490.0                 -193.0                    587   \n",
       "\n",
       "        Home_Score_against_Number  Away_Score_for_Number  \\\n",
       "0                          -204.0                  186.0   \n",
       "1                          -651.0                  228.0   \n",
       "2                          -249.0                  124.0   \n",
       "3                          -137.0                  648.0   \n",
       "4                          -300.0                  523.0   \n",
       "...                           ...                    ...   \n",
       "146493                     -478.0                  106.0   \n",
       "146494                     -283.0                  307.0   \n",
       "146495                     -341.0                  404.0   \n",
       "146496                     -300.0                   57.0   \n",
       "146497                     -500.0                  197.0   \n",
       "\n",
       "        Away_Score_against_Number  Home_Team_for_Average  \\\n",
       "0                            -281               1.570588   \n",
       "1                            -398               1.633028   \n",
       "2                            -251               1.517045   \n",
       "3                            -813               1.398148   \n",
       "4                            -731               1.666667   \n",
       "...                           ...                    ...   \n",
       "146493                       -161               1.315403   \n",
       "146494                       -390               1.347107   \n",
       "146495                       -608               1.195965   \n",
       "146496                        -72               1.273616   \n",
       "146497                       -256               1.246285   \n",
       "\n",
       "        Home_Team_against_Average  Away_Team_for_Average  \\\n",
       "0                       -1.200000               1.010870   \n",
       "1                       -1.194495               0.995633   \n",
       "2                       -1.414773               0.837838   \n",
       "3                       -1.268519               1.186813   \n",
       "4                       -1.250000               1.110403   \n",
       "...                           ...                    ...   \n",
       "146493                  -1.168704               0.929825   \n",
       "146494                  -1.169421               1.092527   \n",
       "146495                  -0.982709               0.922374   \n",
       "146496                  -0.977199               1.075472   \n",
       "146497                  -1.061571               1.031414   \n",
       "\n",
       "        Away_Team_against_Average  \n",
       "0                       -1.527174  \n",
       "1                       -1.737991  \n",
       "2                       -1.695946  \n",
       "3                       -1.489011  \n",
       "4                       -1.552017  \n",
       "...                           ...  \n",
       "146493                  -1.412281  \n",
       "146494                  -1.387900  \n",
       "146495                  -1.388128  \n",
       "146496                  -1.358491  \n",
       "146497                  -1.340314  \n",
       "\n",
       "[146498 rows x 28 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('DSFootballprojectdata.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna( method ='ffill', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['Home_Win', 'Away_Win','Draw','Home_Team','Away_Team','Elo_Home','Elo_Away'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data[['Home_Win','Away_Win','Draw']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = Y.to_numpy()\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 0.]],\n",
       "\n",
       "       [[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [0., 1., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]],\n",
       "\n",
       "       [[0., 1., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical   \n",
    "Y = to_categorical(Y, num_classes=3)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146498, 21)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146498, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102548, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43950, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_testdf = pd.DataFrame(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Y_test_new = pd.DataFrame([x for x in np.where(Y_testdf ==1, Y_testdf.columns,'').flatten().tolist() if len(x) >0],columns= ([\"class\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43943</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43944</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43945</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43946</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43947</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43948 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class\n",
       "0         0\n",
       "1         2\n",
       "2         0\n",
       "3         0\n",
       "4         2\n",
       "...     ...\n",
       "43943     1\n",
       "43944     1\n",
       "43945     2\n",
       "43946     1\n",
       "43947     2\n",
       "\n",
       "[43948 rows x 1 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        2\n",
       "2        0\n",
       "3        0\n",
       "4        2\n",
       "        ..\n",
       "43943    1\n",
       "43944    1\n",
       "43945    2\n",
       "43946    1\n",
       "43947    2\n",
       "Name: class, Length: 43948, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_new['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '1',\n",
       " '2',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '2',\n",
       " '0',\n",
       " '0',\n",
       " '0',\n",
       " '1',\n",
       " ...]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_new = Y_test_new['class'].tolist()\n",
    "Y_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_new = np.asarray(Y_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43943</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43944</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43945</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43946</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43947</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43948 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      class\n",
       "0         0\n",
       "1         2\n",
       "2         0\n",
       "3         0\n",
       "4         2\n",
       "...     ...\n",
       "43943     1\n",
       "43944     1\n",
       "43945     2\n",
       "43946     1\n",
       "43947     2\n",
       "\n",
       "[43948 rows x 1 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0'],\n",
       "       ['2'],\n",
       "       ['0'],\n",
       "       ...,\n",
       "       ['2'],\n",
       "       ['1'],\n",
       "       ['2']], dtype=object)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_new.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create validation and test split\n",
    "#\n",
    "X_val, Xnew_test, Y_val, Ynew_test = train_test_split(X_test, Y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_train_std = ss.fit_transform(X_train)\n",
    "X_test_std = ss.transform(X_test)\n",
    "# X_val_std = ss.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-24 19:59:12.019793: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-24 19:59:12.019864: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(15, activation='relu', input_shape=(21,)))\n",
    "    network.add(layers.Dense(10, activation='relu'))\n",
    "    network.add(layers.Dense(3, activation='softmax'))\n",
    "    #\n",
    "    # Configure the network with optimizer, loss function and accuracy\n",
    "    #\n",
    "    network.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9773/3166071793.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_classifier, epochs=20, batch_size=60)\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model = KerasClassifier(build_fn=build_classifier, epochs=20, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102548, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-23 20:48:26.373788: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-23 20:48:26.373816: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-23 20:48:26.373843: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-23 20:48:26.375031: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1710/1710 [==============================] - 5s 2ms/step - loss: 0.0263 - accuracy: 0.9897\n",
      "Epoch 2/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 9.0834e-06 - accuracy: 1.0000\n",
      "Epoch 3/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 2.2706e-06 - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 9.9111e-07 - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 4.4746e-07 - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 1.9891e-04 - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 0.0029 - accuracy: 0.9993\n",
      "Epoch 8/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 2.2328e-06 - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 8.0900e-07 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 3.5775e-07 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 1.6186e-07 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 7.1755e-08 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "1710/1710 [==============================] - 4s 3ms/step - loss: 3.0995e-08 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "1710/1710 [==============================] - 4s 2ms/step - loss: 1.2565e-08 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "1710/1710 [==============================] - 8s 4ms/step - loss: 4.6580e-09 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1710/1710 [==============================] - 5s 3ms/step - loss: 1.5507e-09 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1710/1710 [==============================] - 5s 3ms/step - loss: 5.0800e-10 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1710/1710 [==============================] - 4s 3ms/step - loss: 1.4531e-10 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1710/1710 [==============================] - 4s 3ms/step - loss: 4.4174e-11 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1710/1710 [==============================] - 4s 3ms/step - loss: 1.6275e-11 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9ca473f310>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_std,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 2, 1, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_std)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 ... 2 1 2]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "integer_encoded = label_encoder.fit_transform(y_pred)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 0, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pd.get_dummies(y_pred)\n",
    "y_pred.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preddf = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_new = pd.DataFrame([x for x in np.where(y_preddf ==1, y_preddf.columns,'').flatten().tolist() if len(x) >0],columns= ([\"class\"]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0'],\n",
       "       ['0'],\n",
       "       ['0'],\n",
       "       ...,\n",
       "       ['2'],\n",
       "       ['0'],\n",
       "       ['2']], dtype=object)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_new.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0'],\n",
       "       ['2'],\n",
       "       ['0'],\n",
       "       ...,\n",
       "       ['2'],\n",
       "       ['1'],\n",
       "       ['2']], dtype=object)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test_new.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[23665,     0],\n",
       "        [    0, 20285]],\n",
       "\n",
       "       [[32678,     0],\n",
       "        [    0, 11272]],\n",
       "\n",
       "       [[31557,     2],\n",
       "        [    0, 12391]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(Y_test, onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00     20285\n",
      "           0       1.00      1.00      1.00     11272\n",
      "           2       1.00      1.00      1.00     12391\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     43948\n",
      "   macro avg       1.00      1.00      1.00     43948\n",
      "weighted avg       1.00      1.00      1.00     43948\n",
      " samples avg       1.00      1.00      1.00     43948\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classes = {'0','1','2'}\n",
    "print(classification_report(Y_test, onehot_encoded,target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.9999894377726375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "classes = {'0','1','2'}\n",
    "\n",
    "# set plot figure size\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "\n",
    "# function for scoring roc auc score for multi-class\n",
    "def multiclass_roc_auc_score(Y_test, onehot_encoded, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(Y_test)\n",
    "    Y_test = lb.transform(Y_test)\n",
    "\n",
    "    for (idx, c_label) in enumerate(classes):\n",
    "        fpr, tpr, thresholds = roc_curve(Y_test[:,idx].astype(int), onehot_encoded[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    return roc_auc_score(Y_test, onehot_encoded, average=average)\n",
    "\n",
    "\n",
    "print('ROC AUC score:', multiclass_roc_auc_score(Y_test, onehot_encoded))\n",
    "\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA8GElEQVR4nO3dd3wVZdbA8d9JqCpNAQuwggYREhJKaLL0DhoWl7osiBTFsqvrWlBXBNR9LYi7q74L+FIsKMWVoiKwKCxKB0EWECmC0tQQIBBqynn/mMnlJrmZ3ITcFDjfzyef3Lnz3Jlnnpm5Z+aZO2dEVTHGGGOyE1bYFTDGGFO0WaAwxhjjyQKFMcYYTxYojDHGeLJAYYwxxpMFCmOMMZ4sUOQDERkoIkuCKDdRRJ4piDqFmogMEZGv/IZVRCKyKXutiKwQkZMi8moB1vFXIpIkIuHZjB8jIu8FU9ZcOkSkrYgcCMF0W4nId/k93aLgkg8UIrJPRM64XwI/i8h0EbkqP+ehqjNUtXMQ5Uaq6nP5OW/wfeElu8t4XERWiUiL/J7PRbgHOAKUV9U/X+zE3CClIvJapvd7uu9PB1DVH1X1KlVNzWmaOZX1245O+rXxSBEJah8SkZpu3UoEUz6vgplPpu0l/e/xi5yvL+gWFBEZJiI73HXys4gsFJFyBTj/DAdHqvqlqtYJ0bwmi8h3IpImIkNCMQ8vl3ygcN2hqlcBjYBY4C+ZC4R6By4As9xlrAwsA+YUcn383Qhs1zzc3emxXvYAfTONvwvYmYf6BesOVS2HszwvAk8AU0I4v1Ca5QbG9L+XC7Myud3/RKQN8FdggLtO6gKzQlG3IuIb4H7g68KY+eUSKABQ1YPAZ0AU+I4IHhCRXcAu973bRWSz31FjdPrnRaSGiHwkIvEikiAib7jv+7phxPGaiPwiIidE5L8ikj6/6SLyvN/0RojIbhE5KiILROQGv3HqHrHucuvypohIEMuYAswAqolIFXdaFURkiogcFpGDIvK8fxeLW49v3SOz7SLSyH1/lIjs8Xu/V27b3D26vwt43D1y7SgipUXkbyJyyP37m4iUdsu3FZEDIvKEiPwETMtm0j8B/wW6uJ+7GrgNWOA37wxH1yJSS0T+4y7Pv3GCasCyXlQ1UVUXAP2Au/zWbw8R2eSu9/0iMsbvYyvc/8fddmghIjeLyBfutnRERGaISEW/Oj3hrq+T7tFkB/f9ML91kyAis93lDzifnJbHn4gMdbeFYyKyWERu9Bv3d3e5TojIRhFp5b7fFXgK6OfO8xv3/X0i0tHv8/5dfentPUxEfgS+yGn+mTQBVqvqJnedHFXVt1X1pDud0iIyXkR+FOdsY6KIlM1mmW8QkX+Js1/vFZE/+o0LF5Gn/PaDjeJ8D6S38zfuMveTTF1aIlJXRJa7++82EYnzGzddnH36U3e6a0Xk5uzWi6q+qaqfA2ezKxNKl1WgEJEaQHdgk9/bvwGaAfVEpCEwFbgXuAaYBCxwN7pw4BPgB6AmUA2YGWA2nYHWwC1ABaAvkBCgLu2B/3HHX+9ON/P0bsfZIaLdcl2CWMZSwGB3nsfct6cDKUAE0NCt43C3fB9gjPuZ8kCcX333AK3c5RgLvCci1+dUB3+qOgQncL3sHrkuBZ4GmgMNgBigKRnP8q4DrsY5cr/HY/LvuPUG6A/MB855lH8f2IgTIJ7DCWB5pqrrgAM4bQRwyq1PRaAHcJ+I/MYd19r9X9Fth9WA4GwDN+AcEdfAWReISB3gQaCJe8TcBdjnTuMPONttG/ezx4A3PeYTFBHpifOFfydQBfgS+MCvyHqcdXY1TlvOEZEyqroI5+g+/SwlJth5ustQF+gSxPz9rXU/M1ZEWqYfaPh5EWcfbICz3VcDRgdY5jDgY5wj9mpAB+BhEUnf1x4BBuB8b5QHhgKnVTW9nWPcZZ6Vabol3ekuAarirLMZ7npN1x9nv6oE7AZeyGZZC5+qXtJ/ODtXEnAc58v4f4Gy7jgF2vuV/SfwXKbPf4ezMbcA4oESAeYxBPjKfd0ep/ujORCWqdx04Hn39RScL8/0cVcByUBNv7r92m/8bGBUNss4BjjvLmMqzhd9W3fctThfnmX9yg8AlrmvFwMPBdmWm4GemZfZr74R2XzOt9zu8B6gu99wF2Cf+7qtuyxlPOoxBPgKKAv8jBPI1gAtgeeB6W65mm69SgC/wgmWV/pN533gvcxlPbajjgHeXwM8nc1n/ga8Fsz03TK/ATa5ryOAX4COQMlM5b4FOvgNX+9uOyWCnI//9pL+dwPO2fYwv3JhwGngxmymcwznizJ9mu95tZl/Gb963uQ3Prfz74bzZXwcZx+fAITjBOBTwM1+ZVsAe/22sQPu62bAj5mm+yQwTS/s/z2zmX+GbT7TdFvhnPWG+Y3/ABjjt0/8n9+47sCOIPbBr4Ahweyv+fl3uZxR/EZVK6rqjap6v6qe8Ru33+/1jcCf3VPF4yJyHOco7wb3/w/qdO1kS1W/AN7AOcL7RZyLUOUDFL0BJ3Clfy4J5wu+ml+Zn/xen8YJJtmZraoVcQLDVqCx3zKVBA77LdMknKMc3OXaE2iCIjJYLnTDHcfpsqscqGwuZVh29/UNfsPxqprjKba7Hj/FORu5RlVX5jDPY6p6KtN8L1Y14CiAiDQTkWVuF0YiMBKP9hLn12Az3e6lE8B76eVVdTfwMM6X6y9uufQ2uhGY67devsU5QLg2F/We7e4T6X+H3On+3W+6R3G+dKu59X3U7RZKdMdX8Fq+IGXe/7Kdf2aq+pmq3oFzhtMT5wBiOM7ZyBXARr9pLXLfz+xG4IZM+/xTXGjLbPePHNwA7FfVNL/3fiDv+3ehulwChRf/C6z7gRcy7UBXqOoH7rhfSXB92P9Q1cZAPZzT38cCFEvfMQEQkStxursOXsSyoKpHcLprxrjdRPtxzigq+y1TeVWNdD+yH8jSN+r2Db+F0/1xjRuEtuLsuBcrw7LjHO0f8l+MXEzrHeDPOF+yXg4Dldx29p9vnolIE5wdP/1nwu/jXCOpoaoVgIlcaK9Ay/RX9/36qloe+L1feVT1fVX9NU5bKfCSO2o/0C3TdlpGnWtwF5MOej9wb6bpllXVVe71iMdxukArudtDYg7LdwrnCzvddQHKZN7/As7fq9KqmqZO//0XOAczR4AzQKTfdCqo82OPQMu8N9M8y6lqd7/x2V478HAIqCEZfxX3Ky5y/y4sFigyegsY6R4ZiohcKc4FynLAOpwvmxfd98uISMvMExCRJu7nS+LsKGeBtMzlcE5D7xaRBm7/6l+Btaq672IXQlW/w+lSelxVD+P0k74qIuXFuRB6szi/GgH4P+BREWnsLnOEGySuxNmJ493luhv3RwD54APgLyJSRUQq4/Qd5/Wnlf8BOgGvexVS1R+ADcBYESklIr8G7sjLDN12vB3nmtJ7qvpfd1Q54KiqnhWRpsDv/D4Wj7Md3OT3XjmcLpNEEamG3wGFiNQRkfbutnEW54svfTuaCLzgrifcduzpMZ9gTQSeFJFId7oV3GtY6XVNcadfQkRG4/TZp/sZqJnpi3Ez0F9ESopILND7IuafgTg/he4vIpXc7bYpThfxGvco/i3gNRGp6pav5nfdwd864KQ4PxwoK87F6yj3IACc/eM5EantzidaRK7xW+bs2nktzlnC4+7yt8XZ3gJd18yRu82WwQnMJd3vnwL7/rZA4UdVNwAjcLqOjuFcYBrijkvFWdERwI84FzH7BZhMeZyN9BjOqWYC8EqAeS0FngH+hROAbsa5uJVfXgHucXeUwUApYLtbrw9x+rVR1Tk4F9HeB04C84CrVXU78CqwGmeHqA94de3kxvM4X9pbcH659LX7Xq6p43NVPRpE8d/h9EkfBZ7FORvJjY9F5CTOUebTOH3id/uNvx8Y55YZjXNdKb2ep3HaeaXbxdEc50JmI5wj80+Bj/ymVRrnguwRnC6Kqjh95wB/xzlzWeLOa427XNnNJyiqOhfnrGWm2xW2Fec6ADgHHotwrr/9gBO8/LuN0n+OnSAi6T/hfAZnuz7mLuv7FzH/zI7h7Ku7gPRuu1dUdYY7/gmc/XeNO62lQJZ7HNz9+naci957cdr7/3C61cBZx7NxDrZO4FxbTP/11Bjgbbed+2aa7nmc74tu7jT/Fxisqju82sDDEpyDhduAye7r1p6fyEfiXiAxxhhjArIzCmOMMZ4sUBhjjPFkgcIYY4wnCxTGGGM8FbtEeJUrV9aaNWsWdjWMMaZY2bhx4xFVDXTTYY6KXaCoWbMmGzZsKOxqGGNMsSIiec5EYF1PxhhjPFmgMMYY48kChTHGGE8WKIwxxniyQGGMMcaTBQpjjDGeQhYoRGSqOM+N3prNeBGRf4jzzOgt4j6n2RhjTNESyvsopuOk684ulXM3oLb71wznMaTNcppoWloaaWedh58lp6aRmqaUKRkOwPmUNNI078PnUlJRxXMYoHSJIIZFOJucisiF8YGGw0QoVSIsz8PhYULJ8JyHVZVzKWlZhkuEh1HCHT7vjvcaLhEeRniYkJamJKde/HDJ8DDCwoTUNCXFYzgtTUlOU0qFiW98oOHS4YKIkJKaRopy0cNl3LZOTk0jNZhhd1vJvG0mpzrbWvq6P5+ShnJxw4BvWziXkoogwQ+npmUZHybBbUuBhs+lpBIuzraSeVhVOZ+aluNwiTBn28gwHGC8sy3kfrhkmP+2lP1wapqSktthVUqFhyFyYXx2wympaaT6lc9uOH1d5zScedvKbvhihCxQqOoKEanpUaQn8I46ec7XiEhFEbnefdBOts5v/5bvGjTMz6oaY4zxUJh3Zlcj44NPDrjvZQkUInIPzuM9qVG5DMvblgQgTUHTlPBwcYcVTSPjsEJ4WJDDaU7UDbvI4XBxhlM167AAYXkdTlNEMg8L7uyzDKekKWEew6nusOTHsDr1zW5Y1Wnv3A6Hi+BOjtQ0JTwsuOE0d334ht3plfCt+9wPq6rftpN5OJfbWoi2vWCGRfN/28tpW8vNsG9dFoFtD69ht/Kqmba1EGx73ttigG2LC9876eP5jjwrFik8VHUyzlOdiI2N1fsmWgoPY4zJjfum5f1x94X5q6eDQA2/4eoU0wePG2PMpawwA8UCYLD766fmQGJO1yeMMcYUvJB1PYnIB0BboLKIHMB5mH1JAFWdCCwEuuM8AP00GR9Sb4wxpogI5a+eBuQwXoEHQjV/Y4wx+cPuzDbGGOPJAoUxxhhPFiiMMcZ4skBhjDHGkwUKY4wxnixQGGOM8WSBwhhjjCcLFMYYYzxZoDDGGOPJAoUxxhhPFiiMMcZ4skBhjDHGkwUKY4wxnixQGGOM8WSBwhhjjCcLFMYYYzxZoDDGGOPJAoUxxhhPFiiMMcZ4skBhjDHGkwUKY4wxnixQGGOM8WSBwhhjjCcLFMYYYzxZoDDGGOPJAoUxxhhPFiiMMcZ4skBhjDHGkwUKY4wxnixQGGOM8WSBwhhjjCcLFMYYYzyFNFCISFcR+U5EdovIqADjfyUiy0Rkk4hsEZHuoayPMcaY3AtZoBCRcOBNoBtQDxggIvUyFfsLMFtVGwL9gf8NVX2MMcbkTSjPKJoCu1X1e1U9D8wEemYqo0B593UF4FAI62OMMSYPQhkoqgH7/YYPuO/5GwP8XkQOAAuBPwSakIjcIyIbRGRDfHx8KOpqjDEmG4V9MXsAMF1VqwPdgXdFJEudVHWyqsaqamyVKlUKvJLGGHM5C2WgOAjU8Buu7r7nbxgwG0BVVwNlgMohrJMxxphcCmWgWA/UFpFaIlIK52L1gkxlfgQ6AIhIXZxAYX1LxhhThIQsUKhqCvAgsBj4FufXTdtEZJyIxLnF/gyMEJFvgA+AIaqqoaqTMcaY3CsRyomr6kKci9T+7432e70daBnKOhhjjLk4hX0x2xhjTBFngcIYY4wnCxTGGGM8WaAwxhjjyQKFMcYYTxYojDHGeLJAYYwxxpMFCmOMMZ4sUBhjjPFkgcIYY4wnCxTGGGM8WaAwxhjjyQKFMcYYTxYojDHGeLJAYYwxxpMFCmOMMZ4sUBhjjPFkgcIYY4wnCxTGGGM8WaAwxhjjyQKFMcYYTxYojDHGeLJAYYwxxpMFCmOMMZ4sUBhjjPFkgcIYY4wnCxTGGGM8WaAwxhjjyQKFMcYYTxYojDHGeLJAYYwxxpMFCmOMMZ6CChQi0lJE/i0iO0XkexHZKyLfB/G5riLynYjsFpFR2ZTpKyLbRWSbiLyf2wUwxhgTWiWCLDcF+BOwEUgN5gMiEg68CXQCDgDrRWSBqm73K1MbeBJoqarHRKRqbipvjDEm9IINFImq+lkup90U2K2q3wOIyEygJ7Ddr8wI4E1VPQagqr/kch7GGGNCLNhAsUxEXgE+As6lv6mqX3t8phqw32/4ANAsU5lbAERkJRAOjFHVRUHWyRhjTAEINlCkf8HH+r2nQPt8mH9toC1QHVghIvVV9bh/IRG5B7gH4Fe/+tVFztIYY0xuBBUoVLVdHqZ9EKjhN1zdfc/fAWCtqiYDe0VkJ07gWJ9p/pOByQCxsbGah7oYY4zJo2B/9VRBRCaIyAb371URqZDDx9YDtUWkloiUAvoDCzKVmYdzNoGIVMbpisrx11TGGGMKTrD3UUwFTgJ93b8TwDSvD6hqCvAgsBj4FpitqttEZJyIxLnFFgMJIrIdWAY8pqoJuV8MY4wxoSKqOffkiMhmVW2Q03sFITY2Vjds2FDQszXGmGJNRDaqamzOJbMK9ozijIj82m+GLYEzeZmhMcaY4iXYXz3dB7ztXpcQ4CgwJFSVMsYYU3QE+6unzUCMiJR3h0+EslLGGGOKDs9AISK/V9X3ROSRTO8DoKoTQlg3Y4wxRUBOZxRXuv/LhboixhhjiibPQKGqk9z/YwumOsYYY4qaYG+4e1lEyotISRH5XETiReT3oa6cMcaYwhfsz2M7uxewbwf2ARHAY6GqlDHGmKIj2ECR3kXVA5ijqokhqo8xxpgiJtj7KD4RkR04N9ndJyJVgLOhq5YxxpiiIqgzClUdBdwGxLqZXk/hPITIGGPMJS6n+yjaq+oXInKn33v+RT4KVcWMMcYUDTl1PbUBvgDuCDBOsUBhjDGXvJzuo3jW/X93wVTHGGNMURPsfRR/FZGKfsOVROT5kNXKGGNMkRHsz2O7+T/HWlWPAd1DUiNjjDFFSrCBIlxESqcPiEhZoLRHeWOMMZeIYO+jmAF8LiLpjz+9G3g7NFUyxhhTlAT7PIqXROQboKP71nOqujh01TLGGFNUBHtGAfAtkKKqS0XkChEpp6onQ1UxY4wxRUOwv3oaAXwITHLfqgbMC1GdjDHGFCHBXsx+AGgJnABQ1V1A1VBVyhhjTNERbKA4p6rn0wdEpATOndnGGGMuccEGiv+IyFNAWRHpBMwBPg5dtYwxxhQVwQaKJ4B44L/AvcBC4C+hqpQxxpiiI8dfPYlIOLBNVW8F3gp9lYwxxhQlOZ5RqGoq8J2I/KoA6mOMMaaICfY+ikrANhFZh/PQIgBUNS4ktTLGGFNkBBsonglpLYwxxhRZOT3hrgwwEojAuZA9RVVTCqJixhhjioacrlG8DcTiBIluwKshr5ExxpgiJaeup3qqWh9ARKYA60JfJWOMMUVJTmcUyekvrMvJGGMuTzkFihgROeH+nQSi01+LyImcJi4iXUXkOxHZLSKjPMr9VkRURGJzuwDGGGNCy7PrSVXD8zph90a9N4FOwAFgvYgsUNXtmcqVAx4C1uZ1XsYYY0In2BQeedEU2K2q37sJBWcCPQOUew54CTgbwroYY4zJo1AGimrAfr/hA+57PiLSCKihqp96TUhE7hGRDSKyIT4+Pv9raowxJluhDBSeRCQMmAD8OaeyqjpZVWNVNbZKlSqhr5wxxhifUAaKg0ANv+Hq7nvpygFRwHIR2Qc0BxbYBW1jjClaQhko1gO1RaSWiJQC+gML0keqaqKqVlbVmqpaE1gDxKnqhhDWyRhjTC6FLFC49108CCwGvgVmq+o2ERknIpZM0BhjiolgkwLmiaouxHnIkf97o7Mp2zaUdTHGGJM3hXYx2xhjTPFggcIYY4wnCxTGGGM8WaAwxhjjyQKFMcYYTxYojDHGeLJAYYwxxpMFCmOMMZ4sUBhjjPEU0juzL2fJyckcOHCAs2ftMRvGmIJTpkwZqlevTsmSJfNtmhYoQuTAgQOUK1eOmjVrIiKFXR1jzGVAVUlISODAgQPUqlUr36ZrXU8hcvbsWa655hoLEsaYAiMiXHPNNfnek2GBIoQsSBhjCloovncsUBhjjPFkgeIyt2DBAl588cXCrkahmz59OlWqVKFBgwbUq1ePt956K1+me9ttt3mO7969O8ePH8+XeYXC9OnTefDBBwEYM2YM48ePL+QaZTVv3jzGjRtX2NXI1pw5c4iMjCQsLIwNG7J/LtuiRYuoU6cOERERGfbJvXv30qxZMyIiIujXrx/nz58H4I033mDq1Kkhrz9YoLjsxcXFMWrUqKDKqippaWkhrlH2UlJSQjr9fv36sXnzZpYvX85TTz3Fzz//fNHzX7Vqlef4hQsXUrFixVxP10thr6fcyI91+vLLL3P//fcX6DxzIyoqio8++ojWrVtnWyY1NZUHHniAzz77jO3bt/PBBx+wfft2AJ544gn+9Kc/sXv3bipVqsSUKVMAGDp0KK+//nqBLIMFigLSb9Jq5mzYD0Byahr9Jq1m7qYDAJw5n0q/Sav5+JtDAJw4m0y/SatZtPUwAEdPnaffpNUs3e58cf1yMucLVfv27ePWW29lyJAh3HLLLQwcOJClS5fSsmVLateuzbp164CMR4w///wzvXr1IiYmhpiYGFatWsW+ffuoU6cOgwcPJioqiv379/PYY48RFRVF/fr1mTVrVsD5r1u3jhYtWtCwYUNuu+02vvvuOwCaN2/Otm3bfOXatm3Lhg0bOHXqFEOHDqVp06Y0bNiQ+fPn++oXFxdH+/bt6dChA0lJSXTo0IFGjRpRv359XzmA5557jjp16vDrX/+aAQMG+I5+9+zZQ9euXWncuDGtWrVix44dnm1XtWpVbr75Zn744QeGDBnCyJEjadasGY8//ni20wrUdgBXXXUVAIcPH6Z169Y0aNCAqKgovvzySwBq1qzJkSNHAJgwYQJRUVFERUXxt7/9zbce69aty4gRI4iMjKRz586cOXMm4PrOvJ5eeeUVmjRpQnR0NM8++6yv7DvvvEN0dDQxMTEMGjQIgI8//phmzZrRsGFDOnbsmCVIesluu4mKivKVGT9+PGPGjAGcdf7www8TGxvLCy+8wI033ugLbKdOnaJGjRokJycHtd527txJ6dKlqVy5sudyjBkzhkGDBtGyZUsGDRpEfHw8v/3tb2nSpAlNmjRh5cqVQPbb7cWoW7cuderU8Syzbt06IiIiuOmmmyhVqhT9+/dn/vz5qCpffPEFvXv3BuCuu+5i3rx5AFxxxRXUrFnTty+HlKoWq7/GjRtrcbB9+/YMw30nrtLZ639UVdXzKanad+Iq/ejr/aqqevpcivaduEoXbD6oqqqJZ85r34mr9LP/HlJV1YSkc9p34ir997afVFX15xNncpz/3r17NTw8XLds2aKpqanaqFEjvfvuuzUtLU3nzZunPXv2VFXVadOm6QMPPODUsW9ffe2111RVNSUlRY8fP6579+5VEdHVq1erquqHH36oHTt21JSUFP3pp5+0Ro0aeujQoSzzT0xM1OTkZFVV/fe//6133nmnqqpOmDBBR48eraqqhw4d0ltuuUVVVZ988kl99913VVX12LFjWrt2bU1KStJp06ZptWrVNCEhQVVVk5OTNTExUVVV4+Pj9eabb9a0tDRdt26dxsTE6JkzZ/TEiRMaERGhr7zyiqqqtm/fXnfu3KmqqmvWrNF27dplqa9/O+zZs0erVKmiCQkJetddd2mPHj00JSXFc1qB2k5V9corr1RV1fHjx+vzzz/vG3/ixAlVVb3xxhs1Pj5eN2zYoFFRUZqUlKQnT57UevXq6ddff+1bj5s2bVJV1T59+vjaKfP69l9Pixcv1hEjRmhaWpqmpqZqjx499D//+Y9u3bpVa9eurfHx8aqqvnY9evSopqWlqarqW2+9pY888kiWdnn22Wd9beovu+0mMjLSV+aVV17RZ599VlVV27Rpo/fdd59vXFxcnH7xxReqqjpz5kwdNmxY0Ott6tSpvrp6Lcezzz6rjRo10tOnT6uq6oABA/TLL79UVdUffvhBb731VlXNfrv1d+LECY2JiQn4t23btizl07Vp00bXr18fcNycOXN8y62q+s477+gDDzzg28bT/fjjjxna9fnnn9fx48dnmV7m7x9VVWCD5vF71+6jKCCz7m3he10yPCzDcNlS4RmGy5cpmWH46itLZRiuWq5MUPOsVasW9evXByAyMpIOHTogItSvX599+/ZlKf/FF1/wzjvvABAeHk6FChU4duwYN954I82bNwfgq6++YsCAAYSHh3PttdfSpk0b1q9fT1xcxsegJyYmctddd7Fr1y5EhOTkZAD69u1L586dGTt2LLNnz/YdKS1ZsoQFCxb4zgLOnj3Ljz/+CECnTp24+uqrAefA5qmnnmLFihWEhYVx8OBBfv75Z1auXEnPnj0pU6YMZcqU4Y477gAgKSmJVatW0adPH1/dzp07F7C9Zs2axVdffUXp0qWZNGmSb559+vQhPDzcc1qB2s5fkyZNGDp0KMnJyfzmN7+hQYMGGcZ/9dVX9OrViyuvvBKAO++8ky+//JK4uDhq1arlK9+4ceOA6w7IsJ6WLFnCkiVLaNiwoa8ddu3axTfffEOfPn18R+Dpy3jgwAH69evH4cOHOX/+fK5+g5/dduOlX79+GV7PmjWLdu3aMXPmTO6///6g19vhw4epUqWKb9hrOeLi4ihbtiwAS5cu9XXtAJw4cYKkpKRst1t/5cqVY/PmzTm0SsGoWrVqjmfI+cECxSWsdOnSvtdhYWG+4bCwsFz106Z/eXl58803fReAFy5cyDPPPEO7du2YO3cu+/bto23btgBUq1aNa665hi1btjBr1iwmTpwIOAHgX//6V5ZT9LVr12aY/4wZM4iPj2fjxo2ULFmSmjVrev5mPC0tjYoVKwa1Y/fr14833ngjy/vp88/NtDJr3bo1K1as4NNPP2XIkCE88sgjDB48OKjP+q/H8PBwzpw5w/79+33BcOTIkXTt2jVDO6kqTz75JPfee2+GaWXXp/2HP/yBRx55hLi4OJYvX+7rJsqrEiVKZLhOknkd+dc1Li6Op556iqNHj7Jx40bat2/PqVOngmrrsmXLkpiYGNRy+M8zLS2NNWvWUKZMxoOuBx98MOB26+/kyZO0atUqYH3ef/996tWr51nnQKpVq8b+/ft9wwcOHPDtK8ePHyclJYUSJUr43k939uxZX/ALJbtGYXw6dOjAP//5T8C5uOa/A6Zr1aoVs2bNIjU1lfj4eFasWEHTpk154IEH2Lx5M5s3b+aGG24gMTHRt0FPnz49wzT69evHyy+/TGJiItHR0QB06dKF119/HecMGTZt2hSwjomJiVStWpWSJUuybNkyfvjhBwBatmzJxx9/zNmzZ0lKSuKTTz4BoHz58tSqVYs5c+YAzhfoN998k6f28ZpWTm33ww8/cO211zJixAiGDx/O119/nWF8q1atmDdvHqdPn+bUqVPMnTs32y8jgBo1avjae+TIkVnGd+nShalTp5KUlATAwYMH+eWXX2jfvj1z5swhISEBgKNHjwJkWF9vv/12rtol0LJfe+21/PLLLyQkJHDu3Dnf+gjkqquuokmTJjz00EPcfvvthIeHB73e6taty+7du33DwS5H586dMwTN9IDktd2mSz+jCPSXlyABzhnnrl272Lt3L+fPn2fmzJnExcUhIrRr144PP/zQt0w9e/b0fW7nzp0ZrgWFigUK4/P3v/+dZcuWUb9+fRo3bpzh1Dxdr169fBdC27dvz8svv8x1112Xpdzjjz/Ok08+ScOGDbOcvfTu3ZuZM2fSt29f33vPPPMMycnJREdHExkZyTPPPBOwjgMHDmTDhg3Ur1+fd955h1tvvRVwdrS4uDiio6Pp1q0b9evX93X/zJgxgylTphATE0NkZGSGC+C5ld20cmq75cuXExMTQ8OGDZk1axYPPfRQhvGNGjViyJAhNG3alGbNmjF8+HBft1FedO7cmd/97ne0aNGC+vXr07t3b06ePElkZCRPP/00bdq0ISYmhkceeQRwLvb26dOHxo0b+7qlghVo2UuWLMno0aNp2rQpnTp18q2n7PTr14/33nsvQ5dUMOutdevWbNq0yXeAEexy/OMf/2DDhg1ER0dTr14935mt13abV3PnzqV69eqsXr2aHj160KVLFwAOHTpE9+7dAecM7I033qBLly7UrVuXvn37EhkZCcBLL73EhAkTiIiIICEhgWHDhvmmvXLlSjp16pQv9fQi6Q1cXMTGxqrXb5GLim+//Za6desWdjUuK0lJSVx11VWcPn2a1q1bM3nyZBo1alTY1TIh9tBDD3HHHXfQsWPHwq5Kgdq0aRMTJkzg3XffzTIu0PePiGxU1di8zMvOKMwl45577qFBgwY0atSI3/72txYkLhNPPfUUp0+fLuxqFLgjR47w3HPPFci87GK2uWS8//77hV0FUwiuvfbaLL+6uxwURJdTOjujMMYY48kChTHGGE8WKIwxxniyQGGMMcaTBYpLWHh4uC8JXZ8+ffLllyGjR49m6dKl2Y6fOHGiL51DUZWeqC9z4rqi7h//+Ad169Zl4MCBQZVPT7gYCm+//Ta1a9emdu3anje29e7dm++//z4kdcgPQ4cOpWrVqp7bgaryxz/+kYiICKKjozPcLJldO3Ts2DHHNCbFSl6TRBXWX3FNClgY0hPSqar+7ne/01dffTXD+PTkZ8VBftY1vV0yJ67LD6Fs0zp16uj+/fuDLu+VhO5iJCQkaK1atTQhIUGPHj2qtWrV0qNHj2Ypt3XrVv3Nb36Tq2mnJ18sKP/5z39048aNntvBp59+ql27dtW0tDRdvXq1Nm3aVFW922H69Om+JJCFIb+TAtoZRUH4bBRM65G/f58F9wyJdK1atWL37t0sX76cVq1aERcXR7169UhNTeWxxx7zpaOeNGmS7zMvvfQS9evXJyYmxvfMiiFDhvjSCYwaNYp69eoRHR3No48+CmR8uM3mzZtp3rw50dHR9OrVy3eE1bZtW5544gmaNm3KLbfc4ku5nZl/Ouq///3vbNy4kTZt2tC4cWO6dOnC4cNOGvbdu3fTsWNHYmJiaNSoEXv27PFMRx6MQMvuf4R+5MgRatasCWRNhd6/f38+/fRT37TS28yrrf0FSjc+cuRIvv/+e7p168Zrr72WoXxqaiqPPvooUVFRREdHB8zndN999xEbG0tkZGSGlOOB1uGcOXOIiooiJiYm4DMUFi9e7EvUWKlSJTp16sSiRYuylJsxY0aGdBPZ1aFmzZo88cQTNGrUiDlz5rBkyRJatGhBo0aN6NOnjy8Nybhx42jSpAlRUVHcc889vruxL0br1q19iRGzM3/+fAYPHoyI0Lx5c44fP87hw4c92yEuLo4PPvjgoutXVIT0PgoR6Qr8HQgH/k9VX8w0/hFgOJACxANDVfWHUNbpcpSSksJnn31G165dAfj666/ZunUrtWrVYvLkyVSoUIH169dz7tw5WrZsSefOndmxYwfz589n7dq1XHHFFb6cQOkSEhKYO3cuO3bsQEQCPqVt8ODBvP7667Rp04bRo0czduxY3xdfSkoK69atY+HChYwdOzbb7qzz58+zYcMGkpOTadOmDfPnz6dKlSrMmjWLp59+mqlTpzJw4EBGjRpFr169OHv2LGlpaZQqVYq5c+dSvnx5jhw5QvPmzX25c3Ly2WefeS57IF9//TVbtmzh6quvZu7cucyePZsePXpw/vx5Pv/8c/75z38yZcqUgG3tn+F048aNTJs2jbVr16KqNGvWjDZt2jBx4kQWLVrEsmXLsqSmmDx5Mvv27WPz5s2UKFEiYH1feOEFrr76alJTU+nQoQNbtmyhWrVqAdfhuHHjWLx4MdWqVQu4Xg8ePEiNGjV8w9WrV+fgwYNZyq1cuZIBAwZ41iE919c111zD119/zZEjR7jzzjtZunQpV155pS99xejRo3nwwQcZPXo0AIMGDeKTTz7xJUZMN2PGDF555ZUsdYmIiPAd4ORWdsvr1Q6VKlXi3LlzJCQkcM011+RpvkVJyAKFiIQDbwKdgAPAehFZoKr+SXA2AbGqelpE7gNeBvplnVox161wHjV65swZX3rqVq1aMWzYMFatWkXTpk19X05Llixhy5Ytvp0oMTGRXbt2sXTpUu6++26uuOIKgCxHXRUqVKBMmTIMGzaM22+/ndtvvz3D+MTERI4fP06bNm0A54Er/imj77zzTsA7bTZcSEf93XffsXXrVt9NRqmpqVx//fWcPHmSgwcP0qtXLwBfNtDk5OSA6cgD5aXKLKdlD8Q/FXq3bt146KGHOHfuHIsWLaJ169aULVs227b2DxTZpRv3yvu0dOlSRo4cSYkSJbKt7+zZs5k8eTIpKSkcPnyY7du3U69evYDrsGXLlgwZMoS+ffv61lNeZE4BHqgO6YEifT2vWbOG7du307JlS8A5UGjRwkmxv2zZMl5++WVOnz7N0aNHiYyMzBIoBg4cGPQ1nFCrWrUqhw4dskCRg6bAblX9HkBEZgI9AV+gUNVlfuXXAL8PYX0uO2XLlg2YpjlzOurXX3/dl6gs3eLFiz2nXaJECdatW8fnn3/Ohx9+yBtvvMEXX3wRdN3SU2eHh4f7kq/dfffdbNq0iRtuuIGFCxdmqKuqEhkZyerVqzNM5+TJkwGnn9t05MHwT53tlTa7TJkytG3blsWLFzNr1iz69+/vW4ZAbR1qe/fuZfz48axfv55KlSoxZMgQzp49m+06nDhxImvXruXTTz+lcePGbNy4McOXXbVq1Vi+fLlv+MCBAwHTcZctW9bXTtnVIZ3/eu7UqVOWbpuzZ89y//33s2HDBmrUqMGYMWMCrs9QnFFklwI8p3YoqBTgBSGU1yiqAfv9hg+472VnGPBZoBEico+IbBCRDfHx8flYRdOlSxf++c9/+h7QsnPnTk6dOkWnTp2YNm2a75dSmbsz0h/y0r17d1577bUsKaArVKhApUqVfNcf3n33Xd/ZRXamTZvG5s2bfUHCX506dYiPj/cFiuTkZLZt20a5cuWoXr267/GQ586d4/Tp09mmIw9Gdstes2ZNNm7cCJDjl06/fv2YNm0aX375pa/LL7u29pfbdOPp9Z00aZIv4GZeVydOnODKK6+kQoUK/Pzzz3z2mbObZbcO9+zZQ7NmzRg3bhxVqlTJ8CWZvhxLlizh2LFjHDt2jCVLlgQMfv4pwLOrQ2bNmzdn5cqVvs+dOnWKnTt3+oJC5cqVSUpKyrb9Bw4cGDD9d16DBDjXG9555x1UlTVr1lChQgWuv/56z3ZQVX766SffdazirkjkehKR3wOxQMBvElWdDEwGJ3tsAVbtkjd8+HD27dtHo0aNUFWqVKnCvHnz6Nq1K5s3byY2NpZSpUrRvXt3/vrXv/o+d/LkSXr27MnZs2dRVSZMmJBl2m+//TYjR47k9OnT3HTTTUybNi3P9SxVqhQffvghf/zjH0lMTCQlJYWHH36YyMhI3n33Xe69915Gjx5NyZIlmTNnDgMHDuSOO+6gfv36xMbG5pjm2l92y/7oo4/St29fJk+eTI8ePTyn0blzZwYNGkTPnj0pVaoUkH1b+/NPN57+mZzSjQ8fPpydO3cSHR1NyZIlGTFihO856IAvvfmtt95KjRo1fN062a3Dxx57jF27dqGqdOjQgZiYmAzzu/rqq3nmmWdo0qQJ4PxkOlB3V48ePVi+fLnvhwaB6pBZlSpVmD59OgMGDPA90e7555/nlltuYcSIEURFRXHdddf55n2xBgwYwPLlyzly5AjVq1dn7NixDBs2zJd2fOTIkXTv3p2FCxcSERHBFVdc4duOvdph48aNNG/e3NcdWNyFLM24iLQAxqhqF3f4SQBV/Z9M5ToCrwNtVPWXnKZracaNKR7OnDlDu3btWLlyJeHh4YVdnQL10EMPERcXR4cOHQpl/sUpzfh6oLaI1BKRUkB/YIF/ARFpCEwC4oIJEsaY4qNs2bKMHTs24C+iLnVRUVGFFiRCIWTnRaqaIiIPAotxfh47VVW3icg4nBs/FgCvAFcBc9yfLf6oqpdfvmBjLlEFfeG+qBgxYkRhVyFfhbQDTVUXAgszvTfa7/Xl9UgqY4wphuzObGOMMZ4sUBhjjPFkgcIYY4wnCxSXMEszHpilGb94Xbt2pWLFillSt2T28MMPs2LFipDUIT88/fTT1KhRw7dNZOd//ud/iIiIoE6dOhmyFixatIg6deoQERHBiy9eSNXTv39/du3aFbJ6F7i8pp0trD9LMx48SzMemKUZv3hLly7VBQsWaI8ePbItc+TIEW3WrFmuplvQ2+Tq1av10KFDGfaVzLZt26bR0dF69uxZ/f777/Wmm27SlJQUTUlJ0Ztuukn37Nmj586d0+joaN22bZuqqi5fvlyHDx9eUIuRRX6nGb80bhss4l5a9xI7ju7I12neevWtPNH0iaDLt2rVii1btrB8+XKeeeYZKlWqxI4dO/j2228ZNWoUy5cv59y5czzwwAPce++9Tr1feon33nuPsLAwunXrxosvvsiQIUO4/fbb6d27N6NGjWLBggWUKFGCzp07M378eMaMGcNVV13Fo48+yubNm313Zt98881MnTqVSpUq0bZtW5o1a8ayZcs4fvw4U6ZMCZimom3btjRo0ICvvvqKAQMG0LZtWx555BGSkpKoXLky06dP5/rrr2f37t2MHDmS+Ph4wsPDmTNnDtdeey09e/bk2LFjJCcn8/zzz2dIeZ2TQMvetm1bxo8fT2xsLEeOHCE2NpZ9+/Yxffp0PvroI5KSknzJCgcNGuS7ezu9zXr16pVtW/ubMGECU6dOBZy7rh9++OEMacaHDh3Kn/70J1/51NRUnnjiCRYtWkRYWBgjRozgD3/4Q4Zp3nfffaxfv54zZ87Qu3dvxo4dCxBwHc6ZM4exY8cSHh5OhQoVAp4RdOjQIUOeo0D+9a9/+dKXgJOV9uOPP+bMmTPcdtttTJo0CREJej2/9dZbTJ48mfPnzxMREcG7777rS9yYV82bN8+xzPz58+nfvz+lS5emVq1aREREsG7dOsDJIXXTTTcBzlnE/PnzqVevHq1atWLIkCGkpKRcEndnF/8lMDmyNOOWZjw/04wHa+XKlfTu3ds37JUmPJj1fOedd/ruT/jLX/7ClClTsgTEZcuWZQii6a644gpWrVqVp+U4ePBghoDin048c5rxtWvXAhAWFkZERATffPMNjRs3ztN8ixILFAUgN0f++cnSjFua8XRFIc24V5rwnNYzwNatW/nLX/7C8ePHSUpKCngzX7t27QJmTC4M6WnGLVCYIs3SjFuaccj/NOPB8k8znlOa8JzWMzhdePPmzSMmJobp06cH7PoKxRlFdmnGgWzfB0szbi4hlmY8K0sz7p1mPFj+acaDTROe3XoG56Dg+uuvJzk5mRkzZgT8fPoZRea/vAYJcNKMz5w5k3PnzrF371527dpF06ZNadKkCbt27WLv3r2cP3+emTNnEhd3IQPRzp07i9Wv6rzYGcVlztKMZ2Vpxr3TjIMT0Hbs2EFSUhLVq1dnypQpWc6UevTowaRJkxg+fDgVK1YMKk2413p+7rnnaNasGVWqVKFZs2bZnk3mxuOPP87777/P6dOnqV69OsOHD2fMmDEsWLCADRs2MG7cOCIjI+nbty/16tWjRIkSvPnmm75suG+88QZdunQhNTWVoUOHEhkZCcDPP/9M2bJlg+rqLA5ClmY8VCzNuDHFx69//Ws++eQTKlasWNhVKVCvvfYa5cuXZ9iwYYUy/+KUZtwYc5l79dVX+fHHHwu7GgWuYsWK3HXXXYVdjXxjXU/GmJBp1qxZYVehUNx9992FXYV8ZWcUxhhjPFmgMMYY48kChTHGGE8WKIwxxniyQHEJszTjgVma8YuzefNmWrRoQWRkJNHR0cyaNSvbspZm/BKR17SzhfVnacaDZ2nGA7M04xfnu+++0507d6qq6sGDB/W6667TY8eOZSlnacYtzbjJhZ/++lfOfZu/acZL172V6556Kujylmbc0oznV5rxW265xff6hhtuoGrVqsTHx2e5qc7SjFuacVOMWJpxSzMeqjTj69at4/z589x8881ZxlmacUszbnIhN0f++cnSjFua8XShSDN++PBhBg0axNtvv01YWNbLnZZm3NKMm2LA0oxbmnEITZrxEydO0KNHD1544YVsu28szbilGTeXCEsznpWlGfdOM37+/Hl69erF4MGDM3QtZWZpxi3NuLlEWJrxrCzNuHea8dmzZ7NixQoSEhKYPn06ANOnT/d1c6azNOOWZrzQWJpxY4oPSzNuacaNMcaTpRm/NFjXkzEmZCzN+KXBzihCqLh16xljir9QfO9YoAiRMmXKkJCQYMHCGFNgVJWEhATf/UT5xbqeQqR69eocOHCA+Pj4wq6KMeYyUqZMGapXr56v07RAESIlS5bMcMetMcYUVyHtehKRriLynYjsFpFRAcaXFpFZ7vi1IlIzlPUxxhiTeyELFCISDrwJdAPqAQNEpF6mYsOAY6oaAbwGvBSq+hhjjMmbUJ5RNAV2q+r3qnoemAlkzvPcE3jbff0h0EGCSe9pjDGmwITyGkU1wD9JzAEg84+qfWVUNUVEEoFrgCP+hUTkHuAed/CciGwNSY2Ln8pkaqvLmLXFBdYWF1hbXFAnrx8sFhezVXUyMBlARDbk9Tb0S421xQXWFhdYW1xgbXGBiOQ591Eou54OAjX8hqu77wUsIyIlgApAQgjrZIwxJpdCGSjWA7VFpJaIlAL6AwsylVkApCdE6Q18oXaHmjHGFCkh63pyrzk8CCwGwoGpqrpNRMbhPOR7ATAFeFdEdgNHcYJJTiaHqs7FkLXFBdYWF1hbXGBtcUGe26LYpRk3xhhTsCzXkzHGGE8WKIwxxngqsoHC0n9cEERbPCIi20Vki4h8LiI3FkY9C0JObeFX7rcioiJyyf40Mpi2EJG+7raxTUTeL+g6FpQg9pFficgyEdnk7ifdC6OeoSYiU0Xkl+zuNRPHP9x22iIijYKasKoWuT+ci997gJuAUsA3QL1MZe4HJrqv+wOzCrvehdgW7YAr3Nf3Xc5t4ZYrB6wA1gCxhV3vQtwuagObgErucNXCrnchtsVk4D73dT1gX2HXO0Rt0RpoBGzNZnx34DNAgObA2mCmW1TPKCz9xwU5toWqLlPV0+7gGpx7Vi5FwWwXAM/h5A07W5CVK2DBtMUI4E1VPQagqr8UcB0LSjBtoUB593UF4FAB1q/AqOoKnF+QZqcn8I461gAVReT6nKZbVANFoPQf1bIro6opQHr6j0tNMG3hbxjOEcOlKMe2cE+la6jqpwVZsUIQzHZxC3CLiKwUkTUi0rXAalewgmmLMcDvReQAsBD4Q8FUrcjJ7fcJUExSeJjgiMjvgVigTWHXpTCISBgwARhSyFUpKkrgdD+1xTnLXCEi9VX1eGFWqpAMAKar6qsi0gLn/q0oVU0r7IoVB0X1jMLSf1wQTFsgIh2Bp4E4VT1XQHUraDm1RTkgClguIvtw+mAXXKIXtIPZLg4AC1Q1WVX3AjtxAselJpi2GAbMBlDV1UAZnISBl5ugvk8yK6qBwtJ/XJBjW4hIQ2ASTpC4VPuhIYe2UNVEVa2sqjVVtSbO9Zo4Vc1zMrQiLJh9ZB7O2QQiUhmnK+r7AqxjQQmmLX4EOgCISF2cQHE5Pqd4ATDY/fVTcyBRVQ/n9KEi2fWkoUv/UewE2RavAFcBc9zr+T+qalyhVTpEgmyLy0KQbbEY6Cwi24FU4DFVveTOuoNsiz8Db4nIn3AubA+5FA8sReQDnIODyu71mGeBkgCqOhHn+kx3YDdwGrg7qOlegm1ljDEmHxXVridjjDFFhAUKY4wxnixQGGOM8WSBwhhjjCcLFMYYYzxZoDAmABFJFZHNIrJVRD4WkYr5PP197r0NiEhSfk7bmPxmgcKYwM6oagNVjcK5T+eBwq6QMYXFAoUxOVuNmzhNRG4WkUUislFEvhSRW933rxWRuSLyjft3m/v+PLfsNhG5pxCXwZg8K5J3ZhtTVIhIOE7qhynuW5OBkaq6S0SaAf8LtAf+AfxHVXu5n7nKLT9UVY+KSFlgvYj861K8O9pc2ixQGBNYWRHZjHMm8S3wbxG5CriNC6lSAEq7/9sDgwFUNRUn7T3AH0Wkl/u6Bk5SPgsUplixQGFMYGdUtYGIXIGTQ+gBYDpwXFUbBDMBEWkLdARaqOppEVmOk4zOmGLFrlEY48F9cuAfcZLKnQb2ikgf8D1/OMYt+jnOY2gRkXARqYCT+v6YGyRuxUl7bkyxY4HCmByo6iZgC87DbwYCw0TkG2AbFx65+RDQTkT+C2zEeS7zIqCEiHwLvIiT9tyYYseyxxpjjPFkZxTGGGM8WaAwxhjjyQKFMcYYTxYojDHGeLJAYYwxxpMFCmOMMZ4sUBhjjPH0/6rsIfO0e6sfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_classes=3\n",
    "# Compute Precision-Recall and plot curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
    "                                                        onehot_encoded[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test[:, i], onehot_encoded[:, i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "    onehot_encoded.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test, onehot_encoded,\n",
    "                                                     average=\"micro\")\n",
    "\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "plt.clf()\n",
    "plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
    "         label='micro-average Precision-recall curve (area = {0:0.2f})'\n",
    "               ''.format(average_precision[\"micro\"]), linestyle=':')\n",
    "for i in range(n_classes):\n",
    "    plt.plot(recall[i], precision[i],\n",
    "             label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
    "                   ''.format(i, average_precision[i]))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall for Midi Dataset Feature Selection 1')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(5, activation='relu', input_shape=(21,)))\n",
    "    network.add(layers.Dense(3, activation='softmax'))\n",
    "    #\n",
    "    # Configure the network with optimizer, loss function and accuracy\n",
    "    #\n",
    "    network.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7144/3130689328.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_classifier, epochs=2, batch_size=20)\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model = KerasClassifier(build_fn=build_classifier, epochs=2, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5128/5128 [==============================] - 11s 2ms/step - loss: 0.0254 - accuracy: 0.9926\n",
      "Epoch 2/2\n",
      "5128/5128 [==============================] - 11s 2ms/step - loss: 1.5614e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdded73c490>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_std,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 2, 1, 2])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_std)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 0 ... 2 1 2]\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "integer_encoded = label_encoder.fit_transform(y_pred)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = pd.get_dummies(y_pred)\n",
    "y_pred.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     20285\n",
      "           2       1.00      1.00      1.00     11272\n",
      "           1       1.00      1.00      1.00     12391\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     43948\n",
      "   macro avg       1.00      1.00      1.00     43948\n",
      "weighted avg       1.00      1.00      1.00     43948\n",
      " samples avg       1.00      1.00      1.00     43948\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(Y_test, onehot_encoded)\n",
    "from sklearn.metrics import classification_report\n",
    "classes = {'0','1','2'}\n",
    "print(classification_report(Y_test, onehot_encoded,target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     20285\n",
      "           2       1.00      1.00      1.00     11272\n",
      "           1       1.00      1.00      1.00     12391\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     43948\n",
      "   macro avg       1.00      1.00      1.00     43948\n",
      "weighted avg       1.00      1.00      1.00     43948\n",
      " samples avg       1.00      1.00      1.00     43948\n",
      "\n",
      "[[[23665     0]\n",
      "  [    0 20285]]\n",
      "\n",
      " [[32678     0]\n",
      "  [    0 11272]]\n",
      "\n",
      " [[31557     2]\n",
      "  [    0 12391]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(Y_test, y_pred)\n",
    "from sklearn.metrics import classification_report\n",
    "classes = {'0','1','2'}\n",
    "print(classification_report(Y_test, y_pred,target_names=classes))\n",
    "print(multilabel_confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.9999894377726375\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "classes = {'0','1','2'}\n",
    "\n",
    "# set plot figure size\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "\n",
    "# function for scoring roc auc score for multi-class\n",
    "def multiclass_roc_auc_score(Y_test, onehot_encoded, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(Y_test)\n",
    "    Y_test = lb.transform(Y_test)\n",
    "\n",
    "    for (idx, c_label) in enumerate(classes):\n",
    "        fpr, tpr, thresholds = roc_curve(Y_test[:,idx].astype(int), onehot_encoded[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    return roc_auc_score(Y_test, onehot_encoded, average=average)\n",
    "\n",
    "\n",
    "print('ROC AUC score:', multiclass_roc_auc_score(Y_test, onehot_encoded))\n",
    "\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_classes=3\n",
    "# Compute Precision-Recall and plot curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
    "                                                        onehot_encoded[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test[:, i], onehot_encoded[:, i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "    onehot_encoded.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test, onehot_encoded,\n",
    "                                                     average=\"micro\")\n",
    "\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "plt.clf()\n",
    "plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
    "         label='micro-average Precision-recall curve (area = {0:0.2f})'\n",
    "               ''.format(average_precision[\"micro\"]), linestyle=':')\n",
    "for i in range(n_classes):\n",
    "    plt.plot(recall[i], precision[i],\n",
    "             label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
    "                   ''.format(i, average_precision[i]))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall for Midi Dataset Feature Selection 1')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=3'>4</a>\u001b[0m roc_auc_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=4'>5</a>\u001b[0m \u001b[39m# First aggregate all false positive rates\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=5'>6</a>\u001b[0m all_fpr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39mconcatenate([fpr[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_classes)]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=7'>8</a>\u001b[0m \u001b[39m# Then interpolate all ROC curves at this points\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=8'>9</a>\u001b[0m mean_tpr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(all_fpr)\n",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 29'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=3'>4</a>\u001b[0m roc_auc_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=4'>5</a>\u001b[0m \u001b[39m# First aggregate all false positive rates\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=5'>6</a>\u001b[0m all_fpr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39mconcatenate([fpr[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_classes)]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=7'>8</a>\u001b[0m \u001b[39m# Then interpolate all ROC curves at this points\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000102?line=8'>9</a>\u001b[0m mean_tpr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(all_fpr)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Process of plotting roc-auc curve belonging to all classes.\n",
    "\n",
    "n_classes=3\n",
    "\n",
    "from itertools import cycle\n",
    "roc_auc_scores = []\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "         label='micro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"micro\"]),\n",
    "         color='deeppink', linestyle=':', linewidth=4)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "         color='navy', linestyle=':', linewidth=4)\n",
    "\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i, roc_auc[i]))\n",
    "    roc_auc_scores.append(roc_auc[i])\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Extending the ROC Curve to Multi-Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43950, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43945</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43946</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43947</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43948</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43949</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43950 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0  1  2\n",
       "0      1  0  0\n",
       "1      0  0  1\n",
       "2      1  0  0\n",
       "3      1  0  0\n",
       "4      0  0  1\n",
       "...   .. .. ..\n",
       "43945  0  1  0\n",
       "43946  0  1  0\n",
       "43947  0  0  1\n",
       "43948  0  1  0\n",
       "43949  0  0  1\n",
       "\n",
       "[43950 rows x 3 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]], dtype=uint8)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ynew_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43950, 3)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43950, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00     20285\n",
      "           1       1.00      1.00      1.00     11272\n",
      "           0       1.00      1.00      1.00     12391\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     43948\n",
      "   macro avg       1.00      1.00      1.00     43948\n",
      "weighted avg       1.00      1.00      1.00     43948\n",
      " samples avg       1.00      1.00      1.00     43948\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classes = {'0','1','2'}\n",
    "print(classification_report(Y_test, y_pred,target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:142\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000099?line=6'>7</a>\u001b[0m average_precision \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000099?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_classes):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000099?line=8'>9</a>\u001b[0m     precision[i], recall[i], _ \u001b[39m=\u001b[39m precision_recall_curve(Y_test[:, i],\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000099?line=9'>10</a>\u001b[0m                                                         y_pred[:, i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000099?line=10'>11</a>\u001b[0m     average_precision[i] \u001b[39m=\u001b[39m average_precision_score(Y_test[:, i], y_pred[:, i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000099?line=12'>13</a>\u001b[0m \u001b[39m# Compute micro-average ROC curve and ROC area\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3502'>3503</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3503'>3504</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3504'>3505</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3505'>3506</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3506'>3507</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3628\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m         \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m         \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m         \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3628'>3629</a>\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3630'>3631</a>\u001b[0m \u001b[39m# GH#42269\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:5637\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5632'>5633</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5633'>5634</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5634'>5635</a>\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5635'>5636</a>\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5636'>5637</a>\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "n_classes=3\n",
    "# Compute Precision-Recall and plot curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
    "                                                        y_pred[:, i])\n",
    "    average_precision[i] = average_precision_score(Y_test[:, i], y_pred[:, i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "    y_pred.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(Y_test, y_pred,\n",
    "                                                     average=\"micro\")\n",
    "\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "plt.clf()\n",
    "plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
    "         label='micro-average Precision-recall curve (area = {0:0.2f})'\n",
    "               ''.format(average_precision[\"micro\"]), linestyle=':')\n",
    "for i in range(n_classes):\n",
    "    plt.plot(recall[i], precision[i],\n",
    "             label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
    "                   ''.format(i, average_precision[i]))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall for Midi Dataset Feature Selection 1')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical   \n",
    "y_pred = to_categorical(y_pred, num_classes=3)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_x=np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 15)                120       \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 10)                160       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 3)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 313\n",
      "Trainable params: 313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "build_classifier().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146498, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "classes [0 1] mismatch with the labels [0 1 2] found in the data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000021?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m roc_auc_score\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000021?line=11'>12</a>\u001b[0m n_classes \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(np\u001b[39m.\u001b[39munique(Ynew_test))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000021?line=12'>13</a>\u001b[0m y_test \u001b[39m=\u001b[39m label_binarize(Ynew_test, classes\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49marange(n_classes))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000021?line=13'>14</a>\u001b[0m y_pred \u001b[39m=\u001b[39m label_binarize(y_pred, classes\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marange(n_classes))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000021?line=15'>16</a>\u001b[0m \u001b[39m# Compute ROC curve and ROC area for each class\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py:529\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[0;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py?line=526'>527</a>\u001b[0m     y_n_classes \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mlen\u001b[39m(y[\u001b[39m0\u001b[39m])\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py?line=527'>528</a>\u001b[0m     \u001b[39mif\u001b[39;00m classes\u001b[39m.\u001b[39msize \u001b[39m!=\u001b[39m y_n_classes:\n\u001b[0;32m--> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py?line=528'>529</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py?line=529'>530</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mclasses \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m mismatch with the labels \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m found in the data\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py?line=530'>531</a>\u001b[0m                 classes, unique_labels(y)\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py?line=531'>532</a>\u001b[0m             )\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py?line=532'>533</a>\u001b[0m         )\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py?line=534'>535</a>\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/preprocessing/_label.py?line=535'>536</a>\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y)\n",
      "\u001b[0;31mValueError\u001b[0m: classes [0 1] mismatch with the labels [0 1 2] found in the data"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "n_classes = len(np.unique(Ynew_test))\n",
    "y_test = label_binarize(Ynew_test, classes=np.arange(n_classes))\n",
    "y_pred = label_binarize(y_pred, classes=np.arange(n_classes))\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(Ynew_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(Ynew_test.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "#plt.figure(figsize=(10,5))\n",
    "plt.figure(dpi=600)\n",
    "lw = 2\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "color=\"deeppink\", linestyle=\":\", linewidth=4,)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "color=\"navy\", linestyle=\":\", linewidth=4,)\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"darkgreen\", \"yellow\", \"blue\"])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i]),)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) curve\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "multiclass_roc_auc_score() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000100?line=14'>15</a>\u001b[0m     c_ax\u001b[39m.\u001b[39mplot(fpr, fpr, \u001b[39m'\u001b[39m\u001b[39mb-\u001b[39m\u001b[39m'\u001b[39m, label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRandom Guessing\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000100?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m roc_auc_score(Y_test, y_pred, average\u001b[39m=\u001b[39maverage)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000100?line=18'>19</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mROC AUC score:\u001b[39m\u001b[39m'\u001b[39m, multiclass_roc_auc_score(Y_test, y_pred))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000100?line=20'>21</a>\u001b[0m c_ax\u001b[39m.\u001b[39mlegend()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000100?line=21'>22</a>\u001b[0m c_ax\u001b[39m.\u001b[39mset_xlabel(\u001b[39m'\u001b[39m\u001b[39mFalse Positive Rate\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: multiclass_roc_auc_score() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# set plot figure size\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "\n",
    "# function for scoring roc auc score for multi-class\n",
    "def multiclass_roc_auc_score(Y_test):\n",
    "    Y_test = lb.transform(Y_test)\n",
    "\n",
    "    for (idx, c_label) in enumerate(classes):\n",
    "        fpr, tpr, thresholds = roc_curve(Y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    return roc_auc_score(Y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "print('ROC AUC score:', multiclass_roc_auc_score(Y_test, y_pred))\n",
    "\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(neurons=1):\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(neurons, activation='relu', input_shape=(7,)))\n",
    "    network.add(layers.Dense(10, activation='relu'))\n",
    "    network.add(layers.Dense(3, activation='softmax'))\n",
    "    #\n",
    "    # Configure the network with optimizer, loss function and accuracy\n",
    "    #\n",
    "    network.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9090/514405414.py:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_classifier, epochs=20, batch_size=10, verbose=0)\n",
      "2022-05-10 21:17:01.435655: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:01.437035: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-10 21:17:01.804203: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:01.804498: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-10 21:17:01.915237: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:01.916315: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-10 21:17:02.087354: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:02.087865: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-10 21:17:02.122878: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:02.123011: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-10 21:17:02.134124: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:02.139617: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-10 21:17:02.169086: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:02.169438: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-10 21:17:02.193384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:02.193499: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-10 21:17:08.742916: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:08.742987: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-10 21:17:08.743038: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-10 21:17:08.743633: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 21:17:08.748789: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:08.748863: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-10 21:17:08.748910: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-10 21:17:08.749480: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 21:17:08.759368: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:08.759432: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-10 21:17:08.759473: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-10 21:17:08.760236: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 21:17:08.760827: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:08.760858: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-10 21:17:08.760889: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-10 21:17:08.761435: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 21:17:08.779637: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:08.779695: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-10 21:17:08.779742: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-10 21:17:08.780346: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 21:17:08.786477: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:08.786549: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-10 21:17:08.786622: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-10 21:17:08.787124: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:08.787176: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-10 21:17:08.787226: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-10 21:17:08.787510: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 21:17:08.787856: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-10 21:17:08.792976: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-10 21:17:08.793044: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-10 21:17:08.793099: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-10 21:17:08.797583: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.493147 using {'neurons': 5}\n",
      "0.492491 (0.003510) with: {'neurons': 1}\n",
      "0.493147 (0.002786) with: {'neurons': 5}\n",
      "0.489993 (0.002833) with: {'neurons': 10}\n",
      "0.491973 (0.002686) with: {'neurons': 15}\n",
      "0.489447 (0.004468) with: {'neurons': 20}\n",
      "0.492607 (0.001958) with: {'neurons': 25}\n",
      "0.479829 (0.016256) with: {'neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=build_classifier, epochs=20, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(neurons=1):\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(5, activation='relu', input_shape=(7,)))\n",
    "    network.add(layers.Dense(neurons, activation='relu'))\n",
    "    network.add(layers.Dense(3, activation='softmax'))\n",
    "    #\n",
    "    # Configure the network with optimizer, loss function and accuracy\n",
    "    #\n",
    "    network.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10289/514405414.py:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_classifier, epochs=20, batch_size=10, verbose=0)\n",
      "2022-05-11 20:52:46.047596: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:46.047892: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-11 20:52:46.084910: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:46.085060: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-11 20:52:46.360884: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:46.372959: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-11 20:52:46.665492: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:46.665960: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-11 20:52:46.707194: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:46.707292: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-11 20:52:46.715196: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:46.715240: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-11 20:52:46.829263: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:46.829423: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-11 20:52:48.011698: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:48.012048: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-11 20:52:51.303963: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:51.305257: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 20:52:51.305661: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-11 20:52:51.308511: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-11 20:52:51.344408: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:51.344575: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 20:52:51.344660: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-11 20:52:51.345179: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-05-11 20:52:51.804815: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:51.804934: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 20:52:51.804999: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-11 20:52:51.805484: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-11 20:52:51.819810: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:51.820053: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 20:52:51.820145: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-11 20:52:51.820576: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-11 20:52:51.846703: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:51.846802: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 20:52:51.846862: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-11 20:52:51.848041: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-05-11 20:52:52.815150: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:52.815209: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 20:52:52.815248: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-11 20:52:52.815690: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-05-11 20:52:53.198973: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:53.199034: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 20:52:53.199113: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-11 20:52:53.199614: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-05-11 20:52:54.366708: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 20:52:54.366759: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 20:52:54.366797: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-11 20:52:54.367251: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-05-11 21:40:07.709921: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-11 21:40:07.710049: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-11 21:40:07.711209: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-11 21:40:07.717542: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.494648 using {'neurons': 10}\n",
      "0.461467 (0.004070) with: {'neurons': 1}\n",
      "0.492935 (0.002671) with: {'neurons': 5}\n",
      "0.494648 (0.003331) with: {'neurons': 10}\n",
      "0.487283 (0.001423) with: {'neurons': 15}\n",
      "0.493857 (0.002018) with: {'neurons': 20}\n",
      "0.492321 (0.002473) with: {'neurons': 25}\n",
      "0.491802 (0.002901) with: {'neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=build_classifier, epochs=20, batch_size=10, verbose=0)\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 13:14:53.301742: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:53.302217: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-13 13:14:53.512336: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:53.512899: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-13 13:14:53.962580: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:53.963067: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-13 13:14:54.045930: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:54.046081: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-13 13:14:54.068313: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:54.081466: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-13 13:14:54.083739: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:54.084283: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-13 13:14:54.374671: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:54.376149: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-13 13:14:54.418321: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:54.418471: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-05-13 13:14:58.703149: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:58.703584: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 13:14:58.703683: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 13:14:58.704389: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-05-13 13:14:59.055516: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:59.057635: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 13:14:59.058746: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 13:14:59.060035: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 13:14:59.609819: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:59.610427: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 13:14:59.610539: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 13:14:59.611469: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-13 13:14:59.715958: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:59.716022: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 13:14:59.716065: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 13:14:59.716539: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-13 13:14:59.730334: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:59.730493: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 13:14:59.730608: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 13:14:59.731577: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-13 13:14:59.802117: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:59.803047: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 13:14:59.804471: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 13:14:59.805874: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "2022-05-13 13:14:59.871931: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:14:59.872089: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 13:14:59.872185: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 13:14:59.872705: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 13:15:00.210731: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 13:15:00.210806: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 13:15:00.210856: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 13:15:00.211399: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "Epoch 1/20\n",
      "2442/2442 [==============================] - 20s 7ms/step - loss: 1.0179 - accuracy: 0.4924\n",
      "Epoch 2/20\n",
      "2442/2442 [==============================] - 20s 8ms/step - loss: 1.0209 - accuracy: 0.4884\n",
      "Epoch 2/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0140 - accuracy: 0.4936\n",
      "Epoch 3/20\n",
      "4884/4884 [==============================] - 35s 7ms/step - loss: 1.0191 - accuracy: 0.4903\n",
      "4789/9767 [=============>................] - ETA: 35s - loss: 1.0297 - accuracy: 0.4853Epoch 2/20\n",
      "4884/4884 [==============================] - 36s 7ms/step - loss: 1.0206 - accuracy: 0.4877\n",
      "Epoch 2/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0152 - accuracy: 0.4903\n",
      "4995/9767 [==============>...............] - ETA: 33s - loss: 1.0253 - accuracy: 0.4855Epoch 3/20\n",
      "4884/4884 [==============================] - 37s 7ms/step - loss: 1.0270 - accuracy: 0.4883\n",
      "Epoch 2/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0138 - accuracy: 0.4945\n",
      "Epoch 4/20\n",
      "2442/2442 [==============================] - 16s 6ms/step - loss: 1.0145 - accuracy: 0.4907\n",
      "7307/9767 [=====================>........] - ETA: 17s - loss: 1.0275 - accuracy: 0.4877Epoch 4/20\n",
      "9767/9767 [==============================] - 68s 7ms/step - loss: 1.0263 - accuracy: 0.4882\n",
      "2417/2442 [============================>.] - ETA: 0s - loss: 1.0133 - accuracy: 0.4945Epoch 2/20\n",
      "9767/9767 [==============================] - 68s 7ms/step - loss: 1.0221 - accuracy: 0.4863\n",
      "Epoch 2/20\n",
      "9767/9767 [==============================] - 69s 7ms/step - loss: 1.0208 - accuracy: 0.4890\n",
      "2341/2442 [===========================>..] - ETA: 0s - loss: 1.0141 - accuracy: 0.4916Epoch 2/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0134 - accuracy: 0.4946\n",
      "4842/4884 [============================>.] - ETA: 0s - loss: 1.0167 - accuracy: 0.4913Epoch 5/20\n",
      "4884/4884 [==============================] - 33s 7ms/step - loss: 1.0153 - accuracy: 0.4940\n",
      "Epoch 3/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0165 - accuracy: 0.4913\n",
      "Epoch 3/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0141 - accuracy: 0.4919\n",
      "Epoch 5/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0223 - accuracy: 0.4913\n",
      " 114/4884 [..............................] - ETA: 30s - loss: 1.0153 - accuracy: 0.4912Epoch 3/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0128 - accuracy: 0.4945\n",
      "2298/4884 [=============>................] - ETA: 17s - loss: 1.0226 - accuracy: 0.4891Epoch 6/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0138 - accuracy: 0.4916\n",
      "Epoch 6/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0125 - accuracy: 0.4947\n",
      "2365/2442 [============================>.] - ETA: 0s - loss: 1.0136 - accuracy: 0.4923Epoch 7/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0152 - accuracy: 0.4929\n",
      "5003/9767 [==============>...............] - ETA: 31s - loss: 1.0179 - accuracy: 0.4893Epoch 4/20\n",
      "4884/4884 [==============================] - 33s 7ms/step - loss: 1.0154 - accuracy: 0.4917\n",
      "Epoch 4/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0134 - accuracy: 0.4928\n",
      "5040/9767 [==============>...............] - ETA: 30s - loss: 1.0181 - accuracy: 0.4902Epoch 7/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0213 - accuracy: 0.4914\n",
      "Epoch 4/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0121 - accuracy: 0.4950\n",
      "2342/4884 [=============>................] - ETA: 16s - loss: 1.0138 - accuracy: 0.4935Epoch 8/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0131 - accuracy: 0.4923\n",
      " 105/2442 [>.............................] - ETA: 17s - loss: 1.0080 - accuracy: 0.5050Epoch 8/20\n",
      "9767/9767 [==============================] - 64s 7ms/step - loss: 1.0185 - accuracy: 0.4891\n",
      "Epoch 3/20\n",
      "9767/9767 [==============================] - 65s 7ms/step - loss: 1.0164 - accuracy: 0.4931\n",
      "Epoch 3/20\n",
      "9767/9767 [==============================] - 65s 7ms/step - loss: 1.0233 - accuracy: 0.4912\n",
      "   1/9767 [..............................] - ETA: 35s - loss: 0.8580 - accuracy: 0.6000Epoch 3/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0114 - accuracy: 0.4952\n",
      "Epoch 9/20\n",
      "2442/2442 [==============================] - 16s 6ms/step - loss: 1.0132 - accuracy: 0.4919\n",
      "Epoch 9/20\n",
      "4884/4884 [==============================] - 33s 7ms/step - loss: 1.0138 - accuracy: 0.4945\n",
      " 107/9767 [..............................] - ETA: 1:12 - loss: 0.9901 - accuracy: 0.5178Epoch 5/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0144 - accuracy: 0.4918\n",
      "Epoch 5/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0208 - accuracy: 0.4915\n",
      "Epoch 5/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0118 - accuracy: 0.4948\n",
      "Epoch 10/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0127 - accuracy: 0.4919\n",
      "2509/9767 [======>.......................] - ETA: 48s - loss: 1.0241 - accuracy: 0.4906Epoch 10/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0110 - accuracy: 0.4952\n",
      "4883/9767 [=============>................] - ETA: 32s - loss: 1.0241 - accuracy: 0.4912Epoch 11/20\n",
      "4884/4884 [==============================] - 31s 6ms/step - loss: 1.0205 - accuracy: 0.4920\n",
      "2371/2442 [============================>.] - ETA: 0s - loss: 1.0120 - accuracy: 0.4928Epoch 6/20\n",
      "4884/4884 [==============================] - 32s 6ms/step - loss: 1.0144 - accuracy: 0.4923\n",
      "Epoch 6/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0118 - accuracy: 0.4926\n",
      "Epoch 11/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0134 - accuracy: 0.4944\n",
      "Epoch 6/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0110 - accuracy: 0.4954\n",
      "Epoch 12/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0111 - accuracy: 0.4930\n",
      "Epoch 12/20\n",
      "9767/9767 [==============================] - 62s 6ms/step - loss: 1.0159 - accuracy: 0.4930\n",
      "9767/9767 [==============================] - 63s 6ms/step - loss: 1.0177 - accuracy: 0.4885\n",
      "Epoch 4/20\n",
      "Epoch 4/20\n",
      "9767/9767 [==============================] - 63s 6ms/step - loss: 1.0237 - accuracy: 0.4904\n",
      "Epoch 4/20\n",
      "2442/2442 [==============================] - 15s 6ms/step - loss: 1.0104 - accuracy: 0.4955\n",
      "Epoch 13/20\n",
      "4884/4884 [==============================] - 32s 6ms/step - loss: 1.0206 - accuracy: 0.4918\n",
      " 229/9767 [..............................] - ETA: 1:03 - loss: 1.0194 - accuracy: 0.4926Epoch 7/20\n",
      "4884/4884 [==============================] - 32s 6ms/step - loss: 1.0136 - accuracy: 0.4915\n",
      " 219/9767 [..............................] - ETA: 1:10 - loss: 1.0239 - accuracy: 0.4922Epoch 7/20\n",
      "4884/4884 [==============================] - 31s 6ms/step - loss: 1.0136 - accuracy: 0.4938\n",
      "Epoch 7/20\n",
      "2442/2442 [==============================] - 15s 6ms/step - loss: 1.0108 - accuracy: 0.4927\n",
      "Epoch 13/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0104 - accuracy: 0.4951\n",
      "Epoch 14/20\n",
      "2442/2442 [==============================] - 18s 7ms/step - loss: 1.0116 - accuracy: 0.4934\n",
      "Epoch 14/20\n",
      "2442/2442 [==============================] - 18s 7ms/step - loss: 1.0102 - accuracy: 0.4948\n",
      "5069/9767 [==============>...............] - ETA: 33s - loss: 1.0168 - accuracy: 0.4912Epoch 15/20\n",
      "4884/4884 [==============================] - 35s 7ms/step - loss: 1.0203 - accuracy: 0.4915\n",
      "Epoch 8/20\n",
      "4884/4884 [==============================] - 35s 7ms/step - loss: 1.0143 - accuracy: 0.4925\n",
      "  27/4884 [..............................] - ETA: 29s - loss: 1.0275 - accuracy: 0.4852Epoch 8/20\n",
      "2442/2442 [==============================] - 18s 7ms/step - loss: 1.0113 - accuracy: 0.4930\n",
      "4884/4884 [==============================] - 36s 7ms/step - loss: 1.0135 - accuracy: 0.4936\n",
      "Epoch 15/20\n",
      "   1/2442 [..............................] - ETA: 20s - loss: 0.8770 - accuracy: 0.6250Epoch 8/20\n",
      "2442/2442 [==============================] - 20s 8ms/step - loss: 1.0103 - accuracy: 0.4951\n",
      "2325/2442 [===========================>..] - ETA: 0s - loss: 1.0111 - accuracy: 0.4926Epoch 16/20\n",
      "2442/2442 [==============================] - 20s 8ms/step - loss: 1.0111 - accuracy: 0.4924\n",
      "7664/9767 [======================>.......] - ETA: 15s - loss: 1.0170 - accuracy: 0.4902Epoch 16/20\n",
      "9767/9767 [==============================] - 76s 8ms/step - loss: 1.0152 - accuracy: 0.4936\n",
      "9767/9767 [==============================] - 76s 8ms/step - loss: 1.0170 - accuracy: 0.4900\n",
      "Epoch 5/20\n",
      "9678/9767 [============================>.] - ETA: 0s - loss: 1.0227 - accuracy: 0.4915Epoch 5/20\n",
      "9767/9767 [==============================] - 76s 8ms/step - loss: 1.0227 - accuracy: 0.4914\n",
      "2144/2442 [=========================>....] - ETA: 2s - loss: 1.0095 - accuracy: 0.4959Epoch 5/20\n",
      "4884/4884 [==============================] - 41s 8ms/step - loss: 1.0204 - accuracy: 0.4916\n",
      "Epoch 9/20\n",
      "4884/4884 [==============================] - 41s 8ms/step - loss: 1.0144 - accuracy: 0.4921\n",
      "2205/2442 [==========================>...] - ETA: 2s - loss: 1.0119 - accuracy: 0.4923Epoch 9/20\n",
      "4884/4884 [==============================] - 41s 8ms/step - loss: 1.0134 - accuracy: 0.4924\n",
      " 327/9767 [>.............................] - ETA: 1:12 - loss: 1.0044 - accuracy: 0.4969Epoch 9/20\n",
      "2442/2442 [==============================] - 23s 9ms/step - loss: 1.0100 - accuracy: 0.4956\n",
      "Epoch 17/20\n",
      "2442/2442 [==============================] - 22s 9ms/step - loss: 1.0106 - accuracy: 0.4935\n",
      " 256/4884 [>.............................] - ETA: 36s - loss: 1.0071 - accuracy: 0.4928Epoch 17/20\n",
      "2442/2442 [==============================] - 19s 8ms/step - loss: 1.0100 - accuracy: 0.4958\n",
      "2851/9767 [=======>......................] - ETA: 52s - loss: 1.0131 - accuracy: 0.4950Epoch 18/20\n",
      "2442/2442 [==============================] - 19s 8ms/step - loss: 1.0113 - accuracy: 0.4923\n",
      "Epoch 18/20\n",
      "4884/4884 [==============================] - 36s 7ms/step - loss: 1.0196 - accuracy: 0.4928\n",
      "Epoch 10/20\n",
      "4884/4884 [==============================] - 36s 7ms/step - loss: 1.0133 - accuracy: 0.4920\n",
      "Epoch 10/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0095 - accuracy: 0.4959\n",
      "Epoch 19/20\n",
      "4884/4884 [==============================] - 36s 7ms/step - loss: 1.0131 - accuracy: 0.4949\n",
      "  31/2442 [..............................] - ETA: 13s - loss: 0.9979 - accuracy: 0.5113Epoch 10/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0106 - accuracy: 0.4926\n",
      "Epoch 19/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0096 - accuracy: 0.4961\n",
      "Epoch 20/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0103 - accuracy: 0.4932\n",
      "7852/9767 [=======================>......] - ETA: 13s - loss: 1.0143 - accuracy: 0.4948Epoch 20/20\n",
      "9767/9767 [==============================] - 68s 7ms/step - loss: 1.0142 - accuracy: 0.4949\n",
      "4841/4884 [============================>.] - ETA: 0s - loss: 1.0201 - accuracy: 0.4927Epoch 6/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0200 - accuracy: 0.4928\n",
      "Epoch 11/20\n",
      "9767/9767 [==============================] - 69s 7ms/step - loss: 1.0161 - accuracy: 0.4905\n",
      "1921/2442 [======================>.......] - ETA: 3s - loss: 1.0111 - accuracy: 0.4935Epoch 6/20\n",
      "9767/9767 [==============================] - 69s 7ms/step - loss: 1.0228 - accuracy: 0.4899\n",
      "2252/2442 [==========================>...] - ETA: 1s - loss: 1.0087 - accuracy: 0.4962Epoch 6/20\n",
      "4884/4884 [==============================] - 33s 7ms/step - loss: 1.0137 - accuracy: 0.4923\n",
      "4614/4884 [===========================>..] - ETA: 1s - loss: 1.0131 - accuracy: 0.4941Epoch 11/20\n",
      "2442/2442 [==============================] - 16s 7ms/step - loss: 1.0092 - accuracy: 0.4959\n",
      "4884/4884 [==============================] - 33s 7ms/step - loss: 1.0131 - accuracy: 0.4940\n",
      "Epoch 11/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0112 - accuracy: 0.4931\n",
      "1221/1221 [==============================] - 6s 4ms/step - loss: 1.0560 - accuracy: 0.4901\n",
      "1489/9767 [===>..........................] - ETA: 47s - loss: 1.0238 - accuracy: 0.4926"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1594/4884 [========>.....................] - ETA: 20s - loss: 1.0212 - accuracy: 0.4911Epoch 1/20\n",
      "2442/2442 [==============================] - 14s 5ms/step - loss: 1.0248 - accuracy: 0.4891\n",
      "Epoch 2/20\n",
      "4884/4884 [==============================] - 27s 6ms/step - loss: 1.0194 - accuracy: 0.4933\n",
      "4917/9767 [==============>...............] - ETA: 27s - loss: 1.0139 - accuracy: 0.4942Epoch 12/20\n",
      "4884/4884 [==============================] - 27s 6ms/step - loss: 1.0132 - accuracy: 0.4922\n",
      "Epoch 12/20\n",
      "4884/4884 [==============================] - 26s 5ms/step - loss: 1.0126 - accuracy: 0.4943\n",
      "Epoch 12/20\n",
      "2442/2442 [==============================] - 13s 5ms/step - loss: 1.0214 - accuracy: 0.4910\n",
      "6565/9767 [===================>..........] - ETA: 17s - loss: 1.0227 - accuracy: 0.4896Epoch 3/20\n",
      "2442/2442 [==============================] - 14s 6ms/step - loss: 1.0205 - accuracy: 0.4905\n",
      "Epoch 4/20\n",
      "1221/1221 [==============================] - 7s 5ms/step - loss: 1.0237 - accuracy: 0.4955\n",
      "4884/4884 [==============================] - 26s 5ms/step - loss: 1.0196 - accuracy: 0.4913\n",
      "Epoch 13/20\n",
      "9767/9767 [==============================] - 54s 6ms/step - loss: 1.0142 - accuracy: 0.4941\n",
      "9762/9767 [============================>.] - ETA: 0s - loss: 1.0158 - accuracy: 0.4897Epoch 7/20\n",
      "9767/9767 [==============================] - 54s 5ms/step - loss: 1.0158 - accuracy: 0.4898\n",
      "Epoch 7/20\n",
      "9767/9767 [==============================] - 54s 6ms/step - loss: 1.0227 - accuracy: 0.4908\n",
      "Epoch 7/20\n",
      "4884/4884 [==============================] - 27s 6ms/step - loss: 1.0130 - accuracy: 0.4920\n",
      "4816/4884 [============================>.] - ETA: 0s - loss: 1.0126 - accuracy: 0.4945Epoch 13/20\n",
      " 347/9767 [>.............................] - ETA: 52s - loss: 1.0209 - accuracy: 0.4827"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4884/4884 [==============================] - 27s 5ms/step - loss: 1.0127 - accuracy: 0.4946\n",
      "  82/4884 [..............................] - ETA: 25s - loss: 1.0204 - accuracy: 0.4738Epoch 13/20\n",
      " 206/9767 [..............................] - ETA: 57s - loss: 1.0212 - accuracy: 0.4937Epoch 1/20\n",
      "2442/2442 [==============================] - 14s 6ms/step - loss: 1.0201 - accuracy: 0.4923\n",
      "1699/9767 [====>.........................] - ETA: 49s - loss: 1.0250 - accuracy: 0.4882Epoch 5/20\n",
      "1628/1628 [==============================] - 12s 6ms/step - loss: 1.0185 - accuracy: 0.4913\n",
      "2369/9767 [======>.......................] - ETA: 43s - loss: 1.0158 - accuracy: 0.4923Epoch 2/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0132 - accuracy: 0.4946\n",
      "4068/4884 [=======================>......] - ETA: 4s - loss: 1.0204 - accuracy: 0.4920Epoch 3/20\n",
      "2442/2442 [==============================] - 15s 6ms/step - loss: 1.0198 - accuracy: 0.4926\n",
      "4477/4884 [==========================>...] - ETA: 2s - loss: 1.0200 - accuracy: 0.4922Epoch 6/20\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0196 - accuracy: 0.4925\n",
      "Epoch 14/20\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0137 - accuracy: 0.4922\n",
      "Epoch 14/20\n",
      "4884/4884 [==============================] - 30s 6ms/step - loss: 1.0127 - accuracy: 0.4940\n",
      "  82/4884 [..............................] - ETA: 35s - loss: 1.0116 - accuracy: 0.4902Epoch 14/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0124 - accuracy: 0.4943\n",
      "1265/2442 [==============>...............] - ETA: 6s - loss: 1.0200 - accuracy: 0.4924Epoch 4/20\n",
      "2442/2442 [==============================] - 14s 6ms/step - loss: 1.0198 - accuracy: 0.4924\n",
      "Epoch 7/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0117 - accuracy: 0.4952\n",
      "7308/9767 [=====================>........] - ETA: 14s - loss: 1.0160 - accuracy: 0.4900Epoch 5/20\n",
      "1628/1628 [==============================] - 11s 7ms/step - loss: 1.0114 - accuracy: 0.4952\n",
      "Epoch 6/20\n",
      "2442/2442 [==============================] - 18s 7ms/step - loss: 1.0198 - accuracy: 0.4920\n",
      "Epoch 8/20\n",
      "9767/9767 [==============================] - 61s 6ms/step - loss: 1.0142 - accuracy: 0.4944\n",
      "Epoch 8/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0199 - accuracy: 0.4922\n",
      "Epoch 15/20\n",
      "9767/9767 [==============================] - 62s 6ms/step - loss: 1.0162 - accuracy: 0.4898\n",
      "Epoch 8/20\n",
      "9767/9767 [==============================] - 63s 6ms/step - loss: 1.0222 - accuracy: 0.4898\n",
      "Epoch 8/20\n",
      "4884/4884 [==============================] - 33s 7ms/step - loss: 1.0138 - accuracy: 0.4916\n",
      " 396/4884 [=>............................] - ETA: 30s - loss: 1.0185 - accuracy: 0.4928Epoch 15/20\n",
      "4884/4884 [==============================] - 33s 7ms/step - loss: 1.0128 - accuracy: 0.4941\n",
      "Epoch 15/20\n",
      "1628/1628 [==============================] - 12s 7ms/step - loss: 1.0113 - accuracy: 0.4948\n",
      "Epoch 7/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0200 - accuracy: 0.4921\n",
      "Epoch 9/20\n",
      "1628/1628 [==============================] - 12s 7ms/step - loss: 1.0107 - accuracy: 0.4951\n",
      "Epoch 8/20\n",
      "1628/1628 [==============================] - 11s 7ms/step - loss: 1.0107 - accuracy: 0.4953\n",
      "Epoch 9/20\n",
      "2442/2442 [==============================] - 17s 7ms/step - loss: 1.0195 - accuracy: 0.4925\n",
      "4293/9767 [============>.................] - ETA: 37s - loss: 1.0225 - accuracy: 0.4882Epoch 10/20\n",
      "4884/4884 [==============================] - 33s 7ms/step - loss: 1.0194 - accuracy: 0.4930\n",
      "4447/9767 [============>.................] - ETA: 36s - loss: 1.0226 - accuracy: 0.4879Epoch 16/20\n",
      "4884/4884 [==============================] - 33s 7ms/step - loss: 1.0130 - accuracy: 0.4917\n",
      "1165/1628 [====================>.........] - ETA: 2s - loss: 1.0106 - accuracy: 0.4949Epoch 16/20\n",
      "4884/4884 [==============================] - 32s 7ms/step - loss: 1.0127 - accuracy: 0.4935\n",
      "Epoch 16/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0104 - accuracy: 0.4955\n",
      "Epoch 10/20\n",
      "2442/2442 [==============================] - 15s 6ms/step - loss: 1.0195 - accuracy: 0.4930\n",
      "6760/9767 [===================>..........] - ETA: 19s - loss: 1.0214 - accuracy: 0.4901Epoch 11/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0107 - accuracy: 0.4951\n",
      " 326/2442 [===>..........................] - ETA: 12s - loss: 1.0217 - accuracy: 0.4889Epoch 11/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0098 - accuracy: 0.4956\n",
      "3669/4884 [=====================>........] - ETA: 7s - loss: 1.0125 - accuracy: 0.4953Epoch 12/20\n",
      "4884/4884 [==============================] - 28s 6ms/step - loss: 1.0194 - accuracy: 0.4931\n",
      "Epoch 17/20\n",
      "9767/9767 [==============================] - 62s 6ms/step - loss: 1.0143 - accuracy: 0.4930\n",
      "Epoch 9/20\n",
      "2442/2442 [==============================] - 15s 6ms/step - loss: 1.0197 - accuracy: 0.4921\n",
      "Epoch 12/20\n",
      "9767/9767 [==============================] - 62s 6ms/step - loss: 1.0159 - accuracy: 0.4910\n",
      " 214/4884 [>.............................] - ETA: 25s - loss: 1.0335 - accuracy: 0.4741Epoch 9/20\n",
      "9767/9767 [==============================] - 62s 6ms/step - loss: 1.0223 - accuracy: 0.4899\n",
      "Epoch 9/20\n",
      "4884/4884 [==============================] - 30s 6ms/step - loss: 1.0131 - accuracy: 0.4919\n",
      "Epoch 17/20\n",
      "4884/4884 [==============================] - 30s 6ms/step - loss: 1.0126 - accuracy: 0.4939\n",
      "Epoch 17/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0103 - accuracy: 0.4950\n",
      "Epoch 13/20\n",
      "2442/2442 [==============================] - 15s 6ms/step - loss: 1.0194 - accuracy: 0.4923\n",
      "1805/4884 [==========>...................] - ETA: 18s - loss: 1.0121 - accuracy: 0.4966Epoch 13/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0103 - accuracy: 0.4945\n",
      "2934/4884 [=================>............] - ETA: 11s - loss: 1.0191 - accuracy: 0.4928Epoch 14/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0103 - accuracy: 0.4955\n",
      "Epoch 15/20\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0194 - accuracy: 0.4926\n",
      "Epoch 18/20\n",
      "2442/2442 [==============================] - 14s 6ms/step - loss: 1.0192 - accuracy: 0.4922\n",
      " 402/1628 [======>.......................] - ETA: 7s - loss: 1.0096 - accuracy: 0.4973Epoch 14/20\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0126 - accuracy: 0.4945\n",
      "4906/9767 [==============>...............] - ETA: 29s - loss: 1.0217 - accuracy: 0.4899Epoch 18/20\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0133 - accuracy: 0.4914\n",
      "Epoch 18/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0106 - accuracy: 0.4949\n",
      "Epoch 16/20\n",
      "2442/2442 [==============================] - 15s 6ms/step - loss: 1.0193 - accuracy: 0.4932\n",
      "1204/1628 [=====================>........] - ETA: 2s - loss: 1.0096 - accuracy: 0.4960Epoch 15/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0101 - accuracy: 0.4957\n",
      "2350/4884 [=============>................] - ETA: 14s - loss: 1.0126 - accuracy: 0.4949Epoch 17/20\n",
      "9767/9767 [==============================] - 57s 6ms/step - loss: 1.0140 - accuracy: 0.4941\n",
      "1622/1628 [============================>.] - ETA: 0s - loss: 1.0100 - accuracy: 0.4955Epoch 10/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0100 - accuracy: 0.4954\n",
      "Epoch 18/20\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0198 - accuracy: 0.4924\n",
      "Epoch 19/20\n",
      "9767/9767 [==============================] - 58s 6ms/step - loss: 1.0158 - accuracy: 0.4911\n",
      "Epoch 10/20\n",
      "2442/2442 [==============================] - 15s 6ms/step - loss: 1.0191 - accuracy: 0.4924\n",
      "  58/9767 [..............................] - ETA: 54s - loss: 1.0109 - accuracy: 0.4948Epoch 16/20\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0125 - accuracy: 0.4953\n",
      "Epoch 19/20\n",
      "9767/9767 [==============================] - 59s 6ms/step - loss: 1.0220 - accuracy: 0.4901\n",
      " 452/2442 [====>.........................] - ETA: 12s - loss: 1.0198 - accuracy: 0.4916Epoch 10/20\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0132 - accuracy: 0.4918\n",
      "Epoch 19/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0099 - accuracy: 0.4958\n",
      " 567/4884 [==>...........................] - ETA: 25s - loss: 1.0127 - accuracy: 0.4914Epoch 19/20\n",
      "2442/2442 [==============================] - 15s 6ms/step - loss: 1.0194 - accuracy: 0.4927\n",
      "2072/9767 [=====>........................] - ETA: 44s - loss: 1.0178 - accuracy: 0.4979Epoch 17/20\n",
      "1628/1628 [==============================] - 9s 6ms/step - loss: 1.0098 - accuracy: 0.4957\n",
      "2169/4884 [============>.................] - ETA: 16s - loss: 1.0120 - accuracy: 0.4923Epoch 20/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0100 - accuracy: 0.4951\n",
      "4884/4884 [==============================] - 28s 6ms/step - loss: 1.0194 - accuracy: 0.4931\n",
      "Epoch 20/20\n",
      "2442/2442 [==============================] - 14s 6ms/step - loss: 1.0191 - accuracy: 0.4926\n",
      "4492/4884 [==========================>...] - ETA: 2s - loss: 1.0124 - accuracy: 0.4951Epoch 18/20\n",
      "814/814 [==============================] - 4s 4ms/step - loss: 1.0617 - accuracy: 0.4911\n",
      "4884/4884 [==============================] - 28s 6ms/step - loss: 1.0126 - accuracy: 0.4945\n",
      "Epoch 20/20\n",
      "4884/4884 [==============================] - 28s 6ms/step - loss: 1.0131 - accuracy: 0.4924\n",
      "Epoch 20/20\n",
      "5166/9767 [==============>...............] - ETA: 26s - loss: 1.0221 - accuracy: 0.4910"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 170/4884 [>.............................] - ETA: 26s - loss: 1.0142 - accuracy: 0.4953Epoch 1/20\n",
      "2442/2442 [==============================] - 14s 6ms/step - loss: 1.0193 - accuracy: 0.4927\n",
      "Epoch 19/20\n",
      "1628/1628 [==============================] - 12s 6ms/step - loss: 1.0209 - accuracy: 0.4876\n",
      "Epoch 2/20\n",
      "9767/9767 [==============================] - 57s 6ms/step - loss: 1.0135 - accuracy: 0.4940\n",
      "Epoch 11/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0144 - accuracy: 0.4914\n",
      "Epoch 3/20\n",
      "4884/4884 [==============================] - 28s 6ms/step - loss: 1.0198 - accuracy: 0.4925\n",
      "9767/9767 [==============================] - 57s 6ms/step - loss: 1.0153 - accuracy: 0.4905\n",
      "Epoch 11/20\n",
      "2442/2442 [==============================] - 14s 6ms/step - loss: 1.0192 - accuracy: 0.4931\n",
      "Epoch 20/20\n",
      "9767/9767 [==============================] - 57s 6ms/step - loss: 1.0220 - accuracy: 0.4906\n",
      " 987/1628 [=================>............] - ETA: 3s - loss: 1.0124 - accuracy: 0.4923Epoch 11/20\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0129 - accuracy: 0.4936\n",
      "4884/4884 [==============================] - 29s 6ms/step - loss: 1.0132 - accuracy: 0.4924\n",
      "1628/1628 [==============================] - 8s 5ms/step - loss: 1.0125 - accuracy: 0.4927\n",
      "Epoch 4/20\n",
      "2442/2442 [==============================] - 12s 5ms/step - loss: 1.0192 - accuracy: 0.4923\n",
      "2442/2442 [==============================] - 10s 4ms/step - loss: 1.0251 - accuracy: 0.4961\n",
      "1628/1628 [==============================] - 8s 5ms/step - loss: 1.0121 - accuracy: 0.4930\n",
      "Epoch 5/20\n",
      "  38/1628 [..............................] - ETA: 6s - loss: 1.0182 - accuracy: 0.4851"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50/1628 [..............................] - ETA: 6s - loss: 1.0129 - accuracy: 0.4900Epoch 1/20\n",
      "1221/1221 [==============================] - 5s 4ms/step - loss: 1.0117 - accuracy: 0.4908\n",
      " 561/1628 [=========>....................] - ETA: 4s - loss: 1.0332 - accuracy: 0.4834"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 595/1628 [=========>....................] - ETA: 4s - loss: 1.0325 - accuracy: 0.4842Epoch 1/20\n",
      "1628/1628 [==============================] - 8s 5ms/step - loss: 1.0116 - accuracy: 0.4934\n",
      "5084/9767 [==============>...............] - ETA: 21s - loss: 1.0139 - accuracy: 0.4949Epoch 6/20\n",
      "1628/1628 [==============================] - 9s 5ms/step - loss: 1.0263 - accuracy: 0.4879\n",
      "4311/9767 [============>.................] - ETA: 24s - loss: 1.0212 - accuracy: 0.4919Epoch 2/20\n",
      "1221/1221 [==============================] - 7s 5ms/step - loss: 1.0194 - accuracy: 0.4914\n",
      "Epoch 2/20\n",
      "1628/1628 [==============================] - 8s 5ms/step - loss: 1.0115 - accuracy: 0.4931\n",
      " 851/1221 [===================>..........] - ETA: 1s - loss: 1.0127 - accuracy: 0.4940Epoch 7/20\n",
      "1628/1628 [==============================] - 8s 5ms/step - loss: 1.0209 - accuracy: 0.4917\n",
      "Epoch 3/20\n",
      "1221/1221 [==============================] - 6s 5ms/step - loss: 1.0125 - accuracy: 0.4948\n",
      "7068/9767 [====================>.........] - ETA: 12s - loss: 1.0138 - accuracy: 0.4930Epoch 3/20\n",
      "1221/1221 [==============================] - 7s 5ms/step - loss: 1.0129 - accuracy: 0.4943\n",
      "Epoch 4/20\n",
      "1628/1628 [==============================] - 8s 5ms/step - loss: 1.0109 - accuracy: 0.4925\n",
      "8063/9767 [=======================>......] - ETA: 7s - loss: 1.0155 - accuracy: 0.4906Epoch 8/20\n",
      "1628/1628 [==============================] - 10s 6ms/step - loss: 1.0197 - accuracy: 0.4930\n",
      "Epoch 4/20\n",
      "1221/1221 [==============================] - 9s 8ms/step - loss: 1.0126 - accuracy: 0.4945\n",
      " 763/1628 [=============>................] - ETA: 7s - loss: 1.0194 - accuracy: 0.4916Epoch 5/20\n",
      "9767/9767 [==============================] - 51s 5ms/step - loss: 1.0138 - accuracy: 0.4940\n",
      "Epoch 12/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0105 - accuracy: 0.4926\n",
      "9027/9767 [==========================>...] - ETA: 3s - loss: 1.0224 - accuracy: 0.4905Epoch 9/20\n",
      "9767/9767 [==============================] - 51s 5ms/step - loss: 1.0155 - accuracy: 0.4903\n",
      "Epoch 12/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0203 - accuracy: 0.4922\n",
      " 826/1221 [===================>..........] - ETA: 3s - loss: 1.0119 - accuracy: 0.4941Epoch 5/20\n",
      "9767/9767 [==============================] - 54s 6ms/step - loss: 1.0221 - accuracy: 0.4910\n",
      "Epoch 12/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0116 - accuracy: 0.4948\n",
      "Epoch 6/20\n",
      "1628/1628 [==============================] - 18s 11ms/step - loss: 1.0102 - accuracy: 0.4931\n",
      " 818/1221 [===================>..........] - ETA: 5s - loss: 1.0101 - accuracy: 0.4950Epoch 10/20\n",
      "1911/9767 [====>.........................] - ETA: 1:28 - loss: 1.0181 - accuracy: 0.4846"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1628/1628 [==============================] - 19s 12ms/step - loss: 1.0193 - accuracy: 0.4929\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 1.0101 - accuracy: 0.4955Epoch 6/20\n",
      "1221/1221 [==============================] - 15s 12ms/step - loss: 1.0102 - accuracy: 0.4953\n",
      "Epoch 7/20\n",
      "2442/2442 [==============================] - 22s 9ms/step - loss: 1.0170 - accuracy: 0.4945\n",
      "2442/2442 [==============================] - 23s 9ms/step - loss: 1.0264 - accuracy: 0.4882\n",
      " 228/1628 [===>..........................] - ETA: 11s - loss: 1.0207 - accuracy: 0.4886Epoch 1/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0103 - accuracy: 0.4929\n",
      "2532/9767 [======>.......................] - ETA: 1:11 - loss: 1.0200 - accuracy: 0.4976Epoch 11/20\n",
      "1221/1221 [==============================] - 9s 8ms/step - loss: 1.0105 - accuracy: 0.4955\n",
      "Epoch 8/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0255 - accuracy: 0.4838\n",
      "Epoch 2/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0192 - accuracy: 0.4924\n",
      "Epoch 7/20\n",
      " 896/1221 [=====================>........] - ETA: 3s - loss: 1.0132 - accuracy: 0.4938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.0093 - accuracy: 0.4961\n",
      "3762/9767 [==========>...................] - ETA: 1:00 - loss: 1.0226 - accuracy: 0.4933Epoch 9/20\n",
      "1262/1628 [======================>.......] - ETA: 3s - loss: 1.0100 - accuracy: 0.4934Epoch 1/20\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.0147 - accuracy: 0.4918\n",
      "Epoch 3/20\n",
      "1628/1628 [==============================] - 15s 9ms/step - loss: 1.0096 - accuracy: 0.4943\n",
      "4156/9767 [===========>..................] - ETA: 54s - loss: 1.0224 - accuracy: 0.4933Epoch 12/20\n",
      "1628/1628 [==============================] - 17s 10ms/step - loss: 1.0189 - accuracy: 0.4929\n",
      "Epoch 8/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0094 - accuracy: 0.4957\n",
      " 837/1628 [==============>...............] - ETA: 7s - loss: 1.0072 - accuracy: 0.4973Epoch 10/20\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 1.0145 - accuracy: 0.4919\n",
      "1102/1628 [===================>..........] - ETA: 5s - loss: 1.0089 - accuracy: 0.4955Epoch 4/20\n",
      "1221/1221 [==============================] - 15s 11ms/step - loss: 1.0250 - accuracy: 0.4890\n",
      " 278/1221 [=====>........................] - ETA: 13s - loss: 1.0101 - accuracy: 0.4960Epoch 2/20\n",
      "1628/1628 [==============================] - 16s 10ms/step - loss: 1.0096 - accuracy: 0.4942\n",
      "Epoch 13/20\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.0102 - accuracy: 0.4957\n",
      "Epoch 11/20\n",
      "1628/1628 [==============================] - 15s 9ms/step - loss: 1.0189 - accuracy: 0.4924\n",
      "Epoch 9/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0134 - accuracy: 0.4920\n",
      "Epoch 5/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0208 - accuracy: 0.4919\n",
      "Epoch 3/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0088 - accuracy: 0.4936\n",
      "1149/1628 [====================>.........] - ETA: 3s - loss: 1.0191 - accuracy: 0.4916Epoch 14/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0087 - accuracy: 0.4963\n",
      "8517/9767 [=========================>....] - ETA: 11s - loss: 1.0140 - accuracy: 0.4951Epoch 12/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0131 - accuracy: 0.4910\n",
      "Epoch 6/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0200 - accuracy: 0.4920\n",
      "Epoch 4/20\n",
      "1628/1628 [==============================] - 12s 7ms/step - loss: 1.0192 - accuracy: 0.4922\n",
      "Epoch 10/20\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0093 - accuracy: 0.4955\n",
      "8717/9767 [=========================>....] - ETA: 9s - loss: 1.0218 - accuracy: 0.4918Epoch 13/20\n",
      "9767/9767 [==============================] - 88s 9ms/step - loss: 1.0136 - accuracy: 0.4950\n",
      "Epoch 13/20\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0129 - accuracy: 0.4921\n",
      "Epoch 7/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0206 - accuracy: 0.4912\n",
      "Epoch 5/20\n",
      "1628/1628 [==============================] - 14s 8ms/step - loss: 1.0091 - accuracy: 0.4940\n",
      "  25/1221 [..............................] - ETA: 7s - loss: 1.0301 - accuracy: 0.4720Epoch 15/20\n",
      "9767/9767 [==============================] - 89s 9ms/step - loss: 1.0154 - accuracy: 0.4899\n",
      "Epoch 13/20\n",
      "1628/1628 [==============================] - 14s 9ms/step - loss: 1.0186 - accuracy: 0.4928\n",
      "Epoch 11/20\n",
      "9767/9767 [==============================] - 88s 9ms/step - loss: 1.0219 - accuracy: 0.4912\n",
      "Epoch 13/20\n",
      "1221/1221 [==============================] - 9s 8ms/step - loss: 1.0091 - accuracy: 0.4959\n",
      " 882/1221 [====================>.........] - ETA: 2s - loss: 1.0203 - accuracy: 0.4907Epoch 14/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0122 - accuracy: 0.4930\n",
      "Epoch 8/20\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0195 - accuracy: 0.4920\n",
      "1598/9767 [===>..........................] - ETA: 1:05 - loss: 1.0171 - accuracy: 0.4929Epoch 6/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0087 - accuracy: 0.4937\n",
      "2071/9767 [=====>........................] - ETA: 1:00 - loss: 1.0165 - accuracy: 0.4933Epoch 16/20\n",
      "1221/1221 [==============================] - 10s 9ms/step - loss: 1.0090 - accuracy: 0.4960\n",
      " 454/1628 [=======>......................] - ETA: 8s - loss: 1.0108 - accuracy: 0.4928Epoch 15/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0187 - accuracy: 0.4921\n",
      "Epoch 12/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0120 - accuracy: 0.4927\n",
      "Epoch 9/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0192 - accuracy: 0.4920\n",
      "Epoch 7/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0087 - accuracy: 0.4946\n",
      "2601/9767 [======>.......................] - ETA: 58s - loss: 1.0232 - accuracy: 0.4873Epoch 17/20\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0097 - accuracy: 0.4958\n",
      " 979/1221 [=======================>......] - ETA: 1s - loss: 1.0107 - accuracy: 0.4947Epoch 16/20\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0118 - accuracy: 0.4930\n",
      "Epoch 10/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0191 - accuracy: 0.4927\n",
      "Epoch 8/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0187 - accuracy: 0.4921\n",
      "Epoch 13/20\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0087 - accuracy: 0.4958\n",
      "3895/9767 [==========>...................] - ETA: 47s - loss: 1.0209 - accuracy: 0.4904Epoch 17/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0117 - accuracy: 0.4933\n",
      "Epoch 11/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0194 - accuracy: 0.4924\n",
      "5435/9767 [===============>..............] - ETA: 32s - loss: 1.0157 - accuracy: 0.4943Epoch 9/20\n",
      "1628/1628 [==============================] - 14s 8ms/step - loss: 1.0083 - accuracy: 0.4942\n",
      "Epoch 18/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0184 - accuracy: 0.4916\n",
      "Epoch 14/20\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0088 - accuracy: 0.4955\n",
      " 817/1628 [==============>...............] - ETA: 6s - loss: 1.0087 - accuracy: 0.4944Epoch 18/20\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 1.0113 - accuracy: 0.4932\n",
      " 613/1628 [==========>...................] - ETA: 13s - loss: 1.0210 - accuracy: 0.4903Epoch 12/20\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 1.0186 - accuracy: 0.4924\n",
      "Epoch 10/20\n",
      "1628/1628 [==============================] - 19s 12ms/step - loss: 1.0083 - accuracy: 0.4943\n",
      "5933/9767 [=================>............] - ETA: 34s - loss: 1.0214 - accuracy: 0.4915Epoch 19/20\n",
      "1221/1221 [==============================] - 16s 13ms/step - loss: 1.0086 - accuracy: 0.4963\n",
      "Epoch 19/20\n",
      "1628/1628 [==============================] - 20s 12ms/step - loss: 1.0186 - accuracy: 0.4918\n",
      "Epoch 15/20\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 1.0108 - accuracy: 0.4936\n",
      "Epoch 13/20\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 1.0187 - accuracy: 0.4923\n",
      "Epoch 11/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0089 - accuracy: 0.4954\n",
      "8486/9767 [=========================>....] - ETA: 11s - loss: 1.0149 - accuracy: 0.4924Epoch 20/20\n",
      "1628/1628 [==============================] - 15s 9ms/step - loss: 1.0084 - accuracy: 0.4945\n",
      "Epoch 20/20\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.0113 - accuracy: 0.4935\n",
      "Epoch 14/20\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.0184 - accuracy: 0.4927\n",
      "Epoch 12/20\n",
      "1628/1628 [==============================] - 15s 9ms/step - loss: 1.0182 - accuracy: 0.4917\n",
      "9221/9767 [===========================>..] - ETA: 4s - loss: 1.0136 - accuracy: 0.4952Epoch 16/20\n",
      "9767/9767 [==============================] - 85s 9ms/step - loss: 1.0137 - accuracy: 0.4948\n",
      " 531/1628 [========>.....................] - ETA: 7s - loss: 1.0179 - accuracy: 0.4943Epoch 14/20\n",
      "9767/9767 [==============================] - 85s 9ms/step - loss: 1.0154 - accuracy: 0.4913\n",
      "Epoch 14/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0082 - accuracy: 0.4956\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0106 - accuracy: 0.4937\n",
      "Epoch 15/20\n",
      "1628/1628 [==============================] - 14s 9ms/step - loss: 1.0084 - accuracy: 0.4948\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0182 - accuracy: 0.4922\n",
      " 661/9767 [=>............................] - ETA: 1:35 - loss: 1.0187 - accuracy: 0.4850Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611/611 [==============================] - 4s 6ms/step - loss: 1.0636 - accuracy: 0.4902\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0183 - accuracy: 0.4937\n",
      "1072/9767 [==>...........................] - ETA: 1:12 - loss: 1.0190 - accuracy: 0.4859Epoch 17/20\n",
      "1189/9767 [==>...........................] - ETA: 1:09 - loss: 1.0180 - accuracy: 0.4864Epoch 1/20\n",
      "9767/9767 [==============================] - 88s 9ms/step - loss: 1.0222 - accuracy: 0.4905\n",
      "Epoch 14/20\n",
      "1221/1221 [==============================] - 9s 8ms/step - loss: 1.0104 - accuracy: 0.4930\n",
      "Epoch 16/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0184 - accuracy: 0.4920\n",
      "Epoch 14/20\n",
      "977/977 [==============================] - 8s 7ms/step - loss: 1.0208 - accuracy: 0.4883\n",
      "Epoch 2/20\n",
      "1628/1628 [==============================] - 13s 8ms/step - loss: 1.0187 - accuracy: 0.4920\n",
      "Epoch 18/20\n",
      "1612/9767 [===>..........................] - ETA: 1:04 - loss: 1.0224 - accuracy: 0.4907"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0102 - accuracy: 0.4935\n",
      "Epoch 17/20\n",
      "814/814 [==============================] - 6s 7ms/step - loss: 1.0215 - accuracy: 0.4957\n",
      "816/977 [========================>.....] - ETA: 1s - loss: 1.0131 - accuracy: 0.4941Epoch 1/20\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0184 - accuracy: 0.4928\n",
      "Epoch 15/20\n",
      "977/977 [==============================] - 8s 9ms/step - loss: 1.0128 - accuracy: 0.4941\n",
      "Epoch 3/20\n",
      "1221/1221 [==============================] - 10s 8ms/step - loss: 1.0103 - accuracy: 0.4930\n",
      "916/977 [===========================>..] - ETA: 0s - loss: 1.0118 - accuracy: 0.4953Epoch 18/20\n",
      "977/977 [==============================] - 8s 8ms/step - loss: 1.0125 - accuracy: 0.4942\n",
      "  63/1221 [>.............................] - ETA: 8s - loss: 1.0129 - accuracy: 0.4913Epoch 4/20\n",
      "977/977 [==============================] - 10s 8ms/step - loss: 1.0198 - accuracy: 0.4889\n",
      "Epoch 2/20\n",
      "1628/1628 [==============================] - 15s 9ms/step - loss: 1.0180 - accuracy: 0.4925\n",
      "Epoch 19/20\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.0180 - accuracy: 0.4924\n",
      "Epoch 16/20\n",
      "977/977 [==============================] - 12s 12ms/step - loss: 1.0127 - accuracy: 0.4951\n",
      "1044/1221 [========================>.....] - ETA: 2s - loss: 1.0110 - accuracy: 0.4933Epoch 5/20\n",
      "977/977 [==============================] - 12s 12ms/step - loss: 1.0140 - accuracy: 0.4915\n",
      "1111/1221 [==========================>...] - ETA: 1s - loss: 1.0112 - accuracy: 0.4927Epoch 3/20\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 1.0107 - accuracy: 0.4933\n",
      "Epoch 19/20\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 1.0180 - accuracy: 0.4929\n",
      "Epoch 17/20\n",
      "1628/1628 [==============================] - 15s 9ms/step - loss: 1.0185 - accuracy: 0.4918\n",
      "804/977 [=======================>......] - ETA: 1s - loss: 1.0116 - accuracy: 0.4946Epoch 20/20\n",
      "977/977 [==============================] - 9s 9ms/step - loss: 1.0118 - accuracy: 0.4949\n",
      "Epoch 6/20\n",
      "977/977 [==============================] - 9s 9ms/step - loss: 1.0138 - accuracy: 0.4912\n",
      " 534/1221 [============>.................] - ETA: 7s - loss: 1.0183 - accuracy: 0.4932Epoch 4/20\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.0099 - accuracy: 0.4940\n",
      "Epoch 20/20\n",
      "1221/1221 [==============================] - 11s 9ms/step - loss: 1.0177 - accuracy: 0.4931\n",
      " 908/1628 [===============>..............] - ETA: 6s - loss: 1.0181 - accuracy: 0.4910Epoch 18/20\n",
      "977/977 [==============================] - 8s 8ms/step - loss: 1.0118 - accuracy: 0.4953\n",
      "Epoch 7/20\n",
      "977/977 [==============================] - 8s 8ms/step - loss: 1.0131 - accuracy: 0.4925\n",
      "Epoch 5/20\n",
      "1628/1628 [==============================] - 17s 10ms/step - loss: 1.0184 - accuracy: 0.4925\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 1.0102 - accuracy: 0.4938\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.0178 - accuracy: 0.4932\n",
      "Epoch 19/20\n",
      "977/977 [==============================] - 10s 10ms/step - loss: 1.0137 - accuracy: 0.4916\n",
      "Epoch 6/20\n",
      "977/977 [==============================] - 10s 11ms/step - loss: 1.0112 - accuracy: 0.4953\n",
      "8110/9767 [=======================>......] - ETA: 14s - loss: 1.0157 - accuracy: 0.4908Epoch 8/20\n",
      "611/611 [==============================] - 4s 6ms/step - loss: 1.0205 - accuracy: 0.4940\n",
      "8618/9767 [=========================>....] - ETA: 10s - loss: 1.0144 - accuracy: 0.4939"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8642/9767 [=========================>....] - ETA: 9s - loss: 1.0144 - accuracy: 0.4940 Epoch 1/20\n",
      "977/977 [==============================] - 7s 7ms/step - loss: 1.0129 - accuracy: 0.4921\n",
      "9277/9767 [===========================>..] - ETA: 4s - loss: 1.0139 - accuracy: 0.4942Epoch 7/20\n",
      "977/977 [==============================] - 7s 7ms/step - loss: 1.0103 - accuracy: 0.4959\n",
      "Epoch 9/20\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 1.0175 - accuracy: 0.4941\n",
      "8038/9767 [=======================>......] - ETA: 15s - loss: 1.0230 - accuracy: 0.4902Epoch 20/20\n",
      "9767/9767 [==============================] - 83s 8ms/step - loss: 1.0135 - accuracy: 0.4949\n",
      "Epoch 15/20\n",
      "977/977 [==============================] - 7s 6ms/step - loss: 1.0256 - accuracy: 0.4876\n",
      "9649/9767 [============================>.] - ETA: 0s - loss: 1.0159 - accuracy: 0.4905Epoch 2/20\n",
      "9767/9767 [==============================] - 81s 8ms/step - loss: 1.0158 - accuracy: 0.4908\n",
      "Epoch 15/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0127 - accuracy: 0.4920\n",
      "Epoch 8/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0107 - accuracy: 0.4952\n",
      "Epoch 10/20\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 1.0174 - accuracy: 0.4936\n",
      "977/977 [==============================] - 6s 7ms/step - loss: 1.0191 - accuracy: 0.4929\n",
      " 887/9767 [=>............................] - ETA: 58s - loss: 1.0091 - accuracy: 0.4984Epoch 3/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0124 - accuracy: 0.4928\n",
      "1463/9767 [===>..........................] - ETA: 53s - loss: 1.0145 - accuracy: 0.4953Epoch 9/20\n",
      "9767/9767 [==============================] - 81s 8ms/step - loss: 1.0218 - accuracy: 0.4912\n",
      "Epoch 15/20\n",
      "977/977 [==============================] - 6s 7ms/step - loss: 1.0105 - accuracy: 0.4952\n",
      "Epoch 11/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0193 - accuracy: 0.4925\n",
      "Epoch 4/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0113 - accuracy: 0.4924\n",
      "366/977 [==========>...................] - ETA: 2s - loss: 1.0217 - accuracy: 0.4895Epoch 10/20\n",
      "977/977 [==============================] - 5s 6ms/step - loss: 1.0104 - accuracy: 0.4958\n",
      "Epoch 12/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0187 - accuracy: 0.4929\n",
      "Epoch 5/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0104 - accuracy: 0.4935\n",
      "Epoch 11/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0101 - accuracy: 0.4953\n",
      "Epoch 13/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0191 - accuracy: 0.4924\n",
      "Epoch 6/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0100 - accuracy: 0.4931\n",
      "806/977 [=======================>......] - ETA: 0s - loss: 1.0104 - accuracy: 0.4954Epoch 12/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0100 - accuracy: 0.4958\n",
      "Epoch 14/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0182 - accuracy: 0.4926\n",
      "Epoch 7/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0093 - accuracy: 0.4935\n",
      "5214/9767 [===============>..............] - ETA: 24s - loss: 1.0137 - accuracy: 0.4923Epoch 13/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0097 - accuracy: 0.4956\n",
      "Epoch 15/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0184 - accuracy: 0.4928\n",
      "Epoch 8/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0089 - accuracy: 0.4937\n",
      "Epoch 14/20\n",
      "814/814 [==============================] - 4s 5ms/step - loss: 1.0096 - accuracy: 0.4960\n",
      "977/977 [==============================] - 7s 7ms/step - loss: 1.0095 - accuracy: 0.4959\n",
      "Epoch 16/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0187 - accuracy: 0.4930\n",
      "Epoch 9/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0086 - accuracy: 0.4937\n",
      "Epoch 15/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0098 - accuracy: 0.4960\n",
      "154/977 [===>..........................] - ETA: 4s - loss: 1.0037 - accuracy: 0.4992Epoch 17/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0188 - accuracy: 0.4926\n",
      "8194/9767 [========================>.....] - ETA: 8s - loss: 1.0130 - accuracy: 0.4944Epoch 10/20\n",
      "977/977 [==============================] - 8s 9ms/step - loss: 1.0087 - accuracy: 0.4940\n",
      "Epoch 16/20\n",
      "977/977 [==============================] - 9s 9ms/step - loss: 1.0095 - accuracy: 0.4961\n",
      "Epoch 18/20\n",
      "977/977 [==============================] - 10s 10ms/step - loss: 1.0183 - accuracy: 0.4932\n",
      "Epoch 11/20\n",
      "977/977 [==============================] - 9s 9ms/step - loss: 1.0082 - accuracy: 0.4930\n",
      "Epoch 17/20\n",
      "611/611 [==============================] - 6s 9ms/step - loss: 1.0108 - accuracy: 0.4937\n",
      "977/977 [==============================] - 9s 9ms/step - loss: 1.0096 - accuracy: 0.4959\n",
      "Epoch 19/20\n",
      "9767/9767 [==============================] - 62s 6ms/step - loss: 1.0138 - accuracy: 0.4939\n",
      "566/977 [================>.............] - ETA: 4s - loss: 1.0167 - accuracy: 0.4950Epoch 16/20\n",
      "9767/9767 [==============================] - 64s 7ms/step - loss: 1.0152 - accuracy: 0.4904\n",
      "475/977 [=============>................] - ETA: 6s - loss: 1.0080 - accuracy: 0.4927Epoch 16/20\n",
      "977/977 [==============================] - 11s 11ms/step - loss: 1.0184 - accuracy: 0.4931\n",
      "Epoch 12/20\n",
      "977/977 [==============================] - 10s 10ms/step - loss: 1.0082 - accuracy: 0.4935\n",
      "8770/9767 [=========================>....] - ETA: 6s - loss: 1.0218 - accuracy: 0.4919Epoch 18/20\n",
      "977/977 [==============================] - 10s 10ms/step - loss: 1.0097 - accuracy: 0.4956\n",
      "8890/9767 [==========================>...] - ETA: 6s - loss: 1.0218 - accuracy: 0.4918Epoch 20/20\n",
      "977/977 [==============================] - 7s 7ms/step - loss: 1.0185 - accuracy: 0.4931\n",
      "Epoch 13/20\n",
      "977/977 [==============================] - 8s 9ms/step - loss: 1.0085 - accuracy: 0.4935\n",
      "887/977 [==========================>...] - ETA: 0s - loss: 1.0100 - accuracy: 0.4946Epoch 19/20\n",
      "9767/9767 [==============================] - 69s 7ms/step - loss: 1.0217 - accuracy: 0.4921\n",
      "453/977 [============>.................] - ETA: 5s - loss: 1.0204 - accuracy: 0.4925Epoch 16/20\n",
      "977/977 [==============================] - 8s 9ms/step - loss: 1.0096 - accuracy: 0.4953\n",
      "977/977 [==============================] - 9s 9ms/step - loss: 1.0185 - accuracy: 0.4930\n",
      "Epoch 14/20\n",
      "489/489 [==============================] - 4s 6ms/step - loss: 1.0524 - accuracy: 0.4892\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0075 - accuracy: 0.4937\n",
      "Epoch 20/20\n",
      "977/977 [==============================] - 6s 6ms/step - loss: 1.0182 - accuracy: 0.4927\n",
      "3067/9767 [========>.....................] - ETA: 48s - loss: 1.0132 - accuracy: 0.4959Epoch 15/20\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0085 - accuracy: 0.4945\n",
      "489/489 [==============================] - 3s 5ms/step - loss: 1.0245 - accuracy: 0.4961\n",
      "977/977 [==============================] - 5s 5ms/step - loss: 1.0184 - accuracy: 0.4927\n",
      "Epoch 16/20\n",
      "977/977 [==============================] - 4s 4ms/step - loss: 1.0181 - accuracy: 0.4924\n",
      "Epoch 17/20\n",
      "977/977 [==============================] - 3s 3ms/step - loss: 1.0179 - accuracy: 0.4924\n",
      "Epoch 18/20\n",
      "977/977 [==============================] - 3s 3ms/step - loss: 1.0182 - accuracy: 0.4926\n",
      "Epoch 19/20\n",
      "977/977 [==============================] - 3s 3ms/step - loss: 1.0183 - accuracy: 0.4932\n",
      "Epoch 20/20\n",
      "977/977 [==============================] - 3s 3ms/step - loss: 1.0180 - accuracy: 0.4930\n",
      "9767/9767 [==============================] - 49s 5ms/step - loss: 1.0142 - accuracy: 0.4944\n",
      "Epoch 17/20\n",
      "489/489 [==============================] - 2s 3ms/step - loss: 1.0115 - accuracy: 0.4969\n",
      "9767/9767 [==============================] - 46s 5ms/step - loss: 1.0152 - accuracy: 0.4910\n",
      "Epoch 17/20\n",
      "9767/9767 [==============================] - 37s 4ms/step - loss: 1.0219 - accuracy: 0.4905\n",
      "1733/9767 [====>.........................] - ETA: 20s - loss: 1.0148 - accuracy: 0.4921Epoch 17/20\n",
      "9767/9767 [==============================] - 25s 3ms/step - loss: 1.0138 - accuracy: 0.4949\n",
      "7502/9767 [======================>.......] - ETA: 5s - loss: 1.0215 - accuracy: 0.4930Epoch 18/20\n",
      "9767/9767 [==============================] - 25s 3ms/step - loss: 1.0150 - accuracy: 0.4914\n",
      "Epoch 18/20\n",
      "9767/9767 [==============================] - 25s 3ms/step - loss: 1.0223 - accuracy: 0.4916\n",
      "Epoch 18/20\n",
      "9767/9767 [==============================] - 25s 3ms/step - loss: 1.0138 - accuracy: 0.4942\n",
      "Epoch 19/20\n",
      "9767/9767 [==============================] - 25s 3ms/step - loss: 1.0152 - accuracy: 0.4906\n",
      "Epoch 19/20\n",
      "9767/9767 [==============================] - 25s 3ms/step - loss: 1.0220 - accuracy: 0.4906\n",
      "Epoch 19/20\n",
      "9767/9767 [==============================] - 25s 3ms/step - loss: 1.0136 - accuracy: 0.4953\n",
      "Epoch 20/20\n",
      "9767/9767 [==============================] - 25s 3ms/step - loss: 1.0144 - accuracy: 0.4911\n",
      "Epoch 20/20\n",
      "9767/9767 [==============================] - 25s 3ms/step - loss: 1.0217 - accuracy: 0.4916\n",
      "Epoch 20/20\n",
      "9767/9767 [==============================] - 24s 2ms/step - loss: 1.0137 - accuracy: 0.4948\n",
      "9767/9767 [==============================] - 24s 2ms/step - loss: 1.0149 - accuracy: 0.4909\n",
      "9767/9767 [==============================] - 23s 2ms/step - loss: 1.0226 - accuracy: 0.4910\n",
      "4884/4884 [==============================] - 8s 2ms/step - loss: 1.0246 - accuracy: 0.4948\n",
      "4884/4884 [==============================] - 8s 2ms/step - loss: 1.0067 - accuracy: 0.4957\n",
      "4884/4884 [==============================] - 7s 1ms/step - loss: 1.0701 - accuracy: 0.4879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2442/2442 [==============================] - 7s 2ms/step - loss: 1.0217 - accuracy: 0.4885\n",
      "Epoch 2/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0166 - accuracy: 0.4922\n",
      "Epoch 3/20\n",
      "2442/2442 [==============================] - 5s 2ms/step - loss: 1.0167 - accuracy: 0.4924\n",
      "Epoch 4/20\n",
      "2442/2442 [==============================] - 5s 2ms/step - loss: 1.0156 - accuracy: 0.4925\n",
      "Epoch 5/20\n",
      "2442/2442 [==============================] - 5s 2ms/step - loss: 1.0153 - accuracy: 0.4922\n",
      "Epoch 6/20\n",
      "2442/2442 [==============================] - 5s 2ms/step - loss: 1.0151 - accuracy: 0.4928\n",
      "Epoch 7/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0149 - accuracy: 0.4928\n",
      "Epoch 8/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0150 - accuracy: 0.4926\n",
      "Epoch 9/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0145 - accuracy: 0.4930\n",
      "Epoch 10/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0143 - accuracy: 0.4928\n",
      "Epoch 11/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0148 - accuracy: 0.4927\n",
      "Epoch 12/20\n",
      "2442/2442 [==============================] - 5s 2ms/step - loss: 1.0143 - accuracy: 0.4935\n",
      "Epoch 13/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0142 - accuracy: 0.4930\n",
      "Epoch 14/20\n",
      "2442/2442 [==============================] - 5s 2ms/step - loss: 1.0142 - accuracy: 0.4931\n",
      "Epoch 15/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0144 - accuracy: 0.4930\n",
      "Epoch 16/20\n",
      "2442/2442 [==============================] - 5s 2ms/step - loss: 1.0141 - accuracy: 0.4931\n",
      "Epoch 17/20\n",
      "2442/2442 [==============================] - 5s 2ms/step - loss: 1.0143 - accuracy: 0.4931\n",
      "Epoch 18/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0142 - accuracy: 0.4928\n",
      "Epoch 19/20\n",
      "2442/2442 [==============================] - 6s 2ms/step - loss: 1.0141 - accuracy: 0.4937\n",
      "Epoch 20/20\n",
      "2442/2442 [==============================] - 5s 2ms/step - loss: 1.0138 - accuracy: 0.4927\n",
      "Best: 0.494266 using {'batch_size': 60}\n",
      "0.492805 (0.003494) with: {'batch_size': 10}\n",
      "0.492969 (0.003411) with: {'batch_size': 20}\n",
      "0.492143 (0.002378) with: {'batch_size': 40}\n",
      "0.494266 (0.002212) with: {'batch_size': 60}\n",
      "0.492635 (0.001709) with: {'batch_size': 80}\n",
      "0.494089 (0.003488) with: {'batch_size': 100}\n"
     ]
    }
   ],
   "source": [
    "# define the grid search parameters\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "param_grid = dict(batch_size=batch_size)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X, Y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 60}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(5, activation='relu', input_shape=(7,)))\n",
    "    network.add(layers.Dense(10, activation='relu'))\n",
    "    network.add(layers.Dense(3, activation='softmax'))\n",
    "    #\n",
    "    # Configure the network with optimizer, loss function and accuracy\n",
    "    #\n",
    "    network.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10289/2379273355.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_classifier, epochs=20)\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model = KerasClassifier(build_fn=build_classifier, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0240 - accuracy: 0.4873\n",
      "Epoch 2/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0189 - accuracy: 0.4908\n",
      "Epoch 3/20\n",
      "3205/3205 [==============================] - 7s 2ms/step - loss: 1.0186 - accuracy: 0.4897\n",
      "Epoch 4/20\n",
      "3205/3205 [==============================] - 7s 2ms/step - loss: 1.0187 - accuracy: 0.4903\n",
      "Epoch 5/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0184 - accuracy: 0.4907\n",
      "Epoch 6/20\n",
      "3205/3205 [==============================] - 7s 2ms/step - loss: 1.0177 - accuracy: 0.4902\n",
      "Epoch 7/20\n",
      "3205/3205 [==============================] - 7s 2ms/step - loss: 1.0176 - accuracy: 0.4908\n",
      "Epoch 8/20\n",
      "3205/3205 [==============================] - 7s 2ms/step - loss: 1.0171 - accuracy: 0.4909\n",
      "Epoch 9/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0177 - accuracy: 0.4913\n",
      "Epoch 10/20\n",
      "3205/3205 [==============================] - 7s 2ms/step - loss: 1.0170 - accuracy: 0.4915\n",
      "Epoch 11/20\n",
      "3205/3205 [==============================] - 9s 3ms/step - loss: 1.0171 - accuracy: 0.4913\n",
      "Epoch 12/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0171 - accuracy: 0.4920\n",
      "Epoch 13/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0170 - accuracy: 0.4915\n",
      "Epoch 14/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0167 - accuracy: 0.4922\n",
      "Epoch 15/20\n",
      "3205/3205 [==============================] - 7s 2ms/step - loss: 1.0172 - accuracy: 0.4925\n",
      "Epoch 16/20\n",
      "3205/3205 [==============================] - 7s 2ms/step - loss: 1.0170 - accuracy: 0.4917\n",
      "Epoch 17/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0169 - accuracy: 0.4915\n",
      "Epoch 18/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0167 - accuracy: 0.4920\n",
      "Epoch 19/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0164 - accuracy: 0.4921\n",
      "Epoch 20/20\n",
      "3205/3205 [==============================] - 6s 2ms/step - loss: 1.0169 - accuracy: 0.4915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcbac5ef1c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43950,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 12:59:38.556658: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 12:59:38.556708: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 12:59:38.556746: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 12:59:38.558092: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/christopher/.local/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(5, activation='relu', input_shape=(7,)))\n",
    "network.add(layers.Dense(10, activation='relu'))\n",
    "network.add(layers.Dense(3, activation='softmax'))\n",
    "#\n",
    "# Configure the network with optimizer, loss function and accuracy\n",
    "#\n",
    "network.compile(optimizer=optimizers.Adam(lr=0.01),\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102548, 3, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 3, 3) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000033?line=0'>1</a>\u001b[0m network\u001b[39m.\u001b[39;49mfit(X_train_std,Y_train)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1144'>1145</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1145'>1146</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1146'>1147</a>\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1147'>1148</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py?line=1148'>1149</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/home/christopher/.local/lib/python3.8/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 3, 3) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "network.fit(X_train_std,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_x=network.predict(X_test_std) \n",
    "classes_x=np.argmax(predict_x,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of multiclass and continuous-multioutput targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000035?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000035?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000035?line=2'>3</a>\u001b[0m model_conf \u001b[39m=\u001b[39m confusion_matrix(np\u001b[39m.\u001b[39;49margmax(Y_test, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), y_pred)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000035?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(model_conf)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:307\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=221'>222</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconfusion_matrix\u001b[39m(\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=222'>223</a>\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, normalize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=223'>224</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=224'>225</a>\u001b[0m     \u001b[39m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=225'>226</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=226'>227</a>\u001b[0m \u001b[39m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=304'>305</a>\u001b[0m \u001b[39m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=305'>306</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=306'>307</a>\u001b[0m     y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=307'>308</a>\u001b[0m     \u001b[39mif\u001b[39;00m y_type \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=308'>309</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not supported\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m y_type)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=89'>90</a>\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=91'>92</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=92'>93</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=93'>94</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=94'>95</a>\u001b[0m             type_true, type_pred\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=95'>96</a>\u001b[0m         )\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=96'>97</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=98'>99</a>\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py?line=99'>100</a>\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of multiclass and continuous-multioutput targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "model_conf = confusion_matrix(np.argmax(Y_test, axis=1), y_pred)\n",
    "print(model_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=0'>1</a>\u001b[0m nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras \u001b[39m=\u001b[39m roc_curve(Y_test, y_pred)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=1'>2</a>\u001b[0m auc_keras \u001b[39m=\u001b[39m auc(nn_fpr_keras, nn_tpr_keras)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(nn_fpr_keras, nn_tpr_keras, marker\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNeural Network (auc = \u001b[39m\u001b[39m%0.3f\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m auc_keras)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:962\u001b[0m, in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=872'>873</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroc_curve\u001b[39m(\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=873'>874</a>\u001b[0m     y_true, y_score, \u001b[39m*\u001b[39m, pos_label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, drop_intermediate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=874'>875</a>\u001b[0m ):\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=875'>876</a>\u001b[0m     \u001b[39m\"\"\"Compute Receiver operating characteristic (ROC).\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=876'>877</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=877'>878</a>\u001b[0m \u001b[39m    Note: this implementation is restricted to the binary classification task.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=959'>960</a>\u001b[0m \n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=960'>961</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=961'>962</a>\u001b[0m     fps, tps, thresholds \u001b[39m=\u001b[39m _binary_clf_curve(\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=962'>963</a>\u001b[0m         y_true, y_score, pos_label\u001b[39m=\u001b[39;49mpos_label, sample_weight\u001b[39m=\u001b[39;49msample_weight\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=963'>964</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=965'>966</a>\u001b[0m     \u001b[39m# Attempt to drop thresholds corresponding to points in between and\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=966'>967</a>\u001b[0m     \u001b[39m# collinear with other points. These are always suboptimal and do not\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=967'>968</a>\u001b[0m     \u001b[39m# appear on a plotted ROC curve (and thus do not affect the AUC).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=972'>973</a>\u001b[0m     \u001b[39m# but does not drop more complicated cases like fps = [1, 3, 7],\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=973'>974</a>\u001b[0m     \u001b[39m# tps = [1, 2, 4]; there is no harm in keeping too many thresholds.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=974'>975</a>\u001b[0m     \u001b[39mif\u001b[39;00m drop_intermediate \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(fps) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:731\u001b[0m, in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=728'>729</a>\u001b[0m y_type \u001b[39m=\u001b[39m type_of_target(y_true)\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=729'>730</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m (y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m pos_label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m)):\n\u001b[0;32m--> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=730'>731</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m format is not supported\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(y_type))\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=732'>733</a>\u001b[0m check_consistent_length(y_true, y_score, sample_weight)\n\u001b[1;32m    <a href='file:///home/christopher/.local/lib/python3.8/site-packages/sklearn/metrics/_ranking.py?line=733'>734</a>\u001b[0m y_true \u001b[39m=\u001b[39m column_or_1d(y_true)\n",
      "\u001b[0;31mValueError\u001b[0m: multilabel-indicator format is not supported"
     ]
    }
   ],
   "source": [
    "nn_fpr_keras, nn_tpr_keras, nn_thresholds_keras = roc_curve(Y_test, y_pred)\n",
    "auc_keras = auc(nn_fpr_keras, nn_tpr_keras)\n",
    "plt.plot(nn_fpr_keras, nn_tpr_keras, marker='.', label='Neural Network (auc = %0.3f)' % auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fpr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 33'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=1'>2</a>\u001b[0m n_classes \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=2'>3</a>\u001b[0m \u001b[39m# First aggregate all false positive rates\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=3'>4</a>\u001b[0m all_fpr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39mconcatenate([fpr[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_classes)]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=5'>6</a>\u001b[0m \u001b[39m# Then interpolate all ROC curves at this points\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=6'>7</a>\u001b[0m mean_tpr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(all_fpr)\n",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 33'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=1'>2</a>\u001b[0m n_classes \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=2'>3</a>\u001b[0m \u001b[39m# First aggregate all false positive rates\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=3'>4</a>\u001b[0m all_fpr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(np\u001b[39m.\u001b[39mconcatenate([fpr[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_classes)]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=5'>6</a>\u001b[0m \u001b[39m# Then interpolate all ROC curves at this points\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000032?line=6'>7</a>\u001b[0m mean_tpr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(all_fpr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fpr' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "n_classes = 3\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.figure()\n",
    "plt.plot(\n",
    "    fpr[\"micro\"],\n",
    "    tpr[\"micro\"],\n",
    "    label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "    color=\"deeppink\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(\n",
    "        fpr[i],\n",
    "        tpr[i],\n",
    "        color=color,\n",
    "        lw=lw,\n",
    "        label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i]),\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Some extension of Receiver operating characteristic to multiclass\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(10, activation='relu', input_shape=(2,)))\n",
    "    network.add(layers.Dense(7, activation='relu'))\n",
    "    network.add(layers.Dense(5, activation='relu'))\n",
    "    network.add(layers.Dense(3, activation='softmax'))\n",
    "    #\n",
    "    # Configure the network with optimizer, loss function and accuracy\n",
    "    #\n",
    "    network.compile(optimizer=optimizers.Adam(),\n",
    "                    loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3808/2005499737.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=build_classifier, epochs=20, batch_size=100)\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "model = KerasClassifier(build_fn=build_classifier, epochs=20, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, dummy_y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 1s 3ms/step - loss: 1.3457 - accuracy: 0.3524\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.3147 - accuracy: 0.3524\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2864 - accuracy: 0.3524\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.2609 - accuracy: 0.3524\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2403 - accuracy: 0.3524\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 1.2190 - accuracy: 0.3524\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.2016 - accuracy: 0.3524\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.1844 - accuracy: 0.3524\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.1696 - accuracy: 0.3524\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1566 - accuracy: 0.3524\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1450 - accuracy: 0.3524\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.1376 - accuracy: 0.3524\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.1319 - accuracy: 0.3524\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.1282 - accuracy: 0.3524\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 1.1254 - accuracy: 0.3524\n",
      "Epoch 16/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.1234 - accuracy: 0.3524\n",
      "Epoch 17/20\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 1.1217 - accuracy: 0.3524\n",
      "Epoch 18/20\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 1.1203 - accuracy: 0.3524\n",
      "Epoch 19/20\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 1.1192 - accuracy: 0.3524\n",
      "Epoch 20/20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1181 - accuracy: 0.3524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40d31e92e0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "3/3 [==============================] - 1s 4ms/step - loss: 1.0515 - accuracy: 0.5048\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.8565 - accuracy: 0.5905\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7616 - accuracy: 0.6381\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.7256 - accuracy: 0.6667\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.6362 - accuracy: 0.9143\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.6062 - accuracy: 0.7429\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5856 - accuracy: 0.8381\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5591 - accuracy: 0.8667\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5268 - accuracy: 0.7619\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.5047 - accuracy: 0.8571\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4870 - accuracy: 0.9143\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4637 - accuracy: 0.9714\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4591 - accuracy: 0.8381\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4575 - accuracy: 0.8190\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4356 - accuracy: 0.9048\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.4160 - accuracy: 0.9143\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.3943 - accuracy: 0.9714\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3907 - accuracy: 0.8857\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.3728 - accuracy: 0.8952\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.3690 - accuracy: 0.9238\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f40d3044910>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the network\n",
    "#\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(4,)))\n",
    "network.add(layers.Dense(3, activation='softmax'))\n",
    "#\n",
    "# Compile the network\n",
    "#\n",
    "network.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "#\n",
    "# Load the iris dataset\n",
    "#\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "#\n",
    "# Create training and test split\n",
    "#\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "#\n",
    "# Create categorical labels\n",
    "#\n",
    "train_labels = to_categorical(y_train)\n",
    "test_labels = to_categorical(y_test)\n",
    "#\n",
    "# Fit the neural network\n",
    "#\n",
    "network.fit(X_train, train_labels, epochs=20, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = network.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.2029937e-03, 5.7934624e-01, 4.1745070e-01],\n",
       "       [3.0325770e-02, 7.1302366e-01, 2.5665057e-01],\n",
       "       [1.5497947e-02, 6.6742307e-01, 3.1707895e-01],\n",
       "       [3.9391182e-02, 7.2874802e-01, 2.3186088e-01],\n",
       "       [1.7762041e-02, 6.3334876e-01, 3.4888917e-01],\n",
       "       [4.7319178e-03, 5.0540286e-01, 4.8986527e-01],\n",
       "       [3.9894123e-02, 6.9507462e-01, 2.6503128e-01],\n",
       "       [6.6410191e-02, 7.4005330e-01, 1.9353645e-01],\n",
       "       [9.1185737e-01, 8.4750898e-02, 3.3917094e-03],\n",
       "       [1.1235812e-02, 6.3477767e-01, 3.5398647e-01],\n",
       "       [9.7126615e-01, 2.8034501e-02, 6.9932290e-04],\n",
       "       [9.0585530e-01, 8.8565297e-02, 5.5793384e-03],\n",
       "       [3.5446163e-03, 5.3174520e-01, 4.6471018e-01],\n",
       "       [8.6937686e-03, 6.0017651e-01, 3.9112967e-01],\n",
       "       [9.3519545e-01, 6.2299982e-02, 2.5046286e-03],\n",
       "       [8.3283670e-03, 5.1271772e-01, 4.7895390e-01],\n",
       "       [1.2227847e-01, 6.9502133e-01, 1.8270023e-01],\n",
       "       [9.4749832e-01, 5.0751697e-02, 1.7500424e-03],\n",
       "       [8.7067896e-01, 1.2224721e-01, 7.0738452e-03],\n",
       "       [9.3914759e-01, 5.8942556e-02, 1.9098491e-03],\n",
       "       [5.5670258e-02, 7.6329416e-01, 1.8103565e-01],\n",
       "       [9.2659795e-01, 7.0246689e-02, 3.1553835e-03],\n",
       "       [5.3564593e-02, 7.8334498e-01, 1.6309050e-01],\n",
       "       [3.6847696e-03, 4.7095400e-01, 5.2536118e-01],\n",
       "       [1.9821187e-02, 6.3366115e-01, 3.4651762e-01],\n",
       "       [5.0854281e-02, 7.5900346e-01, 1.9014233e-01],\n",
       "       [3.4516539e-02, 6.4044732e-01, 3.2503614e-01],\n",
       "       [1.1332152e-01, 7.0548218e-01, 1.8119632e-01],\n",
       "       [3.5890441e-02, 6.6631395e-01, 2.9779565e-01],\n",
       "       [9.3391627e-01, 6.3256964e-02, 2.8267393e-03],\n",
       "       [6.6936151e-03, 5.6397969e-01, 4.2932665e-01],\n",
       "       [3.8218694e-03, 4.6900502e-01, 5.2717310e-01],\n",
       "       [5.3704120e-02, 7.1490806e-01, 2.3138779e-01],\n",
       "       [9.2709649e-01, 6.9751441e-02, 3.1521155e-03],\n",
       "       [7.3170485e-03, 5.6929719e-01, 4.2338574e-01],\n",
       "       [9.4179535e-01, 5.5744346e-02, 2.4602944e-03],\n",
       "       [9.2159772e-01, 7.3975414e-02, 4.4269501e-03],\n",
       "       [9.2487794e-01, 7.1408257e-02, 3.7137787e-03],\n",
       "       [8.9608294e-01, 9.8169319e-02, 5.7477150e-03],\n",
       "       [1.9566849e-02, 7.1319973e-01, 2.6723349e-01],\n",
       "       [3.5904482e-02, 6.9538832e-01, 2.6870725e-01],\n",
       "       [9.5884520e-01, 3.9349690e-02, 1.8050701e-03],\n",
       "       [1.4297377e-02, 4.9291039e-01, 4.9279225e-01],\n",
       "       [1.0732642e-02, 6.0919893e-01, 3.8006842e-01],\n",
       "       [4.1045558e-02, 7.8129220e-01, 1.7766215e-01]], dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_x=np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 2, 1, 1, 1, 1, 1, 0, 1, 2, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0  1  2\n",
       "0   0  1  0\n",
       "1   0  1  0\n",
       "2   0  1  0\n",
       "3   0  1  0\n",
       "4   0  1  0\n",
       "5   0  1  0\n",
       "6   0  1  0\n",
       "7   0  1  0\n",
       "8   1  0  0\n",
       "9   0  1  0\n",
       "10  1  0  0\n",
       "11  1  0  0\n",
       "12  0  1  0\n",
       "13  0  1  0\n",
       "14  1  0  0\n",
       "15  0  1  0\n",
       "16  0  1  0\n",
       "17  1  0  0\n",
       "18  1  0  0\n",
       "19  1  0  0\n",
       "20  0  1  0\n",
       "21  1  0  0\n",
       "22  0  1  0\n",
       "23  0  0  1\n",
       "24  0  1  0\n",
       "25  0  1  0\n",
       "26  0  1  0\n",
       "27  0  1  0\n",
       "28  0  1  0\n",
       "29  1  0  0\n",
       "30  0  1  0\n",
       "31  0  0  1\n",
       "32  0  1  0\n",
       "33  1  0  0\n",
       "34  0  1  0\n",
       "35  1  0  0\n",
       "36  1  0  0\n",
       "37  1  0  0\n",
       "38  1  0  0\n",
       "39  0  1  0\n",
       "40  0  1  0\n",
       "41  1  0  0\n",
       "42  0  1  0\n",
       "43  0  1  0\n",
       "44  0  1  0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = pd.get_dummies(classes_x)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 3)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 3)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3619'>3620</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3620'>3621</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3621'>3622</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:142\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 66'\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000062?line=17'>18</a>\u001b[0m roc_auc \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000062?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_classes):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000062?line=19'>20</a>\u001b[0m     fpr[i], tpr[i], _ \u001b[39m=\u001b[39m roc_curve(y_test[:, i], classes[:, i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000062?line=20'>21</a>\u001b[0m     roc_auc[i] \u001b[39m=\u001b[39m auc(fpr[i], tpr[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000062?line=22'>23</a>\u001b[0m \u001b[39m# Compute micro-average ROC curve and ROC area\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3502'>3503</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3503'>3504</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3504'>3505</a>\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3505'>3506</a>\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/frame.py?line=3506'>3507</a>\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3628\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3622'>3623</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3623'>3624</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3624'>3625</a>\u001b[0m         \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3625'>3626</a>\u001b[0m         \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3626'>3627</a>\u001b[0m         \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3627'>3628</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_indexing_error(key)\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3628'>3629</a>\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=3630'>3631</a>\u001b[0m \u001b[39m# GH#42269\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:5637\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5632'>5633</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_indexing_error\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5633'>5634</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5634'>5635</a>\u001b[0m         \u001b[39m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5635'>5636</a>\u001b[0m         \u001b[39m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> <a href='file:///home/christopher/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py?line=5636'>5637</a>\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "n_classes = 3\n",
    "y_test = label_binarize(y_test, classes=np.arange(n_classes))\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], classes[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), classes.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "# First aggregate all false positive rates\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "# Then interpolate all ROC curves at this points\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "# Finally average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "# Plot all ROC curves\n",
    "#plt.figure(figsize=(10,5))\n",
    "plt.figure(dpi=600)\n",
    "lw = 2\n",
    "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "label=\"micro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"micro\"]),\n",
    "color=\"deeppink\", linestyle=\":\", linewidth=4,)\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "label=\"macro-average ROC curve (area = {0:0.2f})\".format(roc_auc[\"macro\"]),\n",
    "color=\"navy\", linestyle=\":\", linewidth=4,)\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"darkgreen\", \"yellow\", \"blue\"])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "        label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i, roc_auc[i]),)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) curve\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset\n",
    "#\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3\n",
       "0  5.1  3.5  1.4  0.2\n",
       "1  4.9  3.0  1.4  0.2\n",
       "2  4.7  3.2  1.3  0.2\n",
       "3  4.6  3.1  1.5  0.2\n",
       "4  5.0  3.6  1.4  0.2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Therefore, our final shape of output feature will be (150, 3)\n"
     ]
    }
   ],
   "source": [
    "lbl_clf = LabelEncoder()\n",
    "Y_encoded = lbl_clf.fit_transform(y)\n",
    "\n",
    "#Keras requires your output feature to be one-hot encoded values.\n",
    "Y_final = tf.keras.utils.to_categorical(Y_encoded)\n",
    "\n",
    "print(\"Therefore, our final shape of output feature will be {}\".format(Y_final.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Input shape\t: (112, 4)\n",
      "Testing Input shape\t: (38, 4)\n",
      "Training Output shape\t: (112, 3)\n",
      "Testing Output shape\t: (38, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y_final, test_size=0.25, random_state=seed, stratify=Y_encoded, shuffle=True)\n",
    "\n",
    "print(\"Training Input shape\\t: {}\".format(x_train.shape))\n",
    "print(\"Testing Input shape\\t: {}\".format(x_test.shape))\n",
    "print(\"Training Output shape\\t: {}\".format(y_train.shape))\n",
    "print(\"Testing Output shape\\t: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_clf = StandardScaler()\n",
    "x_train_new = std_clf.fit_transform(x_train)\n",
    "x_test_new = std_clf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-13 18:54:20.471758: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-05-13 18:54:20.471798: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-05-13 18:54:20.471827: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (christopher-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
      "2022-05-13 18:54:20.472957: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, input_dim=4, activation=tf.nn.relu, kernel_initializer='he_normal', \n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(7, activation=tf.nn.relu, kernel_initializer='he_normal', \n",
    "                                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(0.3))\n",
    "model.add(tf.keras.layers.Dense(5, activation=tf.nn.relu, kernel_initializer='he_normal', \n",
    "                                kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.001, l2=0.001)))\n",
    "model.add(tf.keras.layers.Dense(3, activation=tf.nn.softmax))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "16/16 [==============================] - 1s 3ms/step - loss: 1.8662 - accuracy: 0.2321\n",
      "Epoch 2/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.7453 - accuracy: 0.1964\n",
      "Epoch 3/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.6643 - accuracy: 0.3036\n",
      "Epoch 4/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.6569 - accuracy: 0.3214\n",
      "Epoch 5/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.6161 - accuracy: 0.3482\n",
      "Epoch 6/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3454 - accuracy: 0.4196\n",
      "Epoch 7/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3104 - accuracy: 0.4643\n",
      "Epoch 8/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4067 - accuracy: 0.4643\n",
      "Epoch 9/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2835 - accuracy: 0.5000\n",
      "Epoch 10/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.2686 - accuracy: 0.5089\n",
      "Epoch 11/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.4279 - accuracy: 0.4911\n",
      "Epoch 12/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.2373 - accuracy: 0.5357\n",
      "Epoch 13/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.3587 - accuracy: 0.5179\n",
      "Epoch 14/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.1412 - accuracy: 0.6607\n",
      "Epoch 15/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.1805 - accuracy: 0.6071\n",
      "Epoch 16/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.1809 - accuracy: 0.6071\n",
      "Epoch 17/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.1513 - accuracy: 0.6786\n",
      "Epoch 18/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0847 - accuracy: 0.7054\n",
      "Epoch 19/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0741 - accuracy: 0.7143\n",
      "Epoch 20/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0861 - accuracy: 0.7143\n",
      "Epoch 21/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0812 - accuracy: 0.6607\n",
      "Epoch 22/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.0520 - accuracy: 0.7143\n",
      "Epoch 23/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.1220 - accuracy: 0.7054\n",
      "Epoch 24/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.1032 - accuracy: 0.6607\n",
      "Epoch 25/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9631 - accuracy: 0.7054\n",
      "Epoch 26/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0157 - accuracy: 0.8036\n",
      "Epoch 27/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9537 - accuracy: 0.7500\n",
      "Epoch 28/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9585 - accuracy: 0.7232\n",
      "Epoch 29/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9583 - accuracy: 0.8304\n",
      "Epoch 30/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.0080 - accuracy: 0.7143\n",
      "Epoch 31/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9382 - accuracy: 0.7321\n",
      "Epoch 32/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9046 - accuracy: 0.7768\n",
      "Epoch 33/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8920 - accuracy: 0.7679\n",
      "Epoch 34/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9291 - accuracy: 0.6964\n",
      "Epoch 35/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9347 - accuracy: 0.7679\n",
      "Epoch 36/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9097 - accuracy: 0.7679\n",
      "Epoch 37/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8540 - accuracy: 0.7321\n",
      "Epoch 38/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9480 - accuracy: 0.7768\n",
      "Epoch 39/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8319 - accuracy: 0.8482\n",
      "Epoch 40/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9268 - accuracy: 0.6964\n",
      "Epoch 41/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8549 - accuracy: 0.7232\n",
      "Epoch 42/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9407 - accuracy: 0.7054\n",
      "Epoch 43/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9212 - accuracy: 0.7589\n",
      "Epoch 44/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8687 - accuracy: 0.6964\n",
      "Epoch 45/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9159 - accuracy: 0.7946\n",
      "Epoch 46/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8485 - accuracy: 0.7321\n",
      "Epoch 47/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7309 - accuracy: 0.8482\n",
      "Epoch 48/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7651 - accuracy: 0.8036\n",
      "Epoch 49/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7933 - accuracy: 0.7946\n",
      "Epoch 50/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8604 - accuracy: 0.7411\n",
      "Epoch 51/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7848 - accuracy: 0.8304\n",
      "Epoch 52/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.8193 - accuracy: 0.7857\n",
      "Epoch 53/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9199 - accuracy: 0.7411\n",
      "Epoch 54/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7462 - accuracy: 0.8393\n",
      "Epoch 55/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8856 - accuracy: 0.7321\n",
      "Epoch 56/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7652 - accuracy: 0.8036\n",
      "Epoch 57/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.9062 - accuracy: 0.7679\n",
      "Epoch 58/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8704 - accuracy: 0.7321\n",
      "Epoch 59/700\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.9530 - accuracy: 0.6429\n",
      "Epoch 60/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.8050 - accuracy: 0.7768\n",
      "Epoch 61/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7649 - accuracy: 0.8214\n",
      "Epoch 62/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.9153 - accuracy: 0.7321\n",
      "Epoch 63/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.7227 - accuracy: 0.7857\n",
      "Epoch 64/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.9369 - accuracy: 0.6518\n",
      "Epoch 65/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.8693 - accuracy: 0.7589\n",
      "Epoch 66/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7458 - accuracy: 0.8036\n",
      "Epoch 67/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7507 - accuracy: 0.7768\n",
      "Epoch 68/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6786 - accuracy: 0.8482\n",
      "Epoch 69/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7081 - accuracy: 0.8214\n",
      "Epoch 70/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7153 - accuracy: 0.7946\n",
      "Epoch 71/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7650 - accuracy: 0.7500\n",
      "Epoch 72/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7637 - accuracy: 0.7232\n",
      "Epoch 73/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7676 - accuracy: 0.7500\n",
      "Epoch 74/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.7394 - accuracy: 0.7857\n",
      "Epoch 75/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7831 - accuracy: 0.7321\n",
      "Epoch 76/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7059 - accuracy: 0.7946\n",
      "Epoch 77/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7430 - accuracy: 0.8482\n",
      "Epoch 78/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7561 - accuracy: 0.7768\n",
      "Epoch 79/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6099 - accuracy: 0.8482\n",
      "Epoch 80/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6588 - accuracy: 0.8214\n",
      "Epoch 81/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6864 - accuracy: 0.8393\n",
      "Epoch 82/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6432 - accuracy: 0.8125\n",
      "Epoch 83/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.8534 - accuracy: 0.7500\n",
      "Epoch 84/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6895 - accuracy: 0.8036\n",
      "Epoch 85/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6287 - accuracy: 0.8482\n",
      "Epoch 86/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6092 - accuracy: 0.8571\n",
      "Epoch 87/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7273 - accuracy: 0.7946\n",
      "Epoch 88/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6586 - accuracy: 0.8393\n",
      "Epoch 89/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6262 - accuracy: 0.8125\n",
      "Epoch 90/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6627 - accuracy: 0.8393\n",
      "Epoch 91/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6026 - accuracy: 0.8393\n",
      "Epoch 92/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6550 - accuracy: 0.8214\n",
      "Epoch 93/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7131 - accuracy: 0.7768\n",
      "Epoch 94/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6814 - accuracy: 0.7857\n",
      "Epoch 95/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7556 - accuracy: 0.7500\n",
      "Epoch 96/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6605 - accuracy: 0.7857\n",
      "Epoch 97/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6653 - accuracy: 0.8571\n",
      "Epoch 98/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6319 - accuracy: 0.8482\n",
      "Epoch 99/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6517 - accuracy: 0.7946\n",
      "Epoch 100/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.8019 - accuracy: 0.7589\n",
      "Epoch 101/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6121 - accuracy: 0.8750\n",
      "Epoch 102/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7159 - accuracy: 0.7857\n",
      "Epoch 103/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6314 - accuracy: 0.8304\n",
      "Epoch 104/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7176 - accuracy: 0.7589\n",
      "Epoch 105/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6348 - accuracy: 0.8839\n",
      "Epoch 106/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5915 - accuracy: 0.8393\n",
      "Epoch 107/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6054 - accuracy: 0.8393\n",
      "Epoch 108/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5910 - accuracy: 0.8571\n",
      "Epoch 109/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6382 - accuracy: 0.8036\n",
      "Epoch 110/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5510 - accuracy: 0.8750\n",
      "Epoch 111/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.7025 - accuracy: 0.8125\n",
      "Epoch 112/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.6095 - accuracy: 0.8482\n",
      "Epoch 113/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5545 - accuracy: 0.8125\n",
      "Epoch 114/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6451 - accuracy: 0.8214\n",
      "Epoch 115/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.8661\n",
      "Epoch 116/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5152 - accuracy: 0.8571\n",
      "Epoch 117/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5775 - accuracy: 0.8214\n",
      "Epoch 118/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5967 - accuracy: 0.7946\n",
      "Epoch 119/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.5839 - accuracy: 0.8036\n",
      "Epoch 120/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5225 - accuracy: 0.8929\n",
      "Epoch 121/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5818 - accuracy: 0.8214\n",
      "Epoch 122/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5908 - accuracy: 0.8304\n",
      "Epoch 123/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4785 - accuracy: 0.8839\n",
      "Epoch 124/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6432 - accuracy: 0.8482\n",
      "Epoch 125/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6465 - accuracy: 0.8304\n",
      "Epoch 126/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5441 - accuracy: 0.8661\n",
      "Epoch 127/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5904 - accuracy: 0.8304\n",
      "Epoch 128/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5247 - accuracy: 0.8929\n",
      "Epoch 129/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5783 - accuracy: 0.7946\n",
      "Epoch 130/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5491 - accuracy: 0.8571\n",
      "Epoch 131/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5580 - accuracy: 0.8571\n",
      "Epoch 132/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5394 - accuracy: 0.8661\n",
      "Epoch 133/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5386 - accuracy: 0.8571\n",
      "Epoch 134/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5590 - accuracy: 0.8750\n",
      "Epoch 135/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6118 - accuracy: 0.8036\n",
      "Epoch 136/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4926 - accuracy: 0.9018\n",
      "Epoch 137/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6193 - accuracy: 0.8571\n",
      "Epoch 138/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6234 - accuracy: 0.8214\n",
      "Epoch 139/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5377 - accuracy: 0.8571\n",
      "Epoch 140/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5592 - accuracy: 0.8839\n",
      "Epoch 141/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5962 - accuracy: 0.8393\n",
      "Epoch 142/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5991 - accuracy: 0.9107\n",
      "Epoch 143/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5580 - accuracy: 0.8750\n",
      "Epoch 144/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4998 - accuracy: 0.8750\n",
      "Epoch 145/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4458 - accuracy: 0.9018\n",
      "Epoch 146/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4836 - accuracy: 0.8929\n",
      "Epoch 147/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5817 - accuracy: 0.8214\n",
      "Epoch 148/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5751 - accuracy: 0.8482\n",
      "Epoch 149/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5173 - accuracy: 0.8661\n",
      "Epoch 150/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5269 - accuracy: 0.8571\n",
      "Epoch 151/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.9018\n",
      "Epoch 152/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6744 - accuracy: 0.8125\n",
      "Epoch 153/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5166 - accuracy: 0.8839\n",
      "Epoch 154/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5334 - accuracy: 0.8839\n",
      "Epoch 155/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6367 - accuracy: 0.8036\n",
      "Epoch 156/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4479 - accuracy: 0.9107\n",
      "Epoch 157/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5530 - accuracy: 0.8304\n",
      "Epoch 158/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5605 - accuracy: 0.8393\n",
      "Epoch 159/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4487 - accuracy: 0.8661\n",
      "Epoch 160/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5347 - accuracy: 0.8661\n",
      "Epoch 161/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 0.8036\n",
      "Epoch 162/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4852 - accuracy: 0.8482\n",
      "Epoch 163/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4475 - accuracy: 0.9018\n",
      "Epoch 164/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6227 - accuracy: 0.8125\n",
      "Epoch 165/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.5028 - accuracy: 0.8839\n",
      "Epoch 166/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5454 - accuracy: 0.8571\n",
      "Epoch 167/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5400 - accuracy: 0.8750\n",
      "Epoch 168/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4039 - accuracy: 0.9107\n",
      "Epoch 169/700\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5099 - accuracy: 0.8661\n",
      "Epoch 170/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.6180 - accuracy: 0.8661\n",
      "Epoch 171/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4585 - accuracy: 0.9018\n",
      "Epoch 172/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4809 - accuracy: 0.8839\n",
      "Epoch 173/700\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4978 - accuracy: 0.8393\n",
      "Epoch 174/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5743 - accuracy: 0.8214\n",
      "Epoch 175/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5367 - accuracy: 0.8304\n",
      "Epoch 176/700\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5132 - accuracy: 0.8125\n",
      "Epoch 177/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5239 - accuracy: 0.8571\n",
      "Epoch 178/700\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4306 - accuracy: 0.8929\n",
      "Epoch 179/700\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.5284 - accuracy: 0.8661\n",
      "Epoch 180/700\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5383 - accuracy: 0.8214\n",
      "Epoch 181/700\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.6522 - accuracy: 0.7946\n",
      "Epoch 182/700\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5292 - accuracy: 0.8214\n",
      "Epoch 183/700\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 0.4419 - accuracy: 0.9018\n",
      "Epoch 184/700\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.4460 - accuracy: 0.9018\n",
      "Epoch 185/700\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4513 - accuracy: 0.9018\n",
      "Epoch 186/700\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4245 - accuracy: 0.9018\n",
      "Epoch 187/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3792 - accuracy: 0.9107\n",
      "Epoch 188/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5473 - accuracy: 0.8482\n",
      "Epoch 189/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4031 - accuracy: 0.9286\n",
      "Epoch 190/700\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6584 - accuracy: 0.8214\n",
      "Epoch 191/700\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4456 - accuracy: 0.8393\n",
      "Epoch 192/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5176 - accuracy: 0.8661\n",
      "Epoch 193/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5306 - accuracy: 0.8393\n",
      "Epoch 194/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.6340 - accuracy: 0.8393\n",
      "Epoch 195/700\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 0.4458 - accuracy: 0.8750\n",
      "Epoch 196/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3796 - accuracy: 0.9286\n",
      "Epoch 197/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4994 - accuracy: 0.8571\n",
      "Epoch 198/700\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4566 - accuracy: 0.8661\n",
      "Epoch 199/700\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.3688 - accuracy: 0.9464\n",
      "Epoch 200/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4798 - accuracy: 0.8661\n",
      "Epoch 201/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5422 - accuracy: 0.8661\n",
      "Epoch 202/700\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.5356 - accuracy: 0.8304\n",
      "Epoch 203/700\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.4223 - accuracy: 0.8839\n",
      "Epoch 204/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4390 - accuracy: 0.8661\n",
      "Epoch 205/700\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.5099 - accuracy: 0.8482\n",
      "Epoch 206/700\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4828 - accuracy: 0.8929\n",
      "Epoch 207/700\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 0.6213 - accuracy: 0.8661\n",
      "Epoch 208/700\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3805 - accuracy: 0.8929\n",
      "Epoch 209/700\n",
      "16/16 [==============================] - 0s 8ms/step - loss: 0.4675 - accuracy: 0.8929\n",
      "Epoch 210/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4246 - accuracy: 0.8839\n",
      "Epoch 211/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5054 - accuracy: 0.8214\n",
      "Epoch 212/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4326 - accuracy: 0.9107\n",
      "Epoch 213/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4742 - accuracy: 0.8750\n",
      "Epoch 214/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4247 - accuracy: 0.8929\n",
      "Epoch 215/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5180 - accuracy: 0.8214\n",
      "Epoch 216/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4358 - accuracy: 0.8482\n",
      "Epoch 217/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4879 - accuracy: 0.8125\n",
      "Epoch 218/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4021 - accuracy: 0.9107\n",
      "Epoch 219/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4710 - accuracy: 0.8929\n",
      "Epoch 220/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4166 - accuracy: 0.8839\n",
      "Epoch 221/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3860 - accuracy: 0.9286\n",
      "Epoch 222/700\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 0.4338 - accuracy: 0.8750\n",
      "Epoch 223/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4189 - accuracy: 0.8929\n",
      "Epoch 224/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3463 - accuracy: 0.9375\n",
      "Epoch 225/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4474 - accuracy: 0.8929\n",
      "Epoch 226/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4118 - accuracy: 0.8750\n",
      "Epoch 227/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3919 - accuracy: 0.9018\n",
      "Epoch 228/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4087 - accuracy: 0.8929\n",
      "Epoch 229/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4109 - accuracy: 0.9286\n",
      "Epoch 230/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4378 - accuracy: 0.8750\n",
      "Epoch 231/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4956 - accuracy: 0.8750\n",
      "Epoch 232/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.7923 - accuracy: 0.6696\n",
      "Epoch 233/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4218 - accuracy: 0.8929\n",
      "Epoch 234/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4716 - accuracy: 0.8571\n",
      "Epoch 235/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3595 - accuracy: 0.9286\n",
      "Epoch 236/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3831 - accuracy: 0.8839\n",
      "Epoch 237/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4198 - accuracy: 0.8929\n",
      "Epoch 238/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3733 - accuracy: 0.8839\n",
      "Epoch 239/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4199 - accuracy: 0.9018\n",
      "Epoch 240/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5186 - accuracy: 0.8214\n",
      "Epoch 241/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4834 - accuracy: 0.8750\n",
      "Epoch 242/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3935 - accuracy: 0.9018\n",
      "Epoch 243/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3405 - accuracy: 0.9286\n",
      "Epoch 244/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4240 - accuracy: 0.8929\n",
      "Epoch 245/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3974 - accuracy: 0.8929\n",
      "Epoch 246/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4255 - accuracy: 0.9018\n",
      "Epoch 247/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3992 - accuracy: 0.8571\n",
      "Epoch 248/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4340 - accuracy: 0.8750\n",
      "Epoch 249/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3718 - accuracy: 0.9018\n",
      "Epoch 250/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5226 - accuracy: 0.8304\n",
      "Epoch 251/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5152 - accuracy: 0.8661\n",
      "Epoch 252/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.9018\n",
      "Epoch 253/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3550 - accuracy: 0.9286\n",
      "Epoch 254/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5036 - accuracy: 0.8750\n",
      "Epoch 255/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4285 - accuracy: 0.9018\n",
      "Epoch 256/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4044 - accuracy: 0.8929\n",
      "Epoch 257/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3798 - accuracy: 0.9018\n",
      "Epoch 258/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3283 - accuracy: 0.9196\n",
      "Epoch 259/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3975 - accuracy: 0.9107\n",
      "Epoch 260/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4430 - accuracy: 0.8482\n",
      "Epoch 261/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3686 - accuracy: 0.9286\n",
      "Epoch 262/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3179 - accuracy: 0.9375\n",
      "Epoch 263/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5086 - accuracy: 0.8482\n",
      "Epoch 264/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4072 - accuracy: 0.8661\n",
      "Epoch 265/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.9018\n",
      "Epoch 266/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4259 - accuracy: 0.8750\n",
      "Epoch 267/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4159 - accuracy: 0.8482\n",
      "Epoch 268/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.8929\n",
      "Epoch 269/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3712 - accuracy: 0.8839\n",
      "Epoch 270/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3548 - accuracy: 0.9018\n",
      "Epoch 271/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4785 - accuracy: 0.8482\n",
      "Epoch 272/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4125 - accuracy: 0.9286\n",
      "Epoch 273/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3446 - accuracy: 0.9018\n",
      "Epoch 274/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.9196\n",
      "Epoch 275/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3015 - accuracy: 0.9464\n",
      "Epoch 276/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4201 - accuracy: 0.8661\n",
      "Epoch 277/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4218 - accuracy: 0.8750\n",
      "Epoch 278/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4137 - accuracy: 0.8750\n",
      "Epoch 279/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3822 - accuracy: 0.8750\n",
      "Epoch 280/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3466 - accuracy: 0.9018\n",
      "Epoch 281/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5214 - accuracy: 0.8304\n",
      "Epoch 282/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3554 - accuracy: 0.9018\n",
      "Epoch 283/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3744 - accuracy: 0.8929\n",
      "Epoch 284/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4808 - accuracy: 0.8750\n",
      "Epoch 285/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2925 - accuracy: 0.9286\n",
      "Epoch 286/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3680 - accuracy: 0.8839\n",
      "Epoch 287/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3418 - accuracy: 0.8929\n",
      "Epoch 288/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4120 - accuracy: 0.9107\n",
      "Epoch 289/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.6590 - accuracy: 0.8571\n",
      "Epoch 290/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4055 - accuracy: 0.8571\n",
      "Epoch 291/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2732 - accuracy: 0.9643\n",
      "Epoch 292/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3375 - accuracy: 0.9107\n",
      "Epoch 293/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.8839\n",
      "Epoch 294/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3591 - accuracy: 0.9018\n",
      "Epoch 295/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3416 - accuracy: 0.9196\n",
      "Epoch 296/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3658 - accuracy: 0.8750\n",
      "Epoch 297/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3475 - accuracy: 0.8750\n",
      "Epoch 298/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4198 - accuracy: 0.8750\n",
      "Epoch 299/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3624 - accuracy: 0.8839\n",
      "Epoch 300/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2758 - accuracy: 0.9375\n",
      "Epoch 301/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4924 - accuracy: 0.8839\n",
      "Epoch 302/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4794 - accuracy: 0.8304\n",
      "Epoch 303/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4066 - accuracy: 0.8839\n",
      "Epoch 304/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3234 - accuracy: 0.9107\n",
      "Epoch 305/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3940 - accuracy: 0.8750\n",
      "Epoch 306/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.9286\n",
      "Epoch 307/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4980 - accuracy: 0.8304\n",
      "Epoch 308/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3446 - accuracy: 0.9107\n",
      "Epoch 309/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2951 - accuracy: 0.9286\n",
      "Epoch 310/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3066 - accuracy: 0.9196\n",
      "Epoch 311/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4081 - accuracy: 0.8839\n",
      "Epoch 312/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3406 - accuracy: 0.8839\n",
      "Epoch 313/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.8661\n",
      "Epoch 314/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3551 - accuracy: 0.8929\n",
      "Epoch 315/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3440 - accuracy: 0.8929\n",
      "Epoch 316/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3622 - accuracy: 0.8661\n",
      "Epoch 317/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3412 - accuracy: 0.8661\n",
      "Epoch 318/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2967 - accuracy: 0.9018\n",
      "Epoch 319/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3274 - accuracy: 0.8839\n",
      "Epoch 320/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.4110 - accuracy: 0.8661\n",
      "Epoch 321/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3311 - accuracy: 0.9196\n",
      "Epoch 322/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2858 - accuracy: 0.9375\n",
      "Epoch 323/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4080 - accuracy: 0.8482\n",
      "Epoch 324/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3733 - accuracy: 0.8839\n",
      "Epoch 325/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3052 - accuracy: 0.9018\n",
      "Epoch 326/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4321 - accuracy: 0.8571\n",
      "Epoch 327/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.9107\n",
      "Epoch 328/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2930 - accuracy: 0.9286\n",
      "Epoch 329/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2755 - accuracy: 0.9375\n",
      "Epoch 330/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3033 - accuracy: 0.9196\n",
      "Epoch 331/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2791 - accuracy: 0.9018\n",
      "Epoch 332/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3324 - accuracy: 0.9018\n",
      "Epoch 333/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4592 - accuracy: 0.8214\n",
      "Epoch 334/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3589 - accuracy: 0.9018\n",
      "Epoch 335/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5010 - accuracy: 0.8482\n",
      "Epoch 336/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2927 - accuracy: 0.9286\n",
      "Epoch 337/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2770 - accuracy: 0.9196\n",
      "Epoch 338/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2826 - accuracy: 0.9107\n",
      "Epoch 339/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3487 - accuracy: 0.9107\n",
      "Epoch 340/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4104 - accuracy: 0.9196\n",
      "Epoch 341/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3422 - accuracy: 0.8839\n",
      "Epoch 342/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3389 - accuracy: 0.8571\n",
      "Epoch 343/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4040 - accuracy: 0.8482\n",
      "Epoch 344/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.5460 - accuracy: 0.8393\n",
      "Epoch 345/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3070 - accuracy: 0.9018\n",
      "Epoch 346/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2696 - accuracy: 0.9554\n",
      "Epoch 347/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2895 - accuracy: 0.9286\n",
      "Epoch 348/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4035 - accuracy: 0.8482\n",
      "Epoch 349/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3615 - accuracy: 0.9018\n",
      "Epoch 350/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3545 - accuracy: 0.9018\n",
      "Epoch 351/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.9107\n",
      "Epoch 352/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4654 - accuracy: 0.8661\n",
      "Epoch 353/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3500 - accuracy: 0.8750\n",
      "Epoch 354/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2903 - accuracy: 0.9107\n",
      "Epoch 355/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4358 - accuracy: 0.8839\n",
      "Epoch 356/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.9554\n",
      "Epoch 357/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8839\n",
      "Epoch 358/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2696 - accuracy: 0.9286\n",
      "Epoch 359/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3777 - accuracy: 0.8839\n",
      "Epoch 360/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4080 - accuracy: 0.8571\n",
      "Epoch 361/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.9107\n",
      "Epoch 362/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3728 - accuracy: 0.8661\n",
      "Epoch 363/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3433 - accuracy: 0.9018\n",
      "Epoch 364/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3870 - accuracy: 0.8661\n",
      "Epoch 365/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2680 - accuracy: 0.9375\n",
      "Epoch 366/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3377 - accuracy: 0.9107\n",
      "Epoch 367/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4420 - accuracy: 0.8393\n",
      "Epoch 368/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2514 - accuracy: 0.9464\n",
      "Epoch 369/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4143 - accuracy: 0.8571\n",
      "Epoch 370/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2591 - accuracy: 0.9464\n",
      "Epoch 371/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.5472 - accuracy: 0.8304\n",
      "Epoch 372/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2940 - accuracy: 0.9196\n",
      "Epoch 373/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2652 - accuracy: 0.9464\n",
      "Epoch 374/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3244 - accuracy: 0.8929\n",
      "Epoch 375/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3659 - accuracy: 0.8839\n",
      "Epoch 376/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3092 - accuracy: 0.8839\n",
      "Epoch 377/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3427 - accuracy: 0.8661\n",
      "Epoch 378/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3425 - accuracy: 0.8661\n",
      "Epoch 379/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3003 - accuracy: 0.9107\n",
      "Epoch 380/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.8571\n",
      "Epoch 381/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3108 - accuracy: 0.9107\n",
      "Epoch 382/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4014 - accuracy: 0.9018\n",
      "Epoch 383/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8750\n",
      "Epoch 384/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3806 - accuracy: 0.9286\n",
      "Epoch 385/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3147 - accuracy: 0.9196\n",
      "Epoch 386/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3384 - accuracy: 0.9018\n",
      "Epoch 387/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2971 - accuracy: 0.9107\n",
      "Epoch 388/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3904 - accuracy: 0.8750\n",
      "Epoch 389/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2621 - accuracy: 0.9464\n",
      "Epoch 390/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3063 - accuracy: 0.9018\n",
      "Epoch 391/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.9196\n",
      "Epoch 392/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2840 - accuracy: 0.9286\n",
      "Epoch 393/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3212 - accuracy: 0.8929\n",
      "Epoch 394/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2164 - accuracy: 0.9554\n",
      "Epoch 395/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2755 - accuracy: 0.9286\n",
      "Epoch 396/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3369 - accuracy: 0.9196\n",
      "Epoch 397/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2417 - accuracy: 0.9464\n",
      "Epoch 398/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2443 - accuracy: 0.9286\n",
      "Epoch 399/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2238 - accuracy: 0.9554\n",
      "Epoch 400/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3598 - accuracy: 0.8839\n",
      "Epoch 401/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2282 - accuracy: 0.9375\n",
      "Epoch 402/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3097 - accuracy: 0.9196\n",
      "Epoch 403/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3070 - accuracy: 0.9107\n",
      "Epoch 404/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4390 - accuracy: 0.8839\n",
      "Epoch 405/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3737 - accuracy: 0.8929\n",
      "Epoch 406/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2445 - accuracy: 0.9375\n",
      "Epoch 407/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2820 - accuracy: 0.8929\n",
      "Epoch 408/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2262 - accuracy: 0.9107\n",
      "Epoch 409/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3992 - accuracy: 0.8571\n",
      "Epoch 410/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3382 - accuracy: 0.9286\n",
      "Epoch 411/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3413 - accuracy: 0.9107\n",
      "Epoch 412/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2625 - accuracy: 0.9018\n",
      "Epoch 413/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2844 - accuracy: 0.8839\n",
      "Epoch 414/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2866 - accuracy: 0.9018\n",
      "Epoch 415/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2440 - accuracy: 0.9286\n",
      "Epoch 416/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1900 - accuracy: 0.9554\n",
      "Epoch 417/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2839 - accuracy: 0.9375\n",
      "Epoch 418/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3157 - accuracy: 0.9018\n",
      "Epoch 419/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3242 - accuracy: 0.8929\n",
      "Epoch 420/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2778 - accuracy: 0.9018\n",
      "Epoch 421/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2840 - accuracy: 0.9196\n",
      "Epoch 422/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.8393\n",
      "Epoch 423/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3515 - accuracy: 0.8929\n",
      "Epoch 424/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2569 - accuracy: 0.9375\n",
      "Epoch 425/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2981 - accuracy: 0.9286\n",
      "Epoch 426/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2282 - accuracy: 0.9196\n",
      "Epoch 427/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2735 - accuracy: 0.9107\n",
      "Epoch 428/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3390 - accuracy: 0.8571\n",
      "Epoch 429/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.9196\n",
      "Epoch 430/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4519 - accuracy: 0.8929\n",
      "Epoch 431/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2367 - accuracy: 0.9554\n",
      "Epoch 432/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2123 - accuracy: 0.9643\n",
      "Epoch 433/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.9286\n",
      "Epoch 434/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2448 - accuracy: 0.9107\n",
      "Epoch 435/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3274 - accuracy: 0.9196\n",
      "Epoch 436/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2989 - accuracy: 0.8750\n",
      "Epoch 437/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3838 - accuracy: 0.8929\n",
      "Epoch 438/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3673 - accuracy: 0.8750\n",
      "Epoch 439/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3364 - accuracy: 0.9018\n",
      "Epoch 440/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.2435 - accuracy: 0.9375\n",
      "Epoch 441/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3293 - accuracy: 0.8839\n",
      "Epoch 442/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3278 - accuracy: 0.8839\n",
      "Epoch 443/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2664 - accuracy: 0.9196\n",
      "Epoch 444/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3485 - accuracy: 0.8929\n",
      "Epoch 445/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3875 - accuracy: 0.8750\n",
      "Epoch 446/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2025 - accuracy: 0.9643\n",
      "Epoch 447/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3526 - accuracy: 0.8839\n",
      "Epoch 448/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2350 - accuracy: 0.9286\n",
      "Epoch 449/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2611 - accuracy: 0.9375\n",
      "Epoch 450/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3120 - accuracy: 0.8929\n",
      "Epoch 451/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3233 - accuracy: 0.8839\n",
      "Epoch 452/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.4240 - accuracy: 0.8571\n",
      "Epoch 453/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3665 - accuracy: 0.9018\n",
      "Epoch 454/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2477 - accuracy: 0.9375\n",
      "Epoch 455/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2741 - accuracy: 0.9375\n",
      "Epoch 456/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2998 - accuracy: 0.9107\n",
      "Epoch 457/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2260 - accuracy: 0.9464\n",
      "Epoch 458/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2895 - accuracy: 0.9286\n",
      "Epoch 459/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2698 - accuracy: 0.9464\n",
      "Epoch 460/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3226 - accuracy: 0.8929\n",
      "Epoch 461/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2629 - accuracy: 0.9375\n",
      "Epoch 462/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2799 - accuracy: 0.9286\n",
      "Epoch 463/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2351 - accuracy: 0.9375\n",
      "Epoch 464/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2616 - accuracy: 0.9196\n",
      "Epoch 465/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3572 - accuracy: 0.8750\n",
      "Epoch 466/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3454 - accuracy: 0.9018\n",
      "Epoch 467/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2365 - accuracy: 0.9464\n",
      "Epoch 468/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2120 - accuracy: 0.9375\n",
      "Epoch 469/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3153 - accuracy: 0.8929\n",
      "Epoch 470/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.9286\n",
      "Epoch 471/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1856 - accuracy: 0.9554\n",
      "Epoch 472/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1823 - accuracy: 0.9554\n",
      "Epoch 473/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2652 - accuracy: 0.9286\n",
      "Epoch 474/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2731 - accuracy: 0.9107\n",
      "Epoch 475/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3007 - accuracy: 0.9107\n",
      "Epoch 476/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4756 - accuracy: 0.8750\n",
      "Epoch 477/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1695 - accuracy: 0.9643\n",
      "Epoch 478/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2374 - accuracy: 0.9286\n",
      "Epoch 479/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2313 - accuracy: 0.9286\n",
      "Epoch 480/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3469 - accuracy: 0.9107\n",
      "Epoch 481/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2890 - accuracy: 0.9107\n",
      "Epoch 482/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.9107\n",
      "Epoch 483/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3003 - accuracy: 0.9196\n",
      "Epoch 484/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3563 - accuracy: 0.8571\n",
      "Epoch 485/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2381 - accuracy: 0.9375\n",
      "Epoch 486/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 0.9107\n",
      "Epoch 487/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1988 - accuracy: 0.9732\n",
      "Epoch 488/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2588 - accuracy: 0.9375\n",
      "Epoch 489/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2047 - accuracy: 0.9375\n",
      "Epoch 490/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2403 - accuracy: 0.9464\n",
      "Epoch 491/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3108 - accuracy: 0.9018\n",
      "Epoch 492/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3084 - accuracy: 0.9107\n",
      "Epoch 493/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3557 - accuracy: 0.8482\n",
      "Epoch 494/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1693 - accuracy: 0.9732\n",
      "Epoch 495/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2149 - accuracy: 0.9554\n",
      "Epoch 496/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2030 - accuracy: 0.9464\n",
      "Epoch 497/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2451 - accuracy: 0.9464\n",
      "Epoch 498/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9554\n",
      "Epoch 499/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2071 - accuracy: 0.9554\n",
      "Epoch 500/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2940 - accuracy: 0.8839\n",
      "Epoch 501/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2222 - accuracy: 0.9464\n",
      "Epoch 502/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2443 - accuracy: 0.9107\n",
      "Epoch 503/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2084 - accuracy: 0.9375\n",
      "Epoch 504/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1977 - accuracy: 0.9554\n",
      "Epoch 505/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1896 - accuracy: 0.9554\n",
      "Epoch 506/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2281 - accuracy: 0.9196\n",
      "Epoch 507/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2911 - accuracy: 0.9375\n",
      "Epoch 508/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2122 - accuracy: 0.9286\n",
      "Epoch 509/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2489 - accuracy: 0.9464\n",
      "Epoch 510/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3063 - accuracy: 0.9196\n",
      "Epoch 511/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3057 - accuracy: 0.8929\n",
      "Epoch 512/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2519 - accuracy: 0.9375\n",
      "Epoch 513/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2401 - accuracy: 0.9464\n",
      "Epoch 514/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1950 - accuracy: 0.9464\n",
      "Epoch 515/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3059 - accuracy: 0.9018\n",
      "Epoch 516/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2706 - accuracy: 0.9375\n",
      "Epoch 517/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3411 - accuracy: 0.9286\n",
      "Epoch 518/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3550 - accuracy: 0.9107\n",
      "Epoch 519/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2490 - accuracy: 0.9286\n",
      "Epoch 520/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3891 - accuracy: 0.8750\n",
      "Epoch 521/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4270 - accuracy: 0.8929\n",
      "Epoch 522/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2036 - accuracy: 0.9554\n",
      "Epoch 523/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2775 - accuracy: 0.8929\n",
      "Epoch 524/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2390 - accuracy: 0.9107\n",
      "Epoch 525/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.3573 - accuracy: 0.8661\n",
      "Epoch 526/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2438 - accuracy: 0.9375\n",
      "Epoch 527/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1789 - accuracy: 0.9643\n",
      "Epoch 528/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2022 - accuracy: 0.9196\n",
      "Epoch 529/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2890 - accuracy: 0.9196\n",
      "Epoch 530/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2426 - accuracy: 0.9018\n",
      "Epoch 531/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2320 - accuracy: 0.9375\n",
      "Epoch 532/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3325 - accuracy: 0.9018\n",
      "Epoch 533/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2817 - accuracy: 0.9107\n",
      "Epoch 534/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2628 - accuracy: 0.9107\n",
      "Epoch 535/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2027 - accuracy: 0.9375\n",
      "Epoch 536/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2644 - accuracy: 0.9107\n",
      "Epoch 537/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3105 - accuracy: 0.9107\n",
      "Epoch 538/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2433 - accuracy: 0.9196\n",
      "Epoch 539/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1877 - accuracy: 0.9554\n",
      "Epoch 540/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9464\n",
      "Epoch 541/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2119 - accuracy: 0.9375\n",
      "Epoch 542/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2787 - accuracy: 0.9375\n",
      "Epoch 543/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2821 - accuracy: 0.9107\n",
      "Epoch 544/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3721 - accuracy: 0.9196\n",
      "Epoch 545/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2532 - accuracy: 0.9018\n",
      "Epoch 546/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2015 - accuracy: 0.9464\n",
      "Epoch 547/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.9286\n",
      "Epoch 548/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.9464\n",
      "Epoch 549/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3868 - accuracy: 0.9018\n",
      "Epoch 550/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1952 - accuracy: 0.9554\n",
      "Epoch 551/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1931 - accuracy: 0.9464\n",
      "Epoch 552/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3190 - accuracy: 0.8571\n",
      "Epoch 553/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2835 - accuracy: 0.8839\n",
      "Epoch 554/700\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 0.2866 - accuracy: 0.9464\n",
      "Epoch 555/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3110 - accuracy: 0.9107\n",
      "Epoch 556/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2114 - accuracy: 0.9554\n",
      "Epoch 557/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2222 - accuracy: 0.9554\n",
      "Epoch 558/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2172 - accuracy: 0.9464\n",
      "Epoch 559/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2245 - accuracy: 0.9107\n",
      "Epoch 560/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3083 - accuracy: 0.9107\n",
      "Epoch 561/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2606 - accuracy: 0.9375\n",
      "Epoch 562/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2567 - accuracy: 0.9375\n",
      "Epoch 563/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1735 - accuracy: 0.9643\n",
      "Epoch 564/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2203 - accuracy: 0.9375\n",
      "Epoch 565/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1759 - accuracy: 0.9643\n",
      "Epoch 566/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2045 - accuracy: 0.9375\n",
      "Epoch 567/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1744 - accuracy: 0.9643\n",
      "Epoch 568/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.4019 - accuracy: 0.9107\n",
      "Epoch 569/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3171 - accuracy: 0.8929\n",
      "Epoch 570/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2126 - accuracy: 0.9196\n",
      "Epoch 571/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2472 - accuracy: 0.9375\n",
      "Epoch 572/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9286\n",
      "Epoch 573/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3069 - accuracy: 0.9196\n",
      "Epoch 574/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2681 - accuracy: 0.9375\n",
      "Epoch 575/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2721 - accuracy: 0.8750\n",
      "Epoch 576/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3008 - accuracy: 0.9464\n",
      "Epoch 577/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2372 - accuracy: 0.9107\n",
      "Epoch 578/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3183 - accuracy: 0.8929\n",
      "Epoch 579/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2249 - accuracy: 0.9196\n",
      "Epoch 580/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2763 - accuracy: 0.9107\n",
      "Epoch 581/700\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.1957 - accuracy: 0.9375\n",
      "Epoch 582/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2573 - accuracy: 0.9196\n",
      "Epoch 583/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2476 - accuracy: 0.9196\n",
      "Epoch 584/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2322 - accuracy: 0.9554\n",
      "Epoch 585/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3598 - accuracy: 0.9018\n",
      "Epoch 586/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2396 - accuracy: 0.9286\n",
      "Epoch 587/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2485 - accuracy: 0.9286\n",
      "Epoch 588/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2089 - accuracy: 0.9464\n",
      "Epoch 589/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2842 - accuracy: 0.9286\n",
      "Epoch 590/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2759 - accuracy: 0.9018\n",
      "Epoch 591/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9375\n",
      "Epoch 592/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2077 - accuracy: 0.9375\n",
      "Epoch 593/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3080 - accuracy: 0.9464\n",
      "Epoch 594/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1509 - accuracy: 0.9732\n",
      "Epoch 595/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1995 - accuracy: 0.9286\n",
      "Epoch 596/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2097 - accuracy: 0.9286\n",
      "Epoch 597/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2144 - accuracy: 0.9286\n",
      "Epoch 598/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2486 - accuracy: 0.9196\n",
      "Epoch 599/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2929 - accuracy: 0.9196\n",
      "Epoch 600/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2135 - accuracy: 0.9554\n",
      "Epoch 601/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2715 - accuracy: 0.9107\n",
      "Epoch 602/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2856 - accuracy: 0.8750\n",
      "Epoch 603/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2244 - accuracy: 0.9464\n",
      "Epoch 604/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2211 - accuracy: 0.9196\n",
      "Epoch 605/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2116 - accuracy: 0.9554\n",
      "Epoch 606/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1739 - accuracy: 0.9554\n",
      "Epoch 607/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2885 - accuracy: 0.9107\n",
      "Epoch 608/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2039 - accuracy: 0.9554\n",
      "Epoch 609/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3431 - accuracy: 0.8661\n",
      "Epoch 610/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2207 - accuracy: 0.9375\n",
      "Epoch 611/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2423 - accuracy: 0.9107\n",
      "Epoch 612/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2297 - accuracy: 0.9375\n",
      "Epoch 613/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2658 - accuracy: 0.9107\n",
      "Epoch 614/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1500 - accuracy: 0.9732\n",
      "Epoch 615/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2524 - accuracy: 0.9107\n",
      "Epoch 616/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.9554\n",
      "Epoch 617/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3242 - accuracy: 0.9196\n",
      "Epoch 618/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2908 - accuracy: 0.9196\n",
      "Epoch 619/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1767 - accuracy: 0.9286\n",
      "Epoch 620/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2344 - accuracy: 0.9018\n",
      "Epoch 621/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1833 - accuracy: 0.9375\n",
      "Epoch 622/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2089 - accuracy: 0.9554\n",
      "Epoch 623/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2976 - accuracy: 0.9107\n",
      "Epoch 624/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1844 - accuracy: 0.9643\n",
      "Epoch 625/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.9464\n",
      "Epoch 626/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1956 - accuracy: 0.9554\n",
      "Epoch 627/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2684 - accuracy: 0.9018\n",
      "Epoch 628/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2612 - accuracy: 0.9107\n",
      "Epoch 629/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2916 - accuracy: 0.9196\n",
      "Epoch 630/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2010 - accuracy: 0.9375\n",
      "Epoch 631/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1818 - accuracy: 0.9375\n",
      "Epoch 632/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.9018\n",
      "Epoch 633/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2061 - accuracy: 0.9286\n",
      "Epoch 634/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2085 - accuracy: 0.9643\n",
      "Epoch 635/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2843 - accuracy: 0.9375\n",
      "Epoch 636/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1865 - accuracy: 0.9554\n",
      "Epoch 637/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1549 - accuracy: 0.9643\n",
      "Epoch 638/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2127 - accuracy: 0.9464\n",
      "Epoch 639/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9554\n",
      "Epoch 640/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1825 - accuracy: 0.9286\n",
      "Epoch 641/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2430 - accuracy: 0.9286\n",
      "Epoch 642/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1589 - accuracy: 0.9643\n",
      "Epoch 643/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2699 - accuracy: 0.9286\n",
      "Epoch 644/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3507 - accuracy: 0.8929\n",
      "Epoch 645/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2380 - accuracy: 0.8929\n",
      "Epoch 646/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2369 - accuracy: 0.9107\n",
      "Epoch 647/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2666 - accuracy: 0.9107\n",
      "Epoch 648/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2087 - accuracy: 0.9554\n",
      "Epoch 649/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.3484 - accuracy: 0.9018\n",
      "Epoch 650/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1882 - accuracy: 0.9554\n",
      "Epoch 651/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1847 - accuracy: 0.9643\n",
      "Epoch 652/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2246 - accuracy: 0.9375\n",
      "Epoch 653/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3594 - accuracy: 0.8750\n",
      "Epoch 654/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1856 - accuracy: 0.9375\n",
      "Epoch 655/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1988 - accuracy: 0.9554\n",
      "Epoch 656/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3362 - accuracy: 0.9018\n",
      "Epoch 657/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.2241 - accuracy: 0.9196\n",
      "Epoch 658/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1844 - accuracy: 0.9554\n",
      "Epoch 659/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2193 - accuracy: 0.9286\n",
      "Epoch 660/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.9732\n",
      "Epoch 661/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2772 - accuracy: 0.8929\n",
      "Epoch 662/700\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 0.1947 - accuracy: 0.9464\n",
      "Epoch 663/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1870 - accuracy: 0.9554\n",
      "Epoch 664/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1956 - accuracy: 0.9286\n",
      "Epoch 665/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1685 - accuracy: 0.9464\n",
      "Epoch 666/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1727 - accuracy: 0.9643\n",
      "Epoch 667/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1601 - accuracy: 0.9554\n",
      "Epoch 668/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2502 - accuracy: 0.9286\n",
      "Epoch 669/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1896 - accuracy: 0.9286\n",
      "Epoch 670/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2196 - accuracy: 0.9375\n",
      "Epoch 671/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.3300 - accuracy: 0.8839\n",
      "Epoch 672/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2377 - accuracy: 0.8929\n",
      "Epoch 673/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1839 - accuracy: 0.9375\n",
      "Epoch 674/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1880 - accuracy: 0.9286\n",
      "Epoch 675/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2154 - accuracy: 0.8929\n",
      "Epoch 676/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1614 - accuracy: 0.9554\n",
      "Epoch 677/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2047 - accuracy: 0.9107\n",
      "Epoch 678/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1898 - accuracy: 0.9375\n",
      "Epoch 679/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1461 - accuracy: 0.9821\n",
      "Epoch 680/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1340 - accuracy: 0.9732\n",
      "Epoch 681/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2557 - accuracy: 0.9286\n",
      "Epoch 682/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2062 - accuracy: 0.9375\n",
      "Epoch 683/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1973 - accuracy: 0.9375\n",
      "Epoch 684/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.9286\n",
      "Epoch 685/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.9464\n",
      "Epoch 686/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9554\n",
      "Epoch 687/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2416 - accuracy: 0.9107\n",
      "Epoch 688/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9464\n",
      "Epoch 689/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1386 - accuracy: 0.9821\n",
      "Epoch 690/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2747 - accuracy: 0.9464\n",
      "Epoch 691/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2217 - accuracy: 0.9286\n",
      "Epoch 692/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4217 - accuracy: 0.8750\n",
      "Epoch 693/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.3001 - accuracy: 0.8750\n",
      "Epoch 694/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.9821\n",
      "Epoch 695/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1816 - accuracy: 0.9375\n",
      "Epoch 696/700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.2806 - accuracy: 0.8750\n",
      "Epoch 697/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2774 - accuracy: 0.9286\n",
      "Epoch 698/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.2948 - accuracy: 0.8571\n",
      "Epoch 699/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1779 - accuracy: 0.9464\n",
      "Epoch 700/700\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.1772 - accuracy: 0.9464\n"
     ]
    }
   ],
   "source": [
    "iris_model = model.fit(x_train_new, y_train, epochs=700, batch_size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99993205e-01, 5.47849652e-07, 6.23536516e-06],\n",
       "       [1.69847496e-02, 9.76176202e-01, 6.83905324e-03],\n",
       "       [1.60930287e-02, 9.78199542e-01, 5.70742553e-03],\n",
       "       [1.69297289e-02, 9.76305246e-01, 6.76491624e-03],\n",
       "       [9.99984264e-01, 2.33819401e-06, 1.33112562e-05],\n",
       "       [1.57056171e-02, 9.79033887e-01, 5.26050199e-03],\n",
       "       [1.57173054e-04, 1.45410314e-01, 8.54432464e-01],\n",
       "       [5.99954801e-05, 5.05988821e-02, 9.49341118e-01],\n",
       "       [3.93028786e-05, 1.72990747e-02, 9.82661664e-01],\n",
       "       [2.88781248e-05, 1.39424205e-02, 9.86028731e-01],\n",
       "       [1.34200047e-04, 4.13258642e-01, 5.86607158e-01],\n",
       "       [3.57025310e-05, 1.44836511e-02, 9.85480607e-01],\n",
       "       [1.60977934e-02, 9.78189170e-01, 5.71308564e-03],\n",
       "       [1.62907485e-02, 9.77763414e-01, 5.94573608e-03],\n",
       "       [9.99968290e-01, 7.39880488e-06, 2.43606210e-05],\n",
       "       [9.99989510e-01, 1.26586144e-06, 9.14107477e-06],\n",
       "       [9.99942303e-01, 1.80726547e-05, 3.95988063e-05],\n",
       "       [1.63881388e-02, 9.77546155e-01, 6.06570579e-03],\n",
       "       [9.99996543e-01, 2.34985478e-07, 3.24453686e-06],\n",
       "       [1.59533415e-02, 9.78503406e-01, 5.54326316e-03],\n",
       "       [6.92240428e-05, 8.25147405e-02, 9.17416036e-01],\n",
       "       [1.59344915e-02, 9.78544176e-01, 5.52136870e-03],\n",
       "       [2.14759566e-04, 4.50249702e-01, 5.49535513e-01],\n",
       "       [1.59272179e-02, 9.78559792e-01, 5.51293558e-03],\n",
       "       [7.62221753e-05, 6.24473281e-02, 9.37476456e-01],\n",
       "       [1.56588145e-02, 9.79132950e-01, 5.20826504e-03],\n",
       "       [9.98054743e-01, 1.40390010e-03, 5.41324436e-04],\n",
       "       [1.28958462e-04, 3.86725396e-01, 6.13145649e-01],\n",
       "       [9.99984980e-01, 2.19739081e-06, 1.28675238e-05],\n",
       "       [3.18966660e-04, 4.82156098e-01, 5.17524958e-01],\n",
       "       [4.68755461e-05, 2.74791084e-02, 9.72474098e-01],\n",
       "       [3.44267064e-05, 1.33448653e-02, 9.86620665e-01],\n",
       "       [9.99998927e-01, 2.41976252e-08, 1.07006576e-06],\n",
       "       [9.99982834e-01, 2.96202575e-06, 1.42133995e-05],\n",
       "       [9.99984503e-01, 2.64256460e-06, 1.28493821e-05],\n",
       "       [9.99819934e-01, 8.20890345e-05, 9.79916949e-05],\n",
       "       [8.42113441e-05, 3.72816883e-02, 9.62634087e-01],\n",
       "       [1.63051020e-02, 9.77731645e-01, 5.96330222e-03]], dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 2, 2, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 0, 1, 2, 1,\n",
       "       2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 0, 0, 0, 0, 2, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes=np.argmax(y_pred,axis=1)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(classes), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]],\n",
       "\n",
       "       [[1.],\n",
       "        [1.],\n",
       "        [1.]]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[26,  0],\n",
       "        [ 0, 12]],\n",
       "\n",
       "       [[25,  0],\n",
       "        [ 1, 12]],\n",
       "\n",
       "       [[24,  1],\n",
       "        [ 0, 13]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test, onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00        12\n",
      "           0       1.00      0.92      0.96        13\n",
      "           1       0.93      1.00      0.96        13\n",
      "\n",
      "   micro avg       0.97      0.97      0.97        38\n",
      "   macro avg       0.98      0.97      0.97        38\n",
      "weighted avg       0.98      0.97      0.97        38\n",
      " samples avg       0.97      0.97      0.97        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "classes = {'0','1','2'}\n",
    "print(classification_report(y_test, onehot_encoded,target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'interp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb Cell 86'\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000085?line=26'>27</a>\u001b[0m mean_tpr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros_like(all_fpr)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000085?line=28'>29</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_classes):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000085?line=30'>31</a>\u001b[0m     mean_tpr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m interp(all_fpr, fpr[i], tpr[i])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000085?line=34'>35</a>\u001b[0m mean_tpr \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m n_classes\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/christopher/miniconda3/envs/myenv/conda-meta/kerasclassification.ipynb#ch0000085?line=38'>39</a>\u001b[0m fpr[\u001b[39m\"\u001b[39m\u001b[39mmacro\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m all_fpr\n",
      "\u001b[0;31mNameError\u001b[0m: name 'interp' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score , auc, roc_curve\n",
    "\n",
    "n_classes=3\n",
    "\n",
    "fpr = dict()\n",
    "\n",
    "\n",
    "tpr = dict()\n",
    "\n",
    "roc_auc = dict()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(n_classes):\n",
    "\n",
    "    fpr[i], tpr[i], _ = roc_curve(np.array(y_test)[:, i], np.array(onehot_encoded)[:, i])\n",
    "\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "\n",
    "\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "\n",
    "for i in range(n_classes):\n",
    "\n",
    "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "\n",
    "\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "\n",
    "\n",
    "fpr[\"macro\"] = all_fpr\n",
    "\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "\n",
    "\n",
    "lw=2\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "\n",
    "         label='macro-average ROC curve (area = {0:0.2f})'\n",
    "\n",
    "               ''.format(roc_auc[\"macro\"]),\n",
    "\n",
    "         color='brown', linestyle=':', linewidth=4)\n",
    "\n",
    "\n",
    "\n",
    "colors = cycle(['red', 'blue', 'yellow', 'green'])\n",
    "\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
    "\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "\n",
    "             ''.format(i, roc_auc[i]))\n",
    "\n",
    "\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--',color='black', lw=lw)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.annotate('Random Guess',(.5,.48),color='black')\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.title('Receiver Operating Characteristic for Midi Dataset Feature Selection 1')\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "# set plot figure size\n",
    "fig, c_ax = plt.subplots(1,1, figsize = (12, 8))\n",
    "\n",
    "# function for scoring roc auc score for multi-class\n",
    "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_test)\n",
    "    y_test = lb.transform(y_test)\n",
    "\n",
    "    for (idx, c_label) in enumerate(classes):\n",
    "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
    "        c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
    "    c_ax.plot(fpr, fpr, 'b-', label = 'Random Guessing')\n",
    "    return roc_auc_score(y_test, y_pred, average=average)\n",
    "\n",
    "\n",
    "print('ROC AUC score:', multiclass_roc_auc_score(y_test, y_pred))\n",
    "\n",
    "c_ax.legend()\n",
    "c_ax.set_xlabel('False Positive Rate')\n",
    "c_ax.set_ylabel('True Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute Precision-Recall and plot curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "precision = dict()\n",
    "recall = dict()\n",
    "average_precision = dict()\n",
    "for i in range(n_classes):\n",
    "    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i],\n",
    "                                                        onehot_encoded[:, i])\n",
    "    average_precision[i] = average_precision_score(y_test[:, i], onehot_encoded[:, i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(),\n",
    "    onehot_encoded.ravel())\n",
    "average_precision[\"micro\"] = average_precision_score(y_test, onehot_encoded,\n",
    "                                                     average=\"micro\")\n",
    "\n",
    "\n",
    "# Plot Precision-Recall curve for each class\n",
    "plt.clf()\n",
    "plt.plot(recall[\"micro\"], precision[\"micro\"],\n",
    "         label='micro-average Precision-recall curve (area = {0:0.2f})'\n",
    "               ''.format(average_precision[\"micro\"]), linestyle=':')\n",
    "for i in range(n_classes):\n",
    "    plt.plot(recall[i], precision[i],\n",
    "             label='Precision-recall curve of class {0} (area = {1:0.2f})'\n",
    "                   ''.format(i, average_precision[i]))\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall for Midi Dataset Feature Selection 1')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
